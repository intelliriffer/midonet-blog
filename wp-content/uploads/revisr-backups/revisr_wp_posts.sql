
/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;
DROP TABLE IF EXISTS `wp_posts`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `wp_posts` (
  `ID` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `post_author` bigint(20) unsigned NOT NULL DEFAULT '0',
  `post_date` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `post_date_gmt` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `post_content` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `post_title` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `post_excerpt` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `post_status` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'publish',
  `comment_status` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'open',
  `ping_status` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'open',
  `post_password` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT '',
  `post_name` varchar(200) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT '',
  `to_ping` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `pinged` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `post_modified` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `post_modified_gmt` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `post_content_filtered` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `post_parent` bigint(20) unsigned NOT NULL DEFAULT '0',
  `guid` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT '',
  `menu_order` int(11) NOT NULL DEFAULT '0',
  `post_type` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'post',
  `post_mime_type` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT '',
  `comment_count` bigint(20) NOT NULL DEFAULT '0',
  PRIMARY KEY (`ID`),
  KEY `type_status_date` (`post_type`,`post_status`,`post_date`,`ID`),
  KEY `post_parent` (`post_parent`),
  KEY `post_author` (`post_author`),
  KEY `post_name` (`post_name`(191))
) ENGINE=InnoDB AUTO_INCREMENT=581 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

LOCK TABLES `wp_posts` WRITE;
/*!40000 ALTER TABLE `wp_posts` DISABLE KEYS */;
INSERT INTO `wp_posts` VALUES (2,1,'2014-11-13 07:52:59','2014-11-13 07:52:59','This is an example page. It\'s different from a blog post because it will stay in one place and will show up in your site navigation (in most themes). Most people start with an About page that introduces them to potential site visitors. It might say something like this:\n\n<blockquote>Hi there! I\'m a bike messenger by day, aspiring actor by night, and this is my blog. I live in Los Angeles, have a great dog named Jack, and I like pi&#241;a coladas. (And gettin\' caught in the rain.)</blockquote>\n\n...or something like this:\n\n<blockquote>The XYZ Doohickey Company was founded in 1971, and has been providing quality doohickeys to the public ever since. Located in Gotham City, XYZ employs over 2,000 people and does all kinds of awesome things for the Gotham community.</blockquote>\n\nAs a new WordPress user, you should go to <a href=\"http://blog.midonet.org/wp-admin/\">your dashboard</a> to delete this page and create new pages for your content. Have fun!','Sample Page','','publish','open','open','','sample-page','','','2014-11-13 07:52:59','2014-11-13 07:52:59','',0,'http://blog.midonet.org/?page_id=2',0,'page','',0),(4,1,'2014-11-13 08:02:25','2014-11-13 08:02:25','http://blog.midonet.org/wp-content/uploads/2014/11/cropped-header.jpg','cropped-header.jpg','','inherit','closed','open','','cropped-header-jpg','','','2014-11-13 08:02:25','2014-11-13 08:02:25','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/header.jpg',0,'attachment','image/jpeg',0),(5,1,'2014-11-13 08:03:54','2014-11-13 08:03:54','http://blog.midonet.org/wp-content/uploads/2014/11/cropped-header1.jpg','cropped-header1.jpg','','inherit','closed','open','','cropped-header1-jpg','','','2014-11-13 08:03:54','2014-11-13 08:03:54','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/header.jpg',0,'attachment','image/jpeg',0),(6,1,'2014-11-15 02:59:23','2014-11-15 02:59:23','','midonet-logo-blog','','inherit','open','open','','midonet-logo-blog','','','2014-11-15 02:59:23','2014-11-15 02:59:23','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/midonet-logo-blog.png',0,'attachment','image/png',0),(7,1,'2014-11-15 03:12:46','2014-11-15 03:12:46','','','','inherit','open','open','','midonetmark','','','2015-10-15 09:31:27','2015-10-15 09:31:27','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/midonetmark.png',0,'attachment','image/png',0),(8,1,'2014-11-15 03:13:15','2014-11-15 03:13:15','','header','','inherit','open','open','','header','','','2014-11-15 03:13:15','2014-11-15 03:13:15','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/header.jpg',0,'attachment','image/jpeg',0),(9,1,'2014-11-15 03:29:50','2014-11-15 03:29:50','','HOME','','publish','open','closed','','home','','','2015-10-15 08:40:43','2015-10-15 08:40:43','',0,'http://blog.midonet.org/?p=9',1,'nav_menu_item','',0),(11,1,'2014-11-15 03:29:50','2014-11-15 03:29:50','','WIKI','','publish','open','open','','wiki','','','2015-07-16 11:37:08','2015-07-16 11:37:08','',0,'http://blog.midonet.org/?p=11',2,'nav_menu_item','',0),(14,1,'2014-11-15 03:45:18','2014-11-15 03:45:18','','','','inherit','open','open','','large-blog-image','','','2015-10-15 08:42:28','2015-10-15 08:42:28','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/512pxMidoMark-5466cc4e_site_icon.png',0,'attachment','image/png',0),(18,2,'2014-11-15 08:36:24','2014-11-15 08:36:24','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<em>New MidoNet Contributors</em>\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">how to contribute guide</a> on the wiki for more information on how to get involved.\n\n[mc4wp_form]','MidoNet Community Newsletter Issue #0','','publish','open','open','','midonet-community-newsletter-issue-0','','','2014-12-01 17:39:50','2014-12-01 17:39:50','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org). MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n_New MidoNet Contributors_\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\r\n<ul>\r\n	<li>Working on documentation</li>\r\n	<li>Submitting bug reports</li>\r\n	<li>Benchmarking MidoNet on various platforms and sharing the results</li>\r\n	<li>Being an active user that voices opinions regarding new features and integrations</li>\r\n	<li>Fixing bugs or adding new features</li>\r\n	<li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\r\n</ul>\r\nCheck out the [how to contribute guide](http://wiki.midonet.org/How%20to%20Contribute) on the wiki for more information on how to get involved.\r\n\r\n[mc4wp_form]',0,'http://blog.midonet.org/?p=18',0,'post','',0),(19,2,'2014-11-15 06:30:12','2014-11-15 06:30:12','#First Newsletter\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\nMidoNet Open Sources','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 06:30:12','2014-11-15 06:30:12','#First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\nMidoNet Open Sources',18,'http://blog.midonet.org/?p=19',0,'revision','',0),(20,2,'2014-11-15 06:30:39','2014-11-15 06:30:39','<h1>First Newsletter</h1>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\nMidoNet Open Sources','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 06:30:39','2014-11-15 06:30:39','# First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\nMidoNet Open Sources',18,'http://blog.midonet.org/?p=20',0,'revision','',0),(21,2,'2014-11-15 06:30:49','2014-11-15 06:30:49','<h3>First Newsletter</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\nMidoNet Open Sources','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 06:30:49','2014-11-15 06:30:49','### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\nMidoNet Open Sources',18,'http://blog.midonet.org/?p=21',0,'revision','',0),(22,2,'2014-11-15 06:31:00','2014-11-15 06:31:00','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\nMidoNet Open Sources','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 06:31:00','2014-11-15 06:31:00','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\nMidoNet Open Sources',18,'http://blog.midonet.org/?p=22',0,'revision','',0),(23,2,'2014-11-15 06:46:16','2014-11-15 06:46:16','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, Midokura open sourced MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 06:46:16','2014-11-15 06:46:16','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, Midokura open sourced MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology',18,'http://blog.midonet.org/?p=23',0,'revision','',0),(24,2,'2014-11-15 06:47:28','2014-11-15 06:47:28','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midonet.org\">Midokura</a> open sourced MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 06:47:28','2014-11-15 06:47:28','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midonet.org) open sourced MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology',18,'http://blog.midonet.org/?p=24',0,'revision','',0),(25,2,'2014-11-15 07:04:30','2014-11-15 07:04:30','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  MidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h4>Try MidoNet with OpenStack in Minutes</h4>\n\nIf you haven\'t tried','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:04:30','2014-11-15 07:04:30','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  MidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n#### Try MidoNet with OpenStack in Minutes\r\n\r\nIf you haven\'t tried',18,'http://blog.midonet.org/?p=25',0,'revision','',0),(26,2,'2014-11-15 07:04:50','2014-11-15 07:04:50','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h4>Try MidoNet with OpenStack in Minutes</h4>\n\nIf you haven\'t tried','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:04:50','2014-11-15 07:04:50','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n#### Try MidoNet with OpenStack in Minutes\r\n\r\nIf you haven\'t tried',18,'http://blog.midonet.org/?p=26',0,'revision','',0),(27,2,'2014-11-15 07:06:25','2014-11-15 07:06:25','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h4>Video from Paris</h4>\n\n<iframe style=\"margin-top: 40px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes</h4>\n\nIf you haven\'t tried','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:06:25','2014-11-15 07:06:25','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n#### Video from Paris\r\n\r\n<iframe style=\"margin-top: 40px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes\r\n\r\nIf you haven\'t tried',18,'http://blog.midonet.org/?p=27',0,'revision','',0),(28,2,'2014-11-15 07:09:14','2014-11-15 07:09:14','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 40px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes</h4>\n\nIf you haven\'t tried','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:09:14','2014-11-15 07:09:14','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n\r\n<iframe style=\"margin-top: 40px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes\r\n\r\nIf you haven\'t tried',18,'http://blog.midonet.org/?p=28',0,'revision','',0),(29,2,'2014-11-15 07:10:22','2014-11-15 07:10:22','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:40px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes</h4>\n\nIf you haven\'t tried','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:10:22','2014-11-15 07:10:22','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:40px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes\r\n\r\nIf you haven\'t tried',18,'http://blog.midonet.org/?p=29',0,'revision','',0),(30,2,'2014-11-15 07:15:10','2014-11-15 07:15:10','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes</h4>\n\nIf you haven\'t tried','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:15:10','2014-11-15 07:15:10','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes\r\n\r\nIf you haven\'t tried',18,'http://blog.midonet.org/?p=30',0,'revision','',0),(31,2,'2014-11-15 07:18:05','2014-11-15 07:18:05','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a>.','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:18:05','2014-11-15 07:18:05','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org).   ',18,'http://blog.midonet.org/?p=31',0,'revision','',0),(32,2,'2014-11-15 07:20:03','2014-11-15 07:20:03','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:20:03','2014-11-15 07:20:03','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  ',18,'http://blog.midonet.org/?p=32',0,'revision','',0),(33,2,'2014-11-15 07:24:17','2014-11-15 07:24:17','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  ]<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:24:17','2014-11-15 07:24:17','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:60px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  ][Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!',18,'http://blog.midonet.org/?p=33',0,'revision','',0),(34,2,'2014-11-15 07:25:13','2014-11-15 07:25:13','<h4>First Newsletter</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:25:13','2014-11-15 07:25:13','#### First Newsletter\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!',18,'http://blog.midonet.org/?p=34',0,'revision','',0),(35,2,'2014-11-15 07:27:34','2014-11-15 07:27:34','MidoNet Launch Party at OpenStack Summit in Paris','IMG_2469','','inherit','open','open','','img_2469','','','2014-11-15 07:35:02','2014-11-15 07:35:02','',18,'http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg',0,'attachment','image/jpeg',0),(36,2,'2014-11-15 07:29:55','2014-11-15 07:29:55','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"300\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-300x225.jpg\" alt=\"Launch Party\" width=\"300\" height=\"225\" class=\"size-medium wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:29:55','2014-11-15 07:29:55','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"300\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-300x225.jpg\" alt=\"Launch Party\" width=\"300\" height=\"225\" class=\"size-medium wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=36',0,'revision','',0),(37,2,'2014-11-15 07:32:02','2014-11-15 07:32:02','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"375\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-300x225.jpg\" alt=\"Launch Party\" width=\"375\" height=\"500\" class=\"size-medium wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:32:02','2014-11-15 07:32:02','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"375\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-300x225.jpg\" alt=\"Launch Party\" width=\"375\" height=\"500\" class=\"size-medium wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=37',0,'revision','',0),(38,2,'2014-11-15 07:33:15','2014-11-15 07:33:15','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1024\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:33:15','2014-11-15 07:33:15','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1024\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=38',0,'revision','',0),(39,2,'2014-11-15 07:33:49','2014-11-15 07:33:49','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1038\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:33:49','2014-11-15 07:33:49','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1038\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=39',0,'revision','',0),(40,2,'2014-11-15 07:34:05','2014-11-15 07:34:05','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1050\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:34:05','2014-11-15 07:34:05','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1050\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=40',0,'revision','',0),(41,2,'2014-11-15 07:34:19','2014-11-15 07:34:19','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1090\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:34:19','2014-11-15 07:34:19','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n[caption id=\"attachment_35\" align=\"aligncenter\" width=\"1090\"]<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"size-large wp-image-35\" /> MidoNet Launch Party at OpenStack Summit in Paris[/caption]\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=41',0,'revision','',0),(42,2,'2014-11-15 07:35:05','2014-11-15 07:35:05','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:35:05','2014-11-15 07:35:05','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=42',0,'revision','',0),(43,2,'2014-11-15 07:35:34','2014-11-15 07:35:34','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:35:34','2014-11-15 07:35:34','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=43',0,'revision','',0),(44,2,'2014-11-15 07:38:29','2014-11-15 07:38:29','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n<center>We\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!</center>\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:38:29','2014-11-15 07:38:29','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n<center>We\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!</center>\r\n\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=44',0,'revision','',0),(45,2,'2014-11-15 07:39:12','2014-11-15 07:39:12','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n####','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:39:12','2014-11-15 07:39:12','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n####',18,'http://blog.midonet.org/?p=45',0,'revision','',0),(46,2,'2014-11-15 07:43:09','2014-11-15 07:43:09','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  <a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h4>MidoNet Updates</h4>\n\nfoo bar','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 07:43:09','2014-11-15 07:43:09','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org)MidoNet.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  [Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n#### MidoNet Updates\r\nfoo bar',18,'http://blog.midonet.org/?p=46',0,'revision','',0),(47,2,'2014-11-15 08:19:49','2014-11-15 08:19:49','<h4>Welcome!</h4>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h4>Midokura Unleashes MidoNet</h4>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h5>La vie d\'un paquet</h5>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h4>Try MidoNet with OpenStack in Minutes With MidoStack</h4>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h4>MidoNet Updates</h4>\n\nfoo bar','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:19:49','2014-11-15 08:19:49','#### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n#### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org).  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n##### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n#### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  \r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n#### MidoNet Updates\r\nfoo bar',18,'http://blog.midonet.org/?p=47',0,'revision','',0),(48,2,'2014-11-15 08:20:31','2014-11-15 08:20:31','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>MidoNet Updates</h3>\n\nfoo bar','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:20:31','2014-11-15 08:20:31','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org).  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  \r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### MidoNet Updates\r\nfoo bar',18,'http://blog.midonet.org/?p=48',0,'revision','',0),(49,2,'2014-11-15 08:29:41','2014-11-15 08:29:41','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<ul>\n    <li>[Bryan Stenson](https://github.com/stensonb)</li>\n    <li>[Tomofumi Hayashi](https://github.com/s1061123)</li>\n</ul>','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:29:41','2014-11-15 08:29:41','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org).  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  \r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n<ul>\r\n	<li>[Bryan Stenson](https://github.com/stensonb)</li>\r\n	<li>[Tomofumi Hayashi](https://github.com/s1061123)</li>\r\n</ul>\r\n\r\n',18,'http://blog.midonet.org/?p=49',0,'revision','',0),(50,2,'2014-11-15 08:30:43','2014-11-15 08:30:43','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:30:43','2014-11-15 08:30:43','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org).  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  \r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\n',18,'http://blog.midonet.org/?p=50',0,'revision','',0),(51,2,'2014-11-15 08:31:16','2014-11-15 08:31:16','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<h5>New MidoNet Contributors</h5>\n\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:31:16','2014-11-15 08:31:16','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org).  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  \r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n##### New MidoNet Contributors\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\n',18,'http://blog.midonet.org/?p=51',0,'revision','',0),(52,2,'2014-11-15 08:31:43','2014-11-15 08:31:43','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>.  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<h4>New MidoNet Contributors</h4>\n\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:31:43','2014-11-15 08:31:43','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org).  MidoNet is a fully distributed, production grade network virtualization overlay technology.  Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come.  MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.  \r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more.  Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"aligncenter size-large wp-image-35\" />\r\n\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit.  In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom:80px\" width=\"853\" height=\"480\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\r\n\r\n\r\n\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started!  [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop.  It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04.  It only takes two commands, and about 30-40 minutes to build and you\'re all set.  \r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project.  Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n#### New MidoNet Contributors\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\n',18,'http://blog.midonet.org/?p=52',0,'revision','',0),(53,2,'2014-11-15 08:35:55','2014-11-15 08:35:55','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img class=\"aligncenter size-large wp-image-35\" src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<h4>New MidoNet Contributors</h4>\n\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">How to Contribute guide</a> on the wiki for more information on how to get involved.','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:35:55','2014-11-15 08:35:55','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org). MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img class=\"aligncenter size-large wp-image-35\" src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" />\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n#### New MidoNet Contributors\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\r\n<ul>\r\n	<li>Working on documentation</li>\r\n	<li>Submitting bug reports</li>\r\n	<li>Benchmarking MidoNet on various platforms and sharing the results</li>\r\n	<li>Being an active user that voices opinions regarding new features and integrations</li>\r\n	<li>Fixing bugs or adding new features</li>\r\n	<li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\r\n</ul>\r\nCheck out the [How to Contribute guide](http://wiki.midonet.org/How%20to%20Contribute) on the wiki for more information on how to get involved.',18,'http://blog.midonet.org/?p=53',0,'revision','',0),(54,2,'2014-11-15 08:36:24','2014-11-15 08:36:24','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<img class=\"aligncenter size-large wp-image-35\" src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" />\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<h4>New MidoNet Contributors</h4>\n\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">how to contribute guide</a> on the wiki for more information on how to get involved.','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 08:36:24','2014-11-15 08:36:24','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org). MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<img class=\"aligncenter size-large wp-image-35\" src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" />\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n#### New MidoNet Contributors\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\r\n<ul>\r\n	<li>Working on documentation</li>\r\n	<li>Submitting bug reports</li>\r\n	<li>Benchmarking MidoNet on various platforms and sharing the results</li>\r\n	<li>Being an active user that voices opinions regarding new features and integrations</li>\r\n	<li>Fixing bugs or adding new features</li>\r\n	<li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\r\n</ul>\r\nCheck out the [how to contribute guide](http://wiki.midonet.org/How%20to%20Contribute) on the wiki for more information on how to get involved.',18,'http://blog.midonet.org/?p=54',0,'revision','',0),(55,2,'2014-11-16 23:05:26','2014-11-16 23:05:26','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<em>New MidoNet Contributors</em>\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">how to contribute guide</a> on the wiki for more information on how to get involved.\n\n[mc4wp_form]','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-autosave-v1','','','2014-11-16 23:05:26','2014-11-16 23:05:26','',18,'http://blog.midonet.org/?p=55',0,'revision','',0),(56,2,'2014-11-15 09:59:32','2014-11-15 09:59:32','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<h4>New MidoNet Contributors</h4>\n\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">how to contribute guide</a> on the wiki for more information on how to get involved.','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 09:59:32','2014-11-15 09:59:32','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org). MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n#### New MidoNet Contributors\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\r\n<ul>\r\n	<li>Working on documentation</li>\r\n	<li>Submitting bug reports</li>\r\n	<li>Benchmarking MidoNet on various platforms and sharing the results</li>\r\n	<li>Being an active user that voices opinions regarding new features and integrations</li>\r\n	<li>Fixing bugs or adding new features</li>\r\n	<li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\r\n</ul>\r\nCheck out the [how to contribute guide](http://wiki.midonet.org/How%20to%20Contribute) on the wiki for more information on how to get involved.',18,'http://blog.midonet.org/?p=56',0,'revision','',0),(57,2,'2014-11-15 10:03:23','2014-11-15 10:03:23','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<em>New MidoNet Contributors</em>\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">how to contribute guide</a> on the wiki for more information on how to get involved.','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-15 10:03:23','2014-11-15 10:03:23','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org). MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n_New MidoNet Contributors_\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\r\n<ul>\r\n	<li>Working on documentation</li>\r\n	<li>Submitting bug reports</li>\r\n	<li>Benchmarking MidoNet on various platforms and sharing the results</li>\r\n	<li>Being an active user that voices opinions regarding new features and integrations</li>\r\n	<li>Fixing bugs or adding new features</li>\r\n	<li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\r\n</ul>\r\nCheck out the [how to contribute guide](http://wiki.midonet.org/How%20to%20Contribute) on the wiki for more information on how to get involved.',18,'http://blog.midonet.org/?p=57',0,'revision','',0),(60,2,'2014-11-16 23:05:27','2014-11-16 23:05:27','<h3>Welcome!</h3>\n\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\n\n<h3>Midokura Unleashes MidoNet</h3>\n\nOn Monday, November 3rd at the OpenStack Summit in Paris, <a href=\"http://midokura.org\">Midokura</a> open sourced <a href=\"http://midonet.org\">MidoNet</a>. MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\n\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\n\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\n\n<h3>La vie d\'un paquet</h3>\n\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\n\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\n\n<h3>Try MidoNet with OpenStack in Minutes With MidoStack</h3>\n\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! <a href=\"http://github.com/midonet/midostack\">MidoStack</a> is a tool based on <a href=\"http://devstack.org\">DevStack</a> which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\n\nAfter you\'ve deployed MidoStack, be sure to check the <a href=\"http://wiki.midonet.org/GettingStartedMidonet\">Getting Started guide</a> on the wiki for what to do next.\n\n<a href=\"http://midonet.org/#quickstart\">Give it a shot</a>, and let us know how it goes!\n\n<h3>Welcome New Contributors!</h3>\n\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\n\n<em>New MidoNet Contributors</em>\n<a href=\"https://github.com/stensonb\">Bryan Stenson</a>\n<a href=\"https://github.com/s1061123\">Tomofumi Hayashi</a>\n\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\n\n<ul>\n    <li>Working on documentation</li>\n    <li>Submitting bug reports</li>\n    <li>Benchmarking MidoNet on various platforms and sharing the results</li>\n    <li>Being an active user that voices opinions regarding new features and integrations</li>\n    <li>Fixing bugs or adding new features</li>\n    <li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\n</ul>\n\nCheck out the <a href=\"http://wiki.midonet.org/How%20to%20Contribute\">how to contribute guide</a> on the wiki for more information on how to get involved.\n\n[mc4wp_form]','MidoNet Community Newsletter Issue #0','','inherit','open','open','','18-revision-v1','','','2014-11-16 23:05:27','2014-11-16 23:05:27','### Welcome!\r\n\r\nWelcome to the first MidoNet Community Newsletter!  You can find regular updates covering what\'s new in MidoNet, the community, and anything related to it.\r\n\r\n### Midokura Unleashes MidoNet\r\n\r\nOn Monday, November 3rd at the OpenStack Summit in Paris, [Midokura](http://midokura.org) open sourced [MidoNet](http://midonet.org). MidoNet is a fully distributed, production grade network virtualization overlay technology. Twenty companies have joined as founding members of the MidoNet community on launch to show their public support, with more to come. MidoNet\'s goal is to provide a truly open, highly scalable, easy to use, production ready solution for Infrastructure as a Service (IaaS) environments.\r\n\r\nMidoNet provides layer 2-4 networking services like distributed routing, load balancers, security groups, NAT, and more. Essentially everything you need for Amazon VPC type networking out of the box.\r\n\r\nWe\'d like to thank everyone who came to the launch party in Paris, we had a fantastic time!\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469.jpg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/11/IMG_2469-1024x768.jpg\" alt=\"Launch Party\" width=\"1024\" height=\"768\" class=\"alignnone size-large wp-image-35\" /></a>\r\n\r\n### La vie d\'un paquet\r\n\r\nCynthia Thomas, from Midokura gave a great presentation on MidoNet at the Paris OpenStack Summit. In the video (below), she covers an overview of MidoNet, and digs deep into what makes MidoNet tick.\r\n<iframe style=\"margin-top: 15px; margin-bottom: 80px;\" src=\"//www.youtube.com/embed/Qoy62Fkd7xQ\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"></iframe>\r\n\r\n### Try MidoNet with OpenStack in Minutes With MidoStack\r\n\r\nIf you haven\'t tried out MidoNet yet, it\'s very easy to get started! [MidoStack](http://github.com/midonet/midostack) is a tool based on [DevStack](http://devstack.org) which installs a developer environment consisting of OpenStack and MidoNet on your laptop. It runs on any platform by using a Vagrant VM, or runs natively on Ubuntu 14.04. It only takes two commands, and about 30-40 minutes to build and you\'re all set.\r\n\r\nAfter you\'ve deployed MidoStack, be sure to check the [Getting Started guide](http://wiki.midonet.org/GettingStartedMidonet) on the wiki for what to do next.\r\n\r\n[Give it a shot](http://midonet.org/#quickstart), and let us know how it goes!\r\n\r\n### Welcome New Contributors!\r\nSince open sourcing, we\'ve had two new contributors to the project. Thanks for your contributions, and we\'re looking forward to seeing many more to come in the future!\r\n\r\n_New MidoNet Contributors_\r\n[Bryan Stenson](https://github.com/stensonb)\r\n[Tomofumi Hayashi](https://github.com/s1061123)\r\n\r\nIf you\'d like to contribute, there are many ways to contribute to MidoNet including\r\n<ul>\r\n	<li>Working on documentation</li>\r\n	<li>Submitting bug reports</li>\r\n	<li>Benchmarking MidoNet on various platforms and sharing the results</li>\r\n	<li>Being an active user that voices opinions regarding new features and integrations</li>\r\n	<li>Fixing bugs or adding new features</li>\r\n	<li>Being an evangelist for MidoNet (speaking, writing blogs, etc)</li>\r\n</ul>\r\nCheck out the [how to contribute guide](http://wiki.midonet.org/How%20to%20Contribute) on the wiki for more information on how to get involved.\r\n\r\n[mc4wp_form]',18,'http://blog.midonet.org/18-revision-v1/',0,'revision','',0),(64,2,'2014-11-18 04:22:48','2014-11-18 04:22:48','So you\'ve installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/','MidoNet Online Meetup','','publish','open','open','','midonet-online-meetup','','','2014-12-03 02:20:10','2014-12-03 02:20:10','So you\'ve installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\r\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/',0,'http://blog.midonet.org/?p=64',0,'post','',0),(65,2,'2014-11-18 04:19:34','2014-11-18 04:19:34','###\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/','MidoNet Online Meetup','','inherit','open','open','','64-revision-v1','','','2014-11-18 04:19:34','2014-11-18 04:19:34','###\r\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/',64,'http://blog.midonet.org/64-revision-v1/',0,'revision','',0),(66,2,'2014-11-18 04:22:06','2014-11-18 04:22:06','So you installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/','MidoNet Online Meetup','','inherit','open','open','','64-revision-v1','','','2014-11-18 04:22:06','2014-11-18 04:22:06','So you installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\r\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/',64,'http://blog.midonet.org/64-revision-v1/',0,'revision','',0),(67,2,'2014-11-18 04:23:19','2014-11-18 04:23:19','So you\'ve installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/','MidoNet Online Meetup','','inherit','open','open','','64-revision-v1','','','2014-11-18 04:23:19','2014-11-18 04:23:19','So you\'ve installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\r\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/',64,'http://blog.midonet.org/64-revision-v1/',0,'revision','',0),(68,2,'2014-11-18 18:28:36','2014-11-18 18:28:36','So you\'ve installed <a href=\"http://midonet.org/#quickstart\" target=\"_blank\">MidoStack</a>, but now what?  Check out our first online meetup to get some tips on next steps.\nhttp://www.meetup.com/Online-MidoNet-Meetup/events/218731246/','MidoNet Online Meetup','','inherit','open','open','','64-autosave-v1','','','2014-11-18 18:28:36','2014-11-18 18:28:36','',64,'http://blog.midonet.org/64-autosave-v1/',0,'revision','',0),(71,5,'2014-12-01 15:34:01','2014-12-01 15:34:01','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[caption id=\"attachment_73\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\"><img class=\"wp-image-73 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\" alt=\"Inter-tenant connectivity via Neutron External Network\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via Neutron External Network[/caption]\n\n[caption id=\"attachment_72\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\"><img class=\"wp-image-72 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\" alt=\"Inter-tenant connectivity via MN Provider Router\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via MN Provider Router[/caption]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','publish','open','open','','introduction-mns-overlay-network-models-part-1-provider-router','','\nhttp://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/','2014-12-04 17:32:36','2014-12-04 17:32:36','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[caption id=\"attachment_73\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\"><img class=\"wp-image-73 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\" alt=\"Inter-tenant connectivity via Neutron External Network\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via Neutron External Network[/caption]\r\n\r\n[caption id=\"attachment_72\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\"><img class=\"wp-image-72 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\" alt=\"Inter-tenant connectivity via MN Provider Router\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via MN Provider Router[/caption]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\r\n\r\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.',0,'http://blog.midonet.org/?p=71',0,'post','',5),(72,5,'2014-12-01 14:23:04','2014-12-01 14:23:04','','MNProviderRouter','Inter-tenant connectivity via MN Provider Router','inherit','open','open','','mnproviderrouter','','','2014-12-01 14:27:29','2014-12-01 14:27:29','',71,'http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg',0,'attachment','image/jpeg',0),(73,5,'2014-12-01 14:23:05','2014-12-01 14:23:05','','ExternalNetworkRouting','Inter-tenant connectivity via Neutron External Network','inherit','open','open','','externalnetworkrouting','','','2014-12-01 14:27:23','2014-12-01 14:27:23','',71,'http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting.jpg',0,'attachment','image/jpeg',0),(75,5,'2014-12-01 15:23:42','2014-12-01 15:23:42','','MN_PR_UplinksToPhys','','inherit','open','open','','mn_pr_uplinkstophys','','','2014-12-01 15:23:42','2014-12-01 15:23:42','',71,'http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg',0,'attachment','image/jpeg',0),(76,5,'2014-12-01 15:33:14','2014-12-01 15:33:14','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n&nbsp;\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-01 15:33:14','2014-12-01 15:33:14','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n&nbsp;\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\n&nbsp;',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(77,5,'2014-12-04 17:31:50','2014-12-04 17:31:50','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[caption id=\"attachment_73\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\"><img class=\"wp-image-73 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\" alt=\"Inter-tenant connectivity via Neutron External Network\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via Neutron External Network[/caption]\n\n[caption id=\"attachment_72\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\"><img class=\"size-medium wp-image-72\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter-300x225.jpg\" alt=\"Inter-tenant connectivity via MN Provider Router\" width=\"300\" height=\"225\" /></a> Inter-tenant connectivity via MN Provider Router[/caption]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-autosave-v1','','','2014-12-04 17:31:50','2014-12-04 17:31:50','',71,'http://blog.midonet.org/71-autosave-v1/',0,'revision','',0),(78,5,'2014-12-01 15:36:39','2014-12-01 15:36:39','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-01 15:36:39','2014-12-01 15:36:39','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\n&nbsp;',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(79,5,'2014-12-01 16:47:54','2014-12-01 16:47:54','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n&nbsp;\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','publish','open','open','','introduction-mns-overlay-network-models-part-2-tenant-routers-bridges','','\nhttp://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/','2014-12-04 17:30:02','2014-12-04 17:30:02','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\r\n\r\n&nbsp;\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',0,'http://blog.midonet.org/?p=79',0,'post','',4),(80,5,'2014-12-01 15:40:48','2014-12-01 15:40:48','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-01 15:40:48','2014-12-01 15:40:48','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\n&nbsp;',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(81,5,'2014-12-01 16:47:54','2014-12-01 16:47:54','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Create more Networks (with Subnets) as needed. Connect them to the Router (unless they are meant to be isolated, e.g. PCI compliant).</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically generate (and store in Neutron DB, usually MySQL) for each vNIC, one MAC address and one IP address per Subnet.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created.</li>\n    <li></li>\n</ol>','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-01 16:47:54','2014-12-01 16:47:54','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Create more Networks (with Subnets) as needed. Connect them to the Router (unless they are meant to be isolated, e.g. PCI compliant).</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically generate (and store in Neutron DB, usually MySQL) for each vNIC, one MAC address and one IP address per Subnet.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created.</li>\r\n	<li></li>\r\n</ol>',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(82,5,'2014-12-01 16:48:02','2014-12-01 16:48:02','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the next post we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-01 16:48:02','2014-12-01 16:48:02','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the next post we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\n&nbsp;',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(83,5,'2014-12-01 16:49:19','2014-12-01 16:49:19','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\n&nbsp;\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.\n\n&nbsp;\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-01 16:49:19','2014-12-01 16:49:19','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\n&nbsp;\r\n\r\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.\r\n\r\n&nbsp;\r\n\r\n&nbsp;',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(84,5,'2014-12-04 17:29:51','2014-12-04 17:29:51','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n&nbsp;\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-autosave-v1','','','2014-12-04 17:29:51','2014-12-04 17:29:51','',79,'http://blog.midonet.org/79-autosave-v1/',0,'revision','',0),(86,1,'2014-12-01 17:14:11','2014-12-01 17:14:11','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\n&nbsp;\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-autosave-v1','','','2014-12-01 17:14:11','2014-12-01 17:14:11','',71,'http://blog.midonet.org/71-autosave-v1/',0,'revision','',0),(87,1,'2014-12-01 17:14:53','2014-12-01 17:14:53','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\n&nbsp;\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-01 17:14:53','2014-12-01 17:14:53','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\n&nbsp;\r\n\r\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.\r\n',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(88,5,'2014-12-01 17:33:02','2014-12-01 17:33:02','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. A static entry mapping P4\'s MAC to its IP is added to the virtual bridge\'s mac-table.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API sores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ol>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet.</li>\n</ol>\n</li>\n</ol>\n\n&nbsp;\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups and FloatingIPs (not the port-masquerading kind) from this discussion. These are be the subject of  Part 3 of this series.','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-01 17:33:02','2014-12-01 17:33:02','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. A static entry mapping P4\'s MAC to its IP is added to the virtual bridge\'s mac-table.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API sores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ol>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet.</li>\r\n</ol>\r\n</li>\r\n</ol>\r\n&nbsp;\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups and FloatingIPs (not the port-masquerading kind) from this discussion. These are be the subject of  Part 3 of this series.',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(89,5,'2014-12-01 18:02:53','2014-12-01 18:02:53','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. A static entry mapping P4\'s MAC to its IP is added to the virtual bridge\'s mac-table.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API sores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-01 18:02:53','2014-12-01 18:02:53','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. A static entry mapping P4\'s MAC to its IP is added to the virtual bridge\'s mac-table.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API sores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(90,5,'2014-12-01 18:06:39','2014-12-01 18:06:39','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n[caption id=\"attachment_162\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\"><img class=\"wp-image-162 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\" alt=\"This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.\" width=\"415\" height=\"373\" /></a> This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.[/caption]\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\n[caption id=\"attachment_161\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\"><img class=\"wp-image-161 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\" alt=\"This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.[/caption]\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','publish','open','open','','introduction-mns-overlay-network-models-part-3-simulations','','http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\nhttp://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/','2014-12-04 17:27:55','2014-12-04 17:27:55','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\r\n<ul>\r\n	<li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\r\n	<li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\r\n	<li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\r\n<ul>\r\n	<li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\r\n	<li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\r\n	<li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\r\n	<li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\r\n	<li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\r\n	<li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\r\n</ol>\r\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\r\n\r\n&nbsp;\r\n\r\n[caption id=\"attachment_162\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\"><img class=\"wp-image-162 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\" alt=\"This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.\" width=\"415\" height=\"373\" /></a> This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.[/caption]\r\n\r\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\r\n<ol>\r\n	<li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\r\n	<li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\r\n	<li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\r\n	<li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\r\n<ul>\r\n	<li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\r\n</ul>\r\n</li>\r\n	<li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\r\n</ol>\r\n[caption id=\"attachment_161\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\"><img class=\"wp-image-161 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\" alt=\"This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.[/caption]\r\n\r\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\r\n\r\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".',0,'http://blog.midonet.org/?p=90',0,'post','',0),(91,5,'2014-12-01 18:06:39','2014-12-01 18:06:39','This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks. In this article we take a break from new models to talk about how the MN Agent <em>simulates</em> the overlay topology.\n\nOk, so far the VM has obtained its IP address and default route (and possibly some other info) via DHCP. We\'ll soon try to add some diagrams to illustrate all of this text. Also, if multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\n\n<ul>\n    <li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254</li>\n    <li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\n</ul>\n\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\n\n<ol>\n    <li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10 (corresponding to Network10). I haven\'t explained this yet, but MN\'s virtual bridge has a MAC-IP table that it uses for ARP proxying/spoofing - this table is seeded along with the DHCPSubnet object - and includes the MAC-IP of the connected virtual router.<em> Similarly, the virtual bridge\'s mac-table is seeded at the same time (when the Neutron port is created) because Neutron already knows the MAC of the VM that will be connected to that port.</em> The virtual bridge will therefore <em>consume</em> the ARP request and generate an ARP reply and emit it from P10. The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101. It would send issue a Netlink request to its datapath to do so. Nothing needs to be done for the ARP request because the datapath doesn\'t buffer missed packets.</li>\n    <li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10.</li>\n    <li>TO BE COMPLETED</li>\n</ol>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-01 18:06:39','2014-12-01 18:06:39','This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks. In this article we take a break from new models to talk about how the MN Agent <em>simulates</em> the overlay topology.\r\n\r\nOk, so far the VM has obtained its IP address and default route (and possibly some other info) via DHCP. We\'ll soon try to add some diagrams to illustrate all of this text. Also, if multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\r\n<ul>\r\n	<li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254</li>\r\n	<li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\r\n</ul>\r\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\r\n<ol>\r\n	<li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10 (corresponding to Network10). I haven\'t explained this yet, but MN\'s virtual bridge has a MAC-IP table that it uses for ARP proxying/spoofing - this table is seeded along with the DHCPSubnet object - and includes the MAC-IP of the connected virtual router.<em> Similarly, the virtual bridge\'s mac-table is seeded at the same time (when the Neutron port is created) because Neutron already knows the MAC of the VM that will be connected to that port.</em> The virtual bridge will therefore <em>consume</em> the ARP request and generate an ARP reply and emit it from P10. The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101. It would send issue a Netlink request to its datapath to do so. Nothing needs to be done for the ARP request because the datapath doesn\'t buffer missed packets.</li>\r\n	<li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10.</li>\r\n	<li>TO BE COMPLETED</li>\r\n</ol>\r\n&nbsp;',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(92,5,'2014-12-04 17:26:56','2014-12-04 17:26:56','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n&nbsp;\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-autosave-v1','','','2014-12-04 17:26:56','2014-12-04 17:26:56','',90,'http://blog.midonet.org/90-autosave-v1/',0,'revision','',0),(93,5,'2014-12-01 18:08:48','2014-12-01 18:08:48','In this article we discuss how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in Part 2, the VM has obtained its IP address and default route (and possibly some other info) via DHCP. We\'ll soon try to add some diagrams to illustrate all of this text (please stay tuned). If multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\n\n<ul>\n    <li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254</li>\n    <li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\n</ul>\n\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\n\n<ol>\n    <li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10 (corresponding to Network10). I haven\'t explained this yet, but MN\'s virtual bridge has a MAC-IP table that it uses for ARP proxying/spoofing - this table is seeded along with the DHCPSubnet object - and includes the MAC-IP of the connected virtual router.<em> Similarly, the virtual bridge\'s mac-table is seeded at the same time (when the Neutron port is created) because Neutron already knows the MAC of the VM that will be connected to that port.</em> The virtual bridge will therefore <em>consume</em> the ARP request and generate an ARP reply and emit it from P10. The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101. It would send issue a Netlink request to its datapath to do so. Nothing needs to be done for the ARP request because the datapath doesn\'t buffer missed packets.</li>\n    <li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10.</li>\n    <li>TO BE COMPLETED</li>\n</ol>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-01 18:08:48','2014-12-01 18:08:48','In this article we discuss how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in Part 2, the VM has obtained its IP address and default route (and possibly some other info) via DHCP. We\'ll soon try to add some diagrams to illustrate all of this text (please stay tuned). If multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\r\n<ul>\r\n	<li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254</li>\r\n	<li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\r\n</ul>\r\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\r\n<ol>\r\n	<li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10 (corresponding to Network10). I haven\'t explained this yet, but MN\'s virtual bridge has a MAC-IP table that it uses for ARP proxying/spoofing - this table is seeded along with the DHCPSubnet object - and includes the MAC-IP of the connected virtual router.<em> Similarly, the virtual bridge\'s mac-table is seeded at the same time (when the Neutron port is created) because Neutron already knows the MAC of the VM that will be connected to that port.</em> The virtual bridge will therefore <em>consume</em> the ARP request and generate an ARP reply and emit it from P10. The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101. It would send issue a Netlink request to its datapath to do so. Nothing needs to be done for the ARP request because the datapath doesn\'t buffer missed packets.</li>\r\n	<li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10.</li>\r\n	<li>TO BE COMPLETED</li>\r\n</ol>\r\n&nbsp;',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(94,5,'2014-12-06 20:20:22','2014-12-06 20:20:22','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\n<h2>Neutron Security Groups</h2>\n\nIn OpenStack the term \"Security Group\" refers to two concepts:\n\n<ul>\n    <li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\n    <li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\n</ul>\n\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\n\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\n\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\n\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\n\n<ol>\n    <li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\n    <li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\n    <li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\n    <li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\n</ol>\n\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\n\n<h2>MidoNet\'s Rule-Chains</h2>\n\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\n\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\n\n<ul>\n    <li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\n    <li>clauses matching the packet\'s ingress port on a specific virtual device</li>\n    <li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\n    <li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\n</ul>\n\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\n\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\n\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\n\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\n\n<h2>Security Groups in MidoNet</h2>\n\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\n\n<ul>\n    <li>Tenant A has two Security Groups:\n<ul>\n    <li>SG1 allows ICMP and HTTP traffic.</li>\n    <li>SG2 allows SSH traffic.</li>\n</ul>\n</li>\n    <li>Tenant A has two VMs:\n<ul>\n    <li>VM1 (with MAC1 and IP1) is bound to network Port1, which belongs to both Security Groups.</li>\n    <li>VM2 (with MAC2 and IP2) is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\n</ul>\n</li>\n</ul>\n\nThe following diagram shows how the <em>ingress</em> Security Group and anti-spoofing rules are translated and organized into MidoNet Rule-Chains.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"720\" height=\"540\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\nWhen a Security Group is created, e.g. SG1, MidoNet\'s low-level topology gets two Rule-Chains: SG1-Ingress will hold translations of the Group\'s <em>ingress</em> rules; SG2-Egress will hold translations of the Group\'s <em>egress</em> rules. When a <em>ingress</em> Security Group Rule is added, a corresponding low-level MN Rule is added to SG1-Ingress. Similarly, a new <em>egress</em> Security Group Rule results in a new MN Rule in SG1-Egress. A Security Group rule is translated to a MN Rule with a condition matching the same traffic and Action set to \"ACCEPT\".\n\nWhen Neutron Port1 is created, MidoNet\'s low-level model gets a new port on the MN virtual bridge corresponding to Port1\'s network. The MN port has two Rule-Chains: Port1-Ingress filters traffic to the VM; Port1-Egress filters traffic from the VM. These Chains are immediately populated with anti-spoofing rules (because the port\'s MAC and IP are already known).\n\nWhen a Security Group is assigned to a Neutron port, for example SG1 is assigned to Port1, the Port1-Ingress gets an unconditional rule whose JUMP action causes the flow of control (during Simulation) to pass to SG1-Ingress. Similarly, Port1-Egress gets an unconditional rule whose action is <em>Jump to SG1-Egress.</em>\n\nPutting this all together, since Port1 is assigned to both SG1 and SG2, the Rule-Chain named Port1-Ingress ends up with these rules:\n\n<ol>\n    <li>DROP traffic whose destination MAC is not MAC1</li>\n    <li>DROP traffic whose destination IP is not IP1</li>\n    <li>ACCEPT reply traffic</li>\n    <li>JUMP to SG1-Ingress</li>\n    <li>JUMP to SG2-Ingress</li>\n    <li>DROP (unconditional, drops everything that gets here)</li>\n</ol>\n\nNotice that Port1-Ingress and Port1-Egress are Rule-Chains that are only used by Port1. In contrast, SG1-Ingress and SG1-Egress are both potentially shared Rule-Chains. They will be <em>jumped-to</em> by rules in every port that belongs to Security Group SG1.\n\n<h2>Rule-Chain Simulation</h2>\n\n<a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> described the MN Agent\'s packet-processing logic, and particularly the Simulation stage. So here we\'ll just jump right into how Rule-Chains behave during Simulation. Assume an SSH packet (to IP1 and TCP port 22 and of course having missed in the kernel and user-space flow tables) is in the Simulation stage. Assume the Simulation has determined that the packet would arrive at the virtual bridge (corresponding to a Neutron network) and would be emitted from Port1. The Simulation would proceed to check how Port1\'s <em>outbound</em> (outbound, from the perspective of the bridge) filter would behave. This means evaluating the packet against the Rule-Chain named <em>Port1-Ingress</em> (the chain name reflect Neutron\'s VM-based perspective). Here\'s what the Simulation would do:\n\n<ol>\n    <li>Check Rule #1\'s condition against the packet. Assume the packet is for MAC1, so the condition does not match the packet. Move on to the next rule.</li>\n    <li>Check Rule #2\'s condition against the packet. The packet is for IP1, so the condition does not match. Move on to the next rule.</li>\n    <li>Check Rule #3\'s condition. This requires doing a lookup in the local flow-state cache using a key constructed by flipping the packet\'s source and destination IP addresses and L4 ports. Assume this is a new SSH connection, no flow-state is found. Move on to the next rule.</li>\n    <li>Rule #4 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\n    <li>Evaluate SG1\'s Rule #1\'s condition. That matches ICMP, this packet\'s SSH, so move on to the next rule.</li>\n    <li>Evaluate SG2\'s Rule #2\'s condition. That matches HTTP, this packet\'s SSH, so move on to the next rule.</li>\n    <li>There\'s no next rule, so return from SG1 to the calling JUMP rule. Move on to the next rule.</li>\n    <li>Rule #5 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\n    <li>Evaluate SG1\'s Rule #1\'s condition. That matches SSH and therefore applies to this packet. The Action is ACCEPT. Break out of Chain evaluation with result ACCEPT.</li>\n    <li>The Simulation moves on to next stage in the device\'s packet processing. For a pre-routing chain, the next stage would be routing. For a bridge port\'s Inbound filter, the next stage would be entering the bridge itself. In this case, an Outbound filter, the next stage is to actually emit the packet from the port (it wasn\'t dropped).</li>\n</ol>\n\nIt\'s not part of the Rule-Chain evaluation, but remember from <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> that since this is an <em>exterior</em> port, the Simulation terminates with the result that the original packet should have its headers modified to what they looked like during Port1\'s simulation (e.g. if they traversed a router, the L2 addresses would be modified) and then be emitted from Port1. The virtual/physical translation stage maps Port1 to the compute host where VM1 resides, e.g. Host5. If VM1 is remote, a datapath flow rule is installed to match all similar packets and emit them (<em>forward</em> them in OVS kmod\'s terminology) from a tunnel port with outer IP address set to Host5\'s IP. The tunnel key will be set to encode Port1\'s ID so that the MN Agent at Host5 can emit the packet/flow from the corresponding datapath port without doing a Simulation. The ingress host of a flow does the simulation and the egress host (the recipient of tunnel traffic) trusts the result.\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','publish','open','open','','introduction-to-mn-part-4-security-groups','','','2014-12-06 20:20:22','2014-12-06 20:20:22','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\n<h2>Neutron Security Groups</h2>\r\nIn OpenStack the term \"Security Group\" refers to two concepts:\r\n<ul>\r\n	<li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\r\n	<li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\r\n</ul>\r\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\r\n\r\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\r\n\r\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\r\n\r\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\r\n<ol>\r\n	<li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\r\n	<li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\r\n	<li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\r\n	<li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\r\n</ol>\r\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\r\n<h2>MidoNet\'s Rule-Chains</h2>\r\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\r\n\r\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\r\n<ul>\r\n	<li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\r\n	<li>clauses matching the packet\'s ingress port on a specific virtual device</li>\r\n	<li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\r\n	<li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\r\n</ul>\r\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\r\n\r\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\r\n\r\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\r\n\r\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\r\n<h2>Security Groups in MidoNet</h2>\r\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\r\n<ul>\r\n	<li>Tenant A has two Security Groups:\r\n<ul>\r\n	<li>SG1 allows ICMP and HTTP traffic.</li>\r\n	<li>SG2 allows SSH traffic.</li>\r\n</ul>\r\n</li>\r\n	<li>Tenant A has two VMs:\r\n<ul>\r\n	<li>VM1 (with MAC1 and IP1) is bound to network Port1, which belongs to both Security Groups.</li>\r\n	<li>VM2 (with MAC2 and IP2) is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe following diagram shows how the <em>ingress</em> Security Group and anti-spoofing rules are translated and organized into MidoNet Rule-Chains.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"720\" height=\"540\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\nWhen a Security Group is created, e.g. SG1, MidoNet\'s low-level topology gets two Rule-Chains: SG1-Ingress will hold translations of the Group\'s <em>ingress</em> rules; SG2-Egress will hold translations of the Group\'s <em>egress</em> rules. When a <em>ingress</em> Security Group Rule is added, a corresponding low-level MN Rule is added to SG1-Ingress. Similarly, a new <em>egress</em> Security Group Rule results in a new MN Rule in SG1-Egress. A Security Group rule is translated to a MN Rule with a condition matching the same traffic and Action set to \"ACCEPT\".\r\n\r\nWhen Neutron Port1 is created, MidoNet\'s low-level model gets a new port on the MN virtual bridge corresponding to Port1\'s network. The MN port has two Rule-Chains: Port1-Ingress filters traffic to the VM; Port1-Egress filters traffic from the VM. These Chains are immediately populated with anti-spoofing rules (because the port\'s MAC and IP are already known).\r\n\r\nWhen a Security Group is assigned to a Neutron port, for example SG1 is assigned to Port1, the Port1-Ingress gets an unconditional rule whose JUMP action causes the flow of control (during Simulation) to pass to SG1-Ingress. Similarly, Port1-Egress gets an unconditional rule whose action is <em>Jump to SG1-Egress.</em>\r\n\r\nPutting this all together, since Port1 is assigned to both SG1 and SG2, the Rule-Chain named Port1-Ingress ends up with these rules:\r\n<ol>\r\n	<li>DROP traffic whose destination MAC is not MAC1</li>\r\n	<li>DROP traffic whose destination IP is not IP1</li>\r\n	<li>ACCEPT reply traffic</li>\r\n	<li>JUMP to SG1-Ingress</li>\r\n	<li>JUMP to SG2-Ingress</li>\r\n	<li>DROP (unconditional, drops everything that gets here)</li>\r\n</ol>\r\nNotice that Port1-Ingress and Port1-Egress are Rule-Chains that are only used by Port1. In contrast, SG1-Ingress and SG1-Egress are both potentially shared Rule-Chains. They will be <em>jumped-to</em> by rules in every port that belongs to Security Group SG1.\r\n<h2>Rule-Chain Simulation</h2>\r\n<a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> described the MN Agent\'s packet-processing logic, and particularly the Simulation stage. So here we\'ll just jump right into how Rule-Chains behave during Simulation. Assume an SSH packet (to IP1 and TCP port 22 and of course having missed in the kernel and user-space flow tables) is in the Simulation stage. Assume the Simulation has determined that the packet would arrive at the virtual bridge (corresponding to a Neutron network) and would be emitted from Port1. The Simulation would proceed to check how Port1\'s <em>outbound</em> (outbound, from the perspective of the bridge) filter would behave. This means evaluating the packet against the Rule-Chain named <em>Port1-Ingress</em> (the chain name reflect Neutron\'s VM-based perspective). Here\'s what the Simulation would do:\r\n<ol>\r\n	<li>Check Rule #1\'s condition against the packet. Assume the packet is for MAC1, so the condition does not match the packet. Move on to the next rule.</li>\r\n	<li>Check Rule #2\'s condition against the packet. The packet is for IP1, so the condition does not match. Move on to the next rule.</li>\r\n	<li>Check Rule #3\'s condition. This requires doing a lookup in the local flow-state cache using a key constructed by flipping the packet\'s source and destination IP addresses and L4 ports. Assume this is a new SSH connection, no flow-state is found. Move on to the next rule.</li>\r\n	<li>Rule #4 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\r\n	<li>Evaluate SG1\'s Rule #1\'s condition. That matches ICMP, this packet\'s SSH, so move on to the next rule.</li>\r\n	<li>Evaluate SG2\'s Rule #2\'s condition. That matches HTTP, this packet\'s SSH, so move on to the next rule.</li>\r\n	<li>There\'s no next rule, so return from SG1 to the calling JUMP rule. Move on to the next rule.</li>\r\n	<li>Rule #5 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\r\n	<li>Evaluate SG1\'s Rule #1\'s condition. That matches SSH and therefore applies to this packet. The Action is ACCEPT. Break out of Chain evaluation with result ACCEPT.</li>\r\n	<li>The Simulation moves on to next stage in the device\'s packet processing. For a pre-routing chain, the next stage would be routing. For a bridge port\'s Inbound filter, the next stage would be entering the bridge itself. In this case, an Outbound filter, the next stage is to actually emit the packet from the port (it wasn\'t dropped).</li>\r\n</ol>\r\nIt\'s not part of the Rule-Chain evaluation, but remember from <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> that since this is an <em>exterior</em> port, the Simulation terminates with the result that the original packet should have its headers modified to what they looked like during Port1\'s simulation (e.g. if they traversed a router, the L2 addresses would be modified) and then be emitted from Port1. The virtual/physical translation stage maps Port1 to the compute host where VM1 resides, e.g. Host5. If VM1 is remote, a datapath flow rule is installed to match all similar packets and emit them (<em>forward</em> them in OVS kmod\'s terminology) from a tunnel port with outer IP address set to Host5\'s IP. The tunnel key will be set to encode Port1\'s ID so that the MN Agent at Host5 can emit the packet/flow from the corresponding datapath port without doing a Simulation. The ingress host of a flow does the simulation and the egress host (the recipient of tunnel traffic) trusts the result.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n&nbsp;',0,'http://blog.midonet.org/?p=94',0,'post','',0),(95,5,'2014-12-02 08:52:48','2014-12-02 08:52:48','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.','Introduction to MN\'s Overlay Network Models (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-02 08:52:48','2014-12-02 08:52:48','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(96,5,'2014-12-06 22:32:09','0000-00-00 00:00:00','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n</ul>\n\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\n\n<h2>Neutron Floating IPs</h2>\n\n&nbsp;\n\n&nbsp;\n\n<h2>Floating IPs in MidoNet</h2>\n\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\n\n&nbsp;\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 6 - Floating IPs)','','draft','open','open','','intro-to-mn-part-6-floatingips','','','2014-12-06 22:32:09','2014-12-06 22:32:09','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n</ul>\r\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\r\n<h2>Neutron Floating IPs</h2>\r\n&nbsp;\r\n\r\n&nbsp;\r\n<h2>Floating IPs in MidoNet</h2>\r\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\r\n\r\n&nbsp;\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',0,'http://blog.midonet.org/?p=96',0,'post','',0),(97,5,'2014-12-02 08:56:41','2014-12-02 08:56:41','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule. In <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.','Introduction to MN\'s Overlay Network Models (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-02 08:56:41','2014-12-02 08:56:41','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule. In <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(100,5,'2014-12-02 09:21:34','2014-12-02 09:21:34','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSGroups.jpg\"><img class=\"alignnone size-medium wp-image-99\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSGroups-300x225.jpg\" alt=\"MN_PortFilterAndSGroups\" width=\"300\" height=\"225\" /></a>','Introduction to MN\'s Overlay Network Models (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-02 09:21:34','2014-12-02 09:21:34','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSGroups.jpg\"><img class=\"alignnone size-medium wp-image-99\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSGroups-300x225.jpg\" alt=\"MN_PortFilterAndSGroups\" width=\"300\" height=\"225\" /></a>',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(101,5,'2014-12-02 09:30:15','2014-12-02 09:30:15','','MN_PortFilterAndSecGroups','','inherit','open','open','','mn_portfilterandsecgroups','','','2014-12-02 09:30:15','2014-12-02 09:30:15','',96,'http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg',0,'attachment','image/jpeg',0),(102,5,'2014-12-02 09:33:18','2014-12-02 09:33:18','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\n\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\n&nbsp;\n\n&nbsp;\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-02 09:33:18','2014-12-02 09:33:18','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\r\n\r\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(103,5,'2014-12-02 09:38:13','2014-12-02 09:38:13','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule. In <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-02 09:38:13','2014-12-02 09:38:13','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule. In <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(104,5,'2014-12-02 10:05:14','0000-00-00 00:00:00','This articles discusses MTU in MidoNet. MidoNet overlays are only capable of transporting Ethernet frames. We will discuss how MN operators should set the MTU of the VM instances in the overlay, of the external-facing virtual router and bridge ports (bound to non-tunnel physical NICs in L3 and L2 Gateway nodes), and of the tunnel-side NICs of the MidoNet hosts (Gateway nodes as well as Nova/Compute hosts).\n\n<h2><b>MTU In the Underlay</b></h2>\n\nSummary: the tunnel interface MTU directly determines the value of the the DHCP MTU option offered to DHCP clients\n\nFirst, note that MN works correctly with any (correctly configured) underlay MTU. The underlay MTU can be smaller than the overlay MTU because the sending/receiving hypervisor hosts will simply use IP fragmentation/reassembly to transmit/receive GRE/VXLAN packets larger than the underlay MTU.\n\nOf course, this is not efficient, so ideally the underlay MTU is large enough to accommodate both the overlay packet (whose max size is the overlay MTU) plus the tunnel encapsulation header.\n\nIn order to avoid IP fragmentation on the underlay, MN’s DHCP implementation sets the MTU option (in DHCP Offer and Ack messages):\n\n<ul>\n    <li>get the TunnelZoneHost information for the local host (the one responding to a locally generated DHCP Discover or Request message)</li>\n    <li>find the local interface corresponding to the TZH’s IP address. We call this the “tunnel interface”.</li>\n    <li>get the MTU of the tunnel interface: MTU-T</li>\n    <li>compute the overheads of all the supported tunnel protocols (currently GRE and VXLAN). Take the max of those.</li>\n    <li>set VM MTU to MTU-T plus the max overhead.</li>\n</ul>\n\nObservations:\n\n<ul>\n    <li>This approach is brittle because hosts with different tunnel interface MTUs will compute different DHCP MTU options for VMs in the same Neutron Network.</li>\n    <li>MidoNet only generates the DHCP message if the DHCP client’s MAC is known, i.e. it’s in the DHCPSubnet object.</li>\n</ul>\n\n<h2><b>MTU In the Overlay - VMs</b></h2>\n\nNote: MidoNet (as of v1.7) ignores the Neutron Subnet’s Extra DHCP Options (which allow Neutron clients to explicitly configure MTU among many other things).\n\nThe previous section explained how MN crafts the DHCP MTU option. It’s worth noting that MidoNet does not model MTUs on any virtual devices or ports. MidoNet’s overlay network is MTU-agnostic. Historically, this was based on the idea that the overlay network had virtual wires (especially those connecting two overlay/virtual devices that could all be unlimited MTU. <i>In retrospect, not modeling MTUs at the overlay network’s edge was naive and we are working on a fix.</i>\n\nThe operator should also be aware that since MN’s devices and ports are MTU-agnostic, MN does not check for packets whose size exceed the MTU implied by the overlay network edge (i.e. where it connects to the physical network and therefore has an MTU). The historical reason for this was that normally (except when testing with “ping -s”), the packet’s first flow is small. MN Agent sees the first packet only, and subsequent packets are handled in the flow datapath, which does not have support for differentiating treatment based on packet size.\n\nUnderstanding the following limitations is crucial for correct operation:\n\n<ul>\n    <li><b>MidoNet never generates ICMPv4 Type (Destination Unreachable) code 4 (fragmentation required but DF flag set).</b></li>\n    <li><b>MidoNet’s virtual routers do not implement IP Fragmentation and Reassembly</b> (because the flow datapath does not have support for fragmentation/reassembly).</li>\n    <li><b>MidoNet does not support MSS clamping</b>.</li>\n</ul>\n\nThese limitations lead to the following conclusions:\n\n<ul>\n    <li>MidoNet does not support PMTUD. Connecions between VM instances and the Internet are therefore vulnerable to “PMTU Black Hole”.</li>\n    <li>In the general case, MidoNet may not correctly forward traffic between two VMs with different MTUs.</li>\n    <li>TCP traffic between two VMs with different MTU should be fine: each of the VMs will set the MSS. The TCP packets from the VM with the higher MTU will respect the smaller MTU.</li>\n    <li>It’s best to set all the VMs in a MN deployment to the same MTU. If VMs are DHCP clients, then MN will set the DHCP MTU option to achieve this <i>as long as all the hypervisor host tunnel interfaces have the same MTU</i>.</li>\n</ul>\n\n<h2><b>On the Overlay edge - L3 and L2 Gateway NICs</b></h2>\n\nThe L2 and L3 Gateway NICs are edges of the virtual network symmetric to the VMs. However, the reason to discuss them separately is that VM’s MTU is usually managed by MidoNet whereas the gateway NIC MTUs are set by the operator AND we have often been bitten by forgetting to disable offload capabilities on the gateway NICs.\n\nIt’s best to disable all the Gateway NIC’s offload options. The offload features are intended to accelerate processing of network stacks. However, the Gateway NIC is the entry point to an entire network, so the offload optimizations don’t help. Compare this to physical routers - they might do IP fragmentation but don’t do TSO because they’re not the endpoint.\n\nIf nothing else, <b>you MUST disable GRO option on the gateway NICs</b>. Here’s how GRO can cause trouble:\n\n<ul>\n    <li>A burst of TCP packets in a single flow arrives at a NIC with GRO enabled.</li>\n    <li>The NIC coalesces the TCP packets into a single larger packet that exceeds the destination VM’s MTU.</li>\n    <li>The packet is tunneled from the gateway node to the egress host. Since it’s a large packet, it may need to be fragmented and reassembled (if so, the receive offload caused more work rather than less).</li>\n    <li>The egress host datapath blindly forwards the large packet to the VM’s NIC - the packet is dropped. Remember MN does not perform fragmentation or reassembly.</li>\n</ul>\n\nL2 Gateway NIC should have an MTU higher than the VMs (since the packets traversing these NICs will be VM packets with VLAN tags) so that it doesn’t drop tagged packets. But beyond that the L2 Gateway’s NIC doesn’t matter much: the virtual L2 Bridge doesn’t have an MTU. That said, the physical servers reachable via the L2 Gateway should have the same MTU as the VM instances they’re bridged to.\n\n<strong>We recommend VMs MTU to be set to the same MTU as Gateway NIC for the reasons listed above in the “MTU in the Overlay - VM” section. However, it’s possible to run all the VMs at some high MTU, e.g. 8850, derived from tunnel interfaces set at 9k MTU, while the L3 Gateway NIC’s MTU, and therefore the peer router’s, is set lower, e.g. at 1500. This works as long as the Gateway NIC’s MTU is not the smallest MTU on the path from VM to end-host on the Internet. Again, remember that MN’s Provider Router, that uses the NIC as its uplink, cannot generate ICMP destination unreachable frag needed. Therefore TCP connections are fine because the server outside the cloud sets a low MSS or because some non-MN router sends ICMP frag needed to the VM instances, which have a higher MTU. UDP connections are more affected, as PMTUD will not work as intended.</strong>','MTU in MidoNet','','draft','open','open','','','','','2014-12-02 10:05:14','2014-12-02 10:05:14','This articles discusses MTU in MidoNet. MidoNet overlays are only capable of transporting Ethernet frames. We will discuss how MN operators should set the MTU of the VM instances in the overlay, of the external-facing virtual router and bridge ports (bound to non-tunnel physical NICs in L3 and L2 Gateway nodes), and of the tunnel-side NICs of the MidoNet hosts (Gateway nodes as well as Nova/Compute hosts).\r\n<h2><b>MTU In the Underlay</b></h2>\r\nSummary: the tunnel interface MTU directly determines the value of the the DHCP MTU option offered to DHCP clients\r\n\r\nFirst, note that MN works correctly with any (correctly configured) underlay MTU. The underlay MTU can be smaller than the overlay MTU because the sending/receiving hypervisor hosts will simply use IP fragmentation/reassembly to transmit/receive GRE/VXLAN packets larger than the underlay MTU.\r\n\r\nOf course, this is not efficient, so ideally the underlay MTU is large enough to accommodate both the overlay packet (whose max size is the overlay MTU) plus the tunnel encapsulation header.\r\n\r\nIn order to avoid IP fragmentation on the underlay, MN’s DHCP implementation sets the MTU option (in DHCP Offer and Ack messages):\r\n<ul>\r\n	<li>get the TunnelZoneHost information for the local host (the one responding to a locally generated DHCP Discover or Request message)</li>\r\n	<li>find the local interface corresponding to the TZH’s IP address. We call this the “tunnel interface”.</li>\r\n	<li>get the MTU of the tunnel interface: MTU-T</li>\r\n	<li>compute the overheads of all the supported tunnel protocols (currently GRE and VXLAN). Take the max of those.</li>\r\n	<li>set VM MTU to MTU-T plus the max overhead.</li>\r\n</ul>\r\nObservations:\r\n<ul>\r\n	<li>This approach is brittle because hosts with different tunnel interface MTUs will compute different DHCP MTU options for VMs in the same Neutron Network.</li>\r\n	<li>MidoNet only generates the DHCP message if the DHCP client’s MAC is known, i.e. it’s in the DHCPSubnet object.</li>\r\n</ul>\r\n<h2><b>MTU In the Overlay - VMs</b></h2>\r\nNote: MidoNet (as of v1.7) ignores the Neutron Subnet’s Extra DHCP Options (which allow Neutron clients to explicitly configure MTU among many other things).\r\n\r\nThe previous section explained how MN crafts the DHCP MTU option. It’s worth noting that MidoNet does not model MTUs on any virtual devices or ports. MidoNet’s overlay network is MTU-agnostic. Historically, this was based on the idea that the overlay network had virtual wires (especially those connecting two overlay/virtual devices that could all be unlimited MTU. <i>In retrospect, not modeling MTUs at the overlay network’s edge was naive and we are working on a fix.</i>\r\n\r\nThe operator should also be aware that since MN’s devices and ports are MTU-agnostic, MN does not check for packets whose size exceed the MTU implied by the overlay network edge (i.e. where it connects to the physical network and therefore has an MTU). The historical reason for this was that normally (except when testing with “ping -s”), the packet’s first flow is small. MN Agent sees the first packet only, and subsequent packets are handled in the flow datapath, which does not have support for differentiating treatment based on packet size.\r\n\r\nUnderstanding the following limitations is crucial for correct operation:\r\n<ul>\r\n	<li><b>MidoNet never generates ICMPv4 Type (Destination Unreachable) code 4 (fragmentation required but DF flag set).</b></li>\r\n	<li><b>MidoNet’s virtual routers do not implement IP Fragmentation and Reassembly</b> (because the flow datapath does not have support for fragmentation/reassembly).</li>\r\n	<li><b>MidoNet does not support MSS clamping</b>.</li>\r\n</ul>\r\nThese limitations lead to the following conclusions:\r\n<ul>\r\n	<li>MidoNet does not support PMTUD. Connecions between VM instances and the Internet are therefore vulnerable to “PMTU Black Hole”.</li>\r\n	<li>In the general case, MidoNet may not correctly forward traffic between two VMs with different MTUs.</li>\r\n	<li>TCP traffic between two VMs with different MTU should be fine: each of the VMs will set the MSS. The TCP packets from the VM with the higher MTU will respect the smaller MTU.</li>\r\n	<li>It’s best to set all the VMs in a MN deployment to the same MTU. If VMs are DHCP clients, then MN will set the DHCP MTU option to achieve this <i>as long as all the hypervisor host tunnel interfaces have the same MTU</i>.</li>\r\n</ul>\r\n<h2><b>On the Overlay edge - L3 and L2 Gateway NICs</b></h2>\r\nThe L2 and L3 Gateway NICs are edges of the virtual network symmetric to the VMs. However, the reason to discuss them separately is that VM’s MTU is usually managed by MidoNet whereas the gateway NIC MTUs are set by the operator AND we have often been bitten by forgetting to disable offload capabilities on the gateway NICs.\r\n\r\nIt’s best to disable all the Gateway NIC’s offload options. The offload features are intended to accelerate processing of network stacks. However, the Gateway NIC is the entry point to an entire network, so the offload optimizations don’t help. Compare this to physical routers - they might do IP fragmentation but don’t do TSO because they’re not the endpoint.\r\n\r\nIf nothing else, <b>you MUST disable GRO option on the gateway NICs</b>. Here’s how GRO can cause trouble:\r\n<ul>\r\n	<li>A burst of TCP packets in a single flow arrives at a NIC with GRO enabled.</li>\r\n	<li>The NIC coalesces the TCP packets into a single larger packet that exceeds the destination VM’s MTU.</li>\r\n	<li>The packet is tunneled from the gateway node to the egress host. Since it’s a large packet, it may need to be fragmented and reassembled (if so, the receive offload caused more work rather than less).</li>\r\n	<li>The egress host datapath blindly forwards the large packet to the VM’s NIC - the packet is dropped. Remember MN does not perform fragmentation or reassembly.</li>\r\n</ul>\r\nL2 Gateway NIC should have an MTU higher than the VMs (since the packets traversing these NICs will be VM packets with VLAN tags) so that it doesn’t drop tagged packets. But beyond that the L2 Gateway’s NIC doesn’t matter much: the virtual L2 Bridge doesn’t have an MTU. That said, the physical servers reachable via the L2 Gateway should have the same MTU as the VM instances they’re bridged to.\r\n\r\n<strong>We recommend VMs MTU to be set to the same MTU as Gateway NIC for the reasons listed above in the “MTU in the Overlay - VM” section. However, it’s possible to run all the VMs at some high MTU, e.g. 8850, derived from tunnel interfaces set at 9k MTU, while the L3 Gateway NIC’s MTU, and therefore the peer router’s, is set lower, e.g. at 1500. This works as long as the Gateway NIC’s MTU is not the smallest MTU on the path from VM to end-host on the Internet. Again, remember that MN’s Provider Router, that uses the NIC as its uplink, cannot generate ICMP destination unreachable frag needed. Therefore TCP connections are fine because the server outside the cloud sets a low MSS or because some non-MN router sends ICMP frag needed to the VM instances, which have a higher MTU. UDP connections are more affected, as PMTUD will not work as intended.</strong>',0,'http://blog.midonet.org/?p=104',0,'post','',0),(105,5,'2014-12-02 09:41:57','2014-12-02 09:41:57','','MTU in MidoNet','','inherit','open','open','','104-revision-v1','','','2014-12-02 09:41:57','2014-12-02 09:41:57','',104,'http://blog.midonet.org/104-revision-v1/',0,'revision','',0),(106,5,'2014-12-02 09:43:42','2014-12-02 09:43:42','<h2><b>MTU In the Underlay</b></h2>\n\nSummary: the tunnel interface MTU directly determines the value of the the DHCP MTU option offered to DHCP clients\n\nFirst, note that MN works correctly with any (correctly configured) underlay MTU. The underlay MTU can be smaller than the overlay MTU because the sending/receiving hypervisor hosts will simply use IP fragmentation/reassembly to transmit/receive GRE/VXLAN packets larger than the underlay MTU.\n\nOf course, this is not efficient, so ideally the underlay MTU is large enough to accommodate both the overlay packet (whose max size is the overlay MTU) plus the tunnel encapsulation header.\n\nIn order to avoid IP fragmentation on the underlay, MN’s DHCP implementation sets the MTU option (in DHCP Offer and Ack messages):\n\n<ul>\n    <li>get the TunnelZoneHost information for the local host (the one responding to a locally generated DHCP Discover or Request message)</li>\n    <li>find the local interface corresponding to the TZH’s IP address. We call this the “tunnel interface”.</li>\n    <li>get the MTU of the tunnel interface: MTU-T</li>\n    <li>compute the overheads of all the supported tunnel protocols (currently GRE and VXLAN). Take the max of those.</li>\n    <li>set VM MTU to MTU-T plus the max overhead.</li>\n</ul>\n\nObservations:\n\n<ul>\n    <li>This approach is brittle because hosts with different tunnel interface MTUs will compute different DHCP MTU options for VMs in the same Neutron Network.</li>\n    <li>MidoNet only generates the DHCP message if the DHCP client’s MAC is known, i.e. it’s in the DHCPSubnet object.</li>\n</ul>\n\n<h2><b>MTU In the Overlay - VMs</b></h2>\n\nNote: MidoNet (as of v1.7) ignores the Neutron Subnet’s Extra DHCP Options (which allow Neutron clients to explicitly configure MTU among many other things).\n\nThe previous section explained how MN crafts the DHCP MTU option. It’s worth noting that MidoNet does not model MTUs on any virtual devices or ports. MidoNet’s overlay network is MTU-agnostic. Historically, this was based on the idea that the overlay network had virtual wires (especially those connecting two overlay/virtual devices that could all be unlimited MTU. <i>In retrospect, not modeling MTUs at the overlay network’s edge was naive and we are working on a fix.</i>\n\nThe operator should also be aware that since MN’s devices and ports are MTU-agnostic, MN does not check for packets whose size exceed the MTU implied by the overlay network edge (i.e. where it connects to the physical network and therefore has an MTU). The historical reason for this was that normally (except when testing with “ping -s”), the packet’s first flow is small. MN Agent sees the first packet only, and subsequent packets are handled in the flow datapath, which does not have support for differentiating treatment based on packet size.\n\nUnderstanding the following limitations is crucial for correct operation:\n\n<ul>\n    <li><b>MidoNet never generates ICMPv4 Type (Destination Unreachable) code 4 (fragmentation required but DF flag set).</b></li>\n    <li><b>MidoNet’s virtual routers do not implement IP Fragmentation and Reassembly</b> (because the flow datapath does not have support for fragmentation/reassembly).</li>\n    <li><b>MidoNet does not support MSS clamping</b>.</li>\n</ul>\n\nThese limitations lead to the following conclusions:\n\n<ul>\n    <li>MidoNet does not support PMTUD. Connecions between VM instances and the Internet are therefore vulnerable to “PMTU Black Hole”.</li>\n    <li>In the general case, MidoNet may not correctly forward traffic between two VMs with different MTUs.</li>\n    <li>TCP traffic between two VMs with different MTU should be fine: each of the VMs will set the MSS. The TCP packets from the VM with the higher MTU will respect the smaller MTU.</li>\n    <li>It’s best to set all the VMs in a MN deployment to the same MTU. If VMs are DHCP clients, then MN will set the DHCP MTU option to achieve this <i>as long as all the hypervisor host tunnel interfaces have the same MTU</i>.</li>\n</ul>\n\n<h2><b>On the Overlay edge - L3 and L2 Gateway NICs</b></h2>\n\nThe L2 and L3 Gateway NICs are edges of the virtual network symmetric to the VMs. However, the reason to discuss them separately is that VM’s MTU is usually managed by MidoNet whereas the gateway NIC MTUs are set by the operator AND we have often been bitten by forgetting to disable offload capabilities on the gateway NICs.\n\nIt’s best to disable all the Gateway NIC’s offload options. The offload features are intended to accelerate processing of network stacks. However, the Gateway NIC is the entry point to an entire network, so the offload optimizations don’t help. Compare this to physical routers - they might do IP fragmentation but don’t do TSO because they’re not the endpoint.\n\nIf nothing else, <b>you MUST disable GRO option on the gateway NICs</b>. Here’s how GRO can cause trouble:\n\n<ul>\n    <li>A burst of TCP packets in a single flow arrives at a NIC with GRO enabled.</li>\n    <li>The NIC coalesces the TCP packets into a single larger packet that exceeds the destination VM’s MTU.</li>\n    <li>The packet is tunneled from the gateway node to the egress host. Since it’s a large packet, it may need to be fragmented and reassembled (if so, the receive offload caused more work rather than less).</li>\n    <li>The egress host datapath blindly forwards the large packet to the VM’s NIC - the packet is dropped. Remember MN does not perform fragmentation or reassembly.</li>\n</ul>\n\nL2 Gateway NIC should have an MTU higher than the VMs (since the packets traversing these NICs will be VM packets with VLAN tags) so that it doesn’t drop tagged packets. But beyond that the L2 Gateway’s NIC doesn’t matter much: the virtual L2 Bridge doesn’t have an MTU. That said, the physical servers reachable via the L2 Gateway should have the same MTU as the VM instances they’re bridged to.\n\n<strong>We recommend VMs MTU to be set to the same MTU as Gateway NIC for the reasons listed above in the “MTU in the Overlay - VM” section. However, it’s possible to run all the VMs at some high MTU, e.g. 8850, derived from tunnel interfaces set at 9k MTU, while the L3 Gateway NIC’s MTU, and therefore the peer router’s, is set lower, e.g. at 1500. This works as long as the Gateway NIC’s MTU is not the smallest MTU on the path from VM to end-host on the Internet. Again, remember that MN’s Provider Router, that uses the NIC as its uplink, cannot generate ICMP destination unreachable frag needed. Therefore TCP connections are fine because the server outside the cloud sets a low MSS or because some non-MN router sends ICMP frag needed to the VM instances, which have a higher MTU. UDP connections are more affected, as PMTUD will not work as intended.</strong>','MTU in MidoNet','','inherit','open','open','','104-revision-v1','','','2014-12-02 09:43:42','2014-12-02 09:43:42','<h2><b>MTU In the Underlay</b></h2>\r\nSummary: the tunnel interface MTU directly determines the value of the the DHCP MTU option offered to DHCP clients\r\n\r\nFirst, note that MN works correctly with any (correctly configured) underlay MTU. The underlay MTU can be smaller than the overlay MTU because the sending/receiving hypervisor hosts will simply use IP fragmentation/reassembly to transmit/receive GRE/VXLAN packets larger than the underlay MTU.\r\n\r\nOf course, this is not efficient, so ideally the underlay MTU is large enough to accommodate both the overlay packet (whose max size is the overlay MTU) plus the tunnel encapsulation header.\r\n\r\nIn order to avoid IP fragmentation on the underlay, MN’s DHCP implementation sets the MTU option (in DHCP Offer and Ack messages):\r\n<ul>\r\n	<li>get the TunnelZoneHost information for the local host (the one responding to a locally generated DHCP Discover or Request message)</li>\r\n	<li>find the local interface corresponding to the TZH’s IP address. We call this the “tunnel interface”.</li>\r\n	<li>get the MTU of the tunnel interface: MTU-T</li>\r\n	<li>compute the overheads of all the supported tunnel protocols (currently GRE and VXLAN). Take the max of those.</li>\r\n	<li>set VM MTU to MTU-T plus the max overhead.</li>\r\n</ul>\r\nObservations:\r\n<ul>\r\n	<li>This approach is brittle because hosts with different tunnel interface MTUs will compute different DHCP MTU options for VMs in the same Neutron Network.</li>\r\n	<li>MidoNet only generates the DHCP message if the DHCP client’s MAC is known, i.e. it’s in the DHCPSubnet object.</li>\r\n</ul>\r\n<h2><b>MTU In the Overlay - VMs</b></h2>\r\nNote: MidoNet (as of v1.7) ignores the Neutron Subnet’s Extra DHCP Options (which allow Neutron clients to explicitly configure MTU among many other things).\r\n\r\nThe previous section explained how MN crafts the DHCP MTU option. It’s worth noting that MidoNet does not model MTUs on any virtual devices or ports. MidoNet’s overlay network is MTU-agnostic. Historically, this was based on the idea that the overlay network had virtual wires (especially those connecting two overlay/virtual devices that could all be unlimited MTU. <i>In retrospect, not modeling MTUs at the overlay network’s edge was naive and we are working on a fix.</i>\r\n\r\nThe operator should also be aware that since MN’s devices and ports are MTU-agnostic, MN does not check for packets whose size exceed the MTU implied by the overlay network edge (i.e. where it connects to the physical network and therefore has an MTU). The historical reason for this was that normally (except when testing with “ping -s”), the packet’s first flow is small. MN Agent sees the first packet only, and subsequent packets are handled in the flow datapath, which does not have support for differentiating treatment based on packet size.\r\n\r\nUnderstanding the following limitations is crucial for correct operation:\r\n<ul>\r\n	<li><b>MidoNet never generates ICMPv4 Type (Destination Unreachable) code 4 (fragmentation required but DF flag set).</b></li>\r\n	<li><b>MidoNet’s virtual routers do not implement IP Fragmentation and Reassembly</b> (because the flow datapath does not have support for fragmentation/reassembly).</li>\r\n	<li><b>MidoNet does not support MSS clamping</b>.</li>\r\n</ul>\r\nThese limitations lead to the following conclusions:\r\n<ul>\r\n	<li>MidoNet does not support PMTUD. Connecions between VM instances and the Internet are therefore vulnerable to “PMTU Black Hole”.</li>\r\n	<li>In the general case, MidoNet may not correctly forward traffic between two VMs with different MTUs.</li>\r\n	<li>TCP traffic between two VMs with different MTU should be fine: each of the VMs will set the MSS. The TCP packets from the VM with the higher MTU will respect the smaller MTU.</li>\r\n	<li>It’s best to set all the VMs in a MN deployment to the same MTU. If VMs are DHCP clients, then MN will set the DHCP MTU option to achieve this <i>as long as all the hypervisor host tunnel interfaces have the same MTU</i>.</li>\r\n</ul>\r\n<h2><b>On the Overlay edge - L3 and L2 Gateway NICs</b></h2>\r\nThe L2 and L3 Gateway NICs are edges of the virtual network symmetric to the VMs. However, the reason to discuss them separately is that VM’s MTU is usually managed by MidoNet whereas the gateway NIC MTUs are set by the operator AND we have often been bitten by forgetting to disable offload capabilities on the gateway NICs.\r\n\r\nIt’s best to disable all the Gateway NIC’s offload options. The offload features are intended to accelerate processing of network stacks. However, the Gateway NIC is the entry point to an entire network, so the offload optimizations don’t help. Compare this to physical routers - they might do IP fragmentation but don’t do TSO because they’re not the endpoint.\r\n\r\nIf nothing else, <b>you MUST disable GRO option on the gateway NICs</b>. Here’s how GRO can cause trouble:\r\n<ul>\r\n	<li>A burst of TCP packets in a single flow arrives at a NIC with GRO enabled.</li>\r\n	<li>The NIC coalesces the TCP packets into a single larger packet that exceeds the destination VM’s MTU.</li>\r\n	<li>The packet is tunneled from the gateway node to the egress host. Since it’s a large packet, it may need to be fragmented and reassembled (if so, the receive offload caused more work rather than less).</li>\r\n	<li>The egress host datapath blindly forwards the large packet to the VM’s NIC - the packet is dropped. Remember MN does not perform fragmentation or reassembly.</li>\r\n</ul>\r\nL2 Gateway NIC should have an MTU higher than the VMs (since the packets traversing these NICs will be VM packets with VLAN tags) so that it doesn’t drop tagged packets. But beyond that the L2 Gateway’s NIC doesn’t matter much: the virtual L2 Bridge doesn’t have an MTU. That said, the physical servers reachable via the L2 Gateway should have the same MTU as the VM instances they’re bridged to.\r\n\r\n<strong>We recommend VMs MTU to be set to the same MTU as Gateway NIC for the reasons listed above in the “MTU in the Overlay - VM” section. However, it’s possible to run all the VMs at some high MTU, e.g. 8850, derived from tunnel interfaces set at 9k MTU, while the L3 Gateway NIC’s MTU, and therefore the peer router’s, is set lower, e.g. at 1500. This works as long as the Gateway NIC’s MTU is not the smallest MTU on the path from VM to end-host on the Internet. Again, remember that MN’s Provider Router, that uses the NIC as its uplink, cannot generate ICMP destination unreachable frag needed. Therefore TCP connections are fine because the server outside the cloud sets a low MSS or because some non-MN router sends ICMP frag needed to the VM instances, which have a higher MTU. UDP connections are more affected, as PMTUD will not work as intended.</strong>',104,'http://blog.midonet.org/104-revision-v1/',0,'revision','',0),(107,5,'2014-12-02 09:44:38','2014-12-02 09:44:38','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\n\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-02 09:44:38','2014-12-02 09:44:38','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\r\n\r\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(108,5,'2014-12-02 10:05:14','2014-12-02 10:05:14','This articles discusses MTU in MidoNet. MidoNet overlays are only capable of transporting Ethernet frames. We will discuss how MN operators should set the MTU of the VM instances in the overlay, of the external-facing virtual router and bridge ports (bound to non-tunnel physical NICs in L3 and L2 Gateway nodes), and of the tunnel-side NICs of the MidoNet hosts (Gateway nodes as well as Nova/Compute hosts).\n\n<h2><b>MTU In the Underlay</b></h2>\n\nSummary: the tunnel interface MTU directly determines the value of the the DHCP MTU option offered to DHCP clients\n\nFirst, note that MN works correctly with any (correctly configured) underlay MTU. The underlay MTU can be smaller than the overlay MTU because the sending/receiving hypervisor hosts will simply use IP fragmentation/reassembly to transmit/receive GRE/VXLAN packets larger than the underlay MTU.\n\nOf course, this is not efficient, so ideally the underlay MTU is large enough to accommodate both the overlay packet (whose max size is the overlay MTU) plus the tunnel encapsulation header.\n\nIn order to avoid IP fragmentation on the underlay, MN’s DHCP implementation sets the MTU option (in DHCP Offer and Ack messages):\n\n<ul>\n    <li>get the TunnelZoneHost information for the local host (the one responding to a locally generated DHCP Discover or Request message)</li>\n    <li>find the local interface corresponding to the TZH’s IP address. We call this the “tunnel interface”.</li>\n    <li>get the MTU of the tunnel interface: MTU-T</li>\n    <li>compute the overheads of all the supported tunnel protocols (currently GRE and VXLAN). Take the max of those.</li>\n    <li>set VM MTU to MTU-T plus the max overhead.</li>\n</ul>\n\nObservations:\n\n<ul>\n    <li>This approach is brittle because hosts with different tunnel interface MTUs will compute different DHCP MTU options for VMs in the same Neutron Network.</li>\n    <li>MidoNet only generates the DHCP message if the DHCP client’s MAC is known, i.e. it’s in the DHCPSubnet object.</li>\n</ul>\n\n<h2><b>MTU In the Overlay - VMs</b></h2>\n\nNote: MidoNet (as of v1.7) ignores the Neutron Subnet’s Extra DHCP Options (which allow Neutron clients to explicitly configure MTU among many other things).\n\nThe previous section explained how MN crafts the DHCP MTU option. It’s worth noting that MidoNet does not model MTUs on any virtual devices or ports. MidoNet’s overlay network is MTU-agnostic. Historically, this was based on the idea that the overlay network had virtual wires (especially those connecting two overlay/virtual devices that could all be unlimited MTU. <i>In retrospect, not modeling MTUs at the overlay network’s edge was naive and we are working on a fix.</i>\n\nThe operator should also be aware that since MN’s devices and ports are MTU-agnostic, MN does not check for packets whose size exceed the MTU implied by the overlay network edge (i.e. where it connects to the physical network and therefore has an MTU). The historical reason for this was that normally (except when testing with “ping -s”), the packet’s first flow is small. MN Agent sees the first packet only, and subsequent packets are handled in the flow datapath, which does not have support for differentiating treatment based on packet size.\n\nUnderstanding the following limitations is crucial for correct operation:\n\n<ul>\n    <li><b>MidoNet never generates ICMPv4 Type (Destination Unreachable) code 4 (fragmentation required but DF flag set).</b></li>\n    <li><b>MidoNet’s virtual routers do not implement IP Fragmentation and Reassembly</b> (because the flow datapath does not have support for fragmentation/reassembly).</li>\n    <li><b>MidoNet does not support MSS clamping</b>.</li>\n</ul>\n\nThese limitations lead to the following conclusions:\n\n<ul>\n    <li>MidoNet does not support PMTUD. Connecions between VM instances and the Internet are therefore vulnerable to “PMTU Black Hole”.</li>\n    <li>In the general case, MidoNet may not correctly forward traffic between two VMs with different MTUs.</li>\n    <li>TCP traffic between two VMs with different MTU should be fine: each of the VMs will set the MSS. The TCP packets from the VM with the higher MTU will respect the smaller MTU.</li>\n    <li>It’s best to set all the VMs in a MN deployment to the same MTU. If VMs are DHCP clients, then MN will set the DHCP MTU option to achieve this <i>as long as all the hypervisor host tunnel interfaces have the same MTU</i>.</li>\n</ul>\n\n<h2><b>On the Overlay edge - L3 and L2 Gateway NICs</b></h2>\n\nThe L2 and L3 Gateway NICs are edges of the virtual network symmetric to the VMs. However, the reason to discuss them separately is that VM’s MTU is usually managed by MidoNet whereas the gateway NIC MTUs are set by the operator AND we have often been bitten by forgetting to disable offload capabilities on the gateway NICs.\n\nIt’s best to disable all the Gateway NIC’s offload options. The offload features are intended to accelerate processing of network stacks. However, the Gateway NIC is the entry point to an entire network, so the offload optimizations don’t help. Compare this to physical routers - they might do IP fragmentation but don’t do TSO because they’re not the endpoint.\n\nIf nothing else, <b>you MUST disable GRO option on the gateway NICs</b>. Here’s how GRO can cause trouble:\n\n<ul>\n    <li>A burst of TCP packets in a single flow arrives at a NIC with GRO enabled.</li>\n    <li>The NIC coalesces the TCP packets into a single larger packet that exceeds the destination VM’s MTU.</li>\n    <li>The packet is tunneled from the gateway node to the egress host. Since it’s a large packet, it may need to be fragmented and reassembled (if so, the receive offload caused more work rather than less).</li>\n    <li>The egress host datapath blindly forwards the large packet to the VM’s NIC - the packet is dropped. Remember MN does not perform fragmentation or reassembly.</li>\n</ul>\n\nL2 Gateway NIC should have an MTU higher than the VMs (since the packets traversing these NICs will be VM packets with VLAN tags) so that it doesn’t drop tagged packets. But beyond that the L2 Gateway’s NIC doesn’t matter much: the virtual L2 Bridge doesn’t have an MTU. That said, the physical servers reachable via the L2 Gateway should have the same MTU as the VM instances they’re bridged to.\n\n<strong>We recommend VMs MTU to be set to the same MTU as Gateway NIC for the reasons listed above in the “MTU in the Overlay - VM” section. However, it’s possible to run all the VMs at some high MTU, e.g. 8850, derived from tunnel interfaces set at 9k MTU, while the L3 Gateway NIC’s MTU, and therefore the peer router’s, is set lower, e.g. at 1500. This works as long as the Gateway NIC’s MTU is not the smallest MTU on the path from VM to end-host on the Internet. Again, remember that MN’s Provider Router, that uses the NIC as its uplink, cannot generate ICMP destination unreachable frag needed. Therefore TCP connections are fine because the server outside the cloud sets a low MSS or because some non-MN router sends ICMP frag needed to the VM instances, which have a higher MTU. UDP connections are more affected, as PMTUD will not work as intended.</strong>','MTU in MidoNet','','inherit','open','open','','104-revision-v1','','','2014-12-02 10:05:14','2014-12-02 10:05:14','This articles discusses MTU in MidoNet. MidoNet overlays are only capable of transporting Ethernet frames. We will discuss how MN operators should set the MTU of the VM instances in the overlay, of the external-facing virtual router and bridge ports (bound to non-tunnel physical NICs in L3 and L2 Gateway nodes), and of the tunnel-side NICs of the MidoNet hosts (Gateway nodes as well as Nova/Compute hosts).\r\n<h2><b>MTU In the Underlay</b></h2>\r\nSummary: the tunnel interface MTU directly determines the value of the the DHCP MTU option offered to DHCP clients\r\n\r\nFirst, note that MN works correctly with any (correctly configured) underlay MTU. The underlay MTU can be smaller than the overlay MTU because the sending/receiving hypervisor hosts will simply use IP fragmentation/reassembly to transmit/receive GRE/VXLAN packets larger than the underlay MTU.\r\n\r\nOf course, this is not efficient, so ideally the underlay MTU is large enough to accommodate both the overlay packet (whose max size is the overlay MTU) plus the tunnel encapsulation header.\r\n\r\nIn order to avoid IP fragmentation on the underlay, MN’s DHCP implementation sets the MTU option (in DHCP Offer and Ack messages):\r\n<ul>\r\n	<li>get the TunnelZoneHost information for the local host (the one responding to a locally generated DHCP Discover or Request message)</li>\r\n	<li>find the local interface corresponding to the TZH’s IP address. We call this the “tunnel interface”.</li>\r\n	<li>get the MTU of the tunnel interface: MTU-T</li>\r\n	<li>compute the overheads of all the supported tunnel protocols (currently GRE and VXLAN). Take the max of those.</li>\r\n	<li>set VM MTU to MTU-T plus the max overhead.</li>\r\n</ul>\r\nObservations:\r\n<ul>\r\n	<li>This approach is brittle because hosts with different tunnel interface MTUs will compute different DHCP MTU options for VMs in the same Neutron Network.</li>\r\n	<li>MidoNet only generates the DHCP message if the DHCP client’s MAC is known, i.e. it’s in the DHCPSubnet object.</li>\r\n</ul>\r\n<h2><b>MTU In the Overlay - VMs</b></h2>\r\nNote: MidoNet (as of v1.7) ignores the Neutron Subnet’s Extra DHCP Options (which allow Neutron clients to explicitly configure MTU among many other things).\r\n\r\nThe previous section explained how MN crafts the DHCP MTU option. It’s worth noting that MidoNet does not model MTUs on any virtual devices or ports. MidoNet’s overlay network is MTU-agnostic. Historically, this was based on the idea that the overlay network had virtual wires (especially those connecting two overlay/virtual devices that could all be unlimited MTU. <i>In retrospect, not modeling MTUs at the overlay network’s edge was naive and we are working on a fix.</i>\r\n\r\nThe operator should also be aware that since MN’s devices and ports are MTU-agnostic, MN does not check for packets whose size exceed the MTU implied by the overlay network edge (i.e. where it connects to the physical network and therefore has an MTU). The historical reason for this was that normally (except when testing with “ping -s”), the packet’s first flow is small. MN Agent sees the first packet only, and subsequent packets are handled in the flow datapath, which does not have support for differentiating treatment based on packet size.\r\n\r\nUnderstanding the following limitations is crucial for correct operation:\r\n<ul>\r\n	<li><b>MidoNet never generates ICMPv4 Type (Destination Unreachable) code 4 (fragmentation required but DF flag set).</b></li>\r\n	<li><b>MidoNet’s virtual routers do not implement IP Fragmentation and Reassembly</b> (because the flow datapath does not have support for fragmentation/reassembly).</li>\r\n	<li><b>MidoNet does not support MSS clamping</b>.</li>\r\n</ul>\r\nThese limitations lead to the following conclusions:\r\n<ul>\r\n	<li>MidoNet does not support PMTUD. Connecions between VM instances and the Internet are therefore vulnerable to “PMTU Black Hole”.</li>\r\n	<li>In the general case, MidoNet may not correctly forward traffic between two VMs with different MTUs.</li>\r\n	<li>TCP traffic between two VMs with different MTU should be fine: each of the VMs will set the MSS. The TCP packets from the VM with the higher MTU will respect the smaller MTU.</li>\r\n	<li>It’s best to set all the VMs in a MN deployment to the same MTU. If VMs are DHCP clients, then MN will set the DHCP MTU option to achieve this <i>as long as all the hypervisor host tunnel interfaces have the same MTU</i>.</li>\r\n</ul>\r\n<h2><b>On the Overlay edge - L3 and L2 Gateway NICs</b></h2>\r\nThe L2 and L3 Gateway NICs are edges of the virtual network symmetric to the VMs. However, the reason to discuss them separately is that VM’s MTU is usually managed by MidoNet whereas the gateway NIC MTUs are set by the operator AND we have often been bitten by forgetting to disable offload capabilities on the gateway NICs.\r\n\r\nIt’s best to disable all the Gateway NIC’s offload options. The offload features are intended to accelerate processing of network stacks. However, the Gateway NIC is the entry point to an entire network, so the offload optimizations don’t help. Compare this to physical routers - they might do IP fragmentation but don’t do TSO because they’re not the endpoint.\r\n\r\nIf nothing else, <b>you MUST disable GRO option on the gateway NICs</b>. Here’s how GRO can cause trouble:\r\n<ul>\r\n	<li>A burst of TCP packets in a single flow arrives at a NIC with GRO enabled.</li>\r\n	<li>The NIC coalesces the TCP packets into a single larger packet that exceeds the destination VM’s MTU.</li>\r\n	<li>The packet is tunneled from the gateway node to the egress host. Since it’s a large packet, it may need to be fragmented and reassembled (if so, the receive offload caused more work rather than less).</li>\r\n	<li>The egress host datapath blindly forwards the large packet to the VM’s NIC - the packet is dropped. Remember MN does not perform fragmentation or reassembly.</li>\r\n</ul>\r\nL2 Gateway NIC should have an MTU higher than the VMs (since the packets traversing these NICs will be VM packets with VLAN tags) so that it doesn’t drop tagged packets. But beyond that the L2 Gateway’s NIC doesn’t matter much: the virtual L2 Bridge doesn’t have an MTU. That said, the physical servers reachable via the L2 Gateway should have the same MTU as the VM instances they’re bridged to.\r\n\r\n<strong>We recommend VMs MTU to be set to the same MTU as Gateway NIC for the reasons listed above in the “MTU in the Overlay - VM” section. However, it’s possible to run all the VMs at some high MTU, e.g. 8850, derived from tunnel interfaces set at 9k MTU, while the L3 Gateway NIC’s MTU, and therefore the peer router’s, is set lower, e.g. at 1500. This works as long as the Gateway NIC’s MTU is not the smallest MTU on the path from VM to end-host on the Internet. Again, remember that MN’s Provider Router, that uses the NIC as its uplink, cannot generate ICMP destination unreachable frag needed. Therefore TCP connections are fine because the server outside the cloud sets a low MSS or because some non-MN router sends ICMP frag needed to the VM instances, which have a higher MTU. UDP connections are more affected, as PMTUD will not work as intended.</strong>',104,'http://blog.midonet.org/104-revision-v1/',0,'revision','',0),(109,5,'2014-12-02 13:10:37','2014-12-02 13:10:37','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API sores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-02 13:10:37','2014-12-02 13:10:37','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API sores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(110,5,'2014-12-02 13:14:16','2014-12-02 13:14:16','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-02 13:14:16','2014-12-02 13:14:16','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range, the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(111,5,'2014-12-02 13:15:46','2014-12-02 13:15:46','In this article we discuss how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in Part 2, the VM has obtained its IP address and default route (and possibly some other info) via DHCP. We\'ll soon try to add some diagrams to illustrate all of this text (please stay tuned). If multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\n\n<ul>\n    <li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254</li>\n    <li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\n</ul>\n\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\n\n<ol>\n    <li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10 (corresponding to Network10). I haven\'t explained this yet, but MN\'s virtual bridge has a MAC-IP table that it uses for ARP proxying/spoofing - this table is seeded along with the DHCPSubnet object - and includes the MAC-IP of the connected virtual router.<em> Similarly, the virtual bridge\'s mac-table is seeded at the same time (when the Neutron port is created) because Neutron already knows the MAC of the VM that will be connected to that port.</em> The virtual bridge will therefore <em>consume</em> the ARP request and generate an ARP reply and emit it from P10. The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101. It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the ARP request because the datapath doesn\'t buffer missed packets.</li>\n    <li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10.</li>\n    <li>TO BE COMPLETED</li>\n</ol>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-02 13:15:46','2014-12-02 13:15:46','In this article we discuss how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in Part 2, the VM has obtained its IP address and default route (and possibly some other info) via DHCP. We\'ll soon try to add some diagrams to illustrate all of this text (please stay tuned). If multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\r\n<ul>\r\n	<li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254</li>\r\n	<li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\r\n</ul>\r\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\r\n<ol>\r\n	<li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10 (corresponding to Network10). I haven\'t explained this yet, but MN\'s virtual bridge has a MAC-IP table that it uses for ARP proxying/spoofing - this table is seeded along with the DHCPSubnet object - and includes the MAC-IP of the connected virtual router.<em> Similarly, the virtual bridge\'s mac-table is seeded at the same time (when the Neutron port is created) because Neutron already knows the MAC of the VM that will be connected to that port.</em> The virtual bridge will therefore <em>consume</em> the ARP request and generate an ARP reply and emit it from P10. The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101. It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the ARP request because the datapath doesn\'t buffer missed packets.</li>\r\n	<li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet. The packet would arrive on P10 and hit Bridge10.</li>\r\n	<li>TO BE COMPLETED</li>\r\n</ol>\r\n&nbsp;',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(113,5,'2014-12-02 17:17:13','0000-00-00 00:00:00','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n</ul>\n\n&nbsp;','Introduction to MN (Part 6 – OpenStack Metadata Service)','','draft','open','open','','introduction-to-mn-part-6-openstack-metadata-service','','','2014-12-02 17:17:13','2014-12-02 17:17:13','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n</ul>\r\n&nbsp;',0,'http://blog.midonet.org/?p=113',0,'post','',0),(114,5,'2014-12-09 12:04:54','2014-12-09 12:04:54','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n</ul>\n\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\n\nFor example, imagine you have an already installed/running cloud and have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1 and you\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\n\nHow you configure this in MidoNet and in your physical network depends on what your physical network design and the physical switch capabilities. We\'ll start by trying to use concepts we\'ve already learned in previous posts and imagining that the servers\' physical L2 segment is VLAN 10 on an 802.1q compatible switch. The diagram below illustrates one possible physical setup as well as the mapping to the overlay topology.\n\n<ul>\n    <li>Let\'s assume that the physical switch has a free port and is within distance to run an Ethernet cable from that port to a commodity x86 server where MidoNet is installed. The MidoNet host need not be a compute host, but in this example let\'s assume it\'s Compute Host 1.</li>\n    <li>Let\'s also imagine that Host 1 has a free NIC, eth1. Host 1 (and Host 2 as well) has a NIC, eth0, that it uses for to send/receive tunneled overlay traffic to/from other compute hosts.<em> If Host 1 were single-homed we could set up some software bridging to accomplish the setup. For simplicity, we\'ll assume Host 1 has an extra NIC.</em></li>\n    <li>You run a cable from the physical switch\'s free port to Host1\'s eth1 and you configure the switch\'s port to carry untagged VLAN 10 traffic.</li>\n    <li>You use MidoNet\'s API (Neutron does not yet have a L2 Gateway API) to create a free port, P4, on Virtual Bridge 1, and to <em>bind</em> P4 to Host1/eth1. MidoNet Agent on Host 1 learns about that port-interface binding from MidoNet\'s state cluster (powered by Apache ZooKeeper) and plugs eth1 directly into the local datapath.</li>\n    <li>For the sake of this example, let\'s also imagine that VM1 is running on Host 1, while VM2 and VM3 are running on Host 2. As usual for MidoNet hosts, Host 1 and Host 2 both have access to an IP fabric and can tunnel (GRE/VXLAN) overlay traffic to each other.</li>\n</ul>\n\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\n\n<ol>\n    <li>The switch will flood the broadcast packet to all ports in VLAN 10. The packet will therefore be emitted (untagged) from the port towards Host 1\'s eth1.</li>\n    <li>When the packet arrives at Host 1\'s eth1, MidoNet interprets it as arriving at Bridge 1\'s P4. Bridge 1 learns that Server 1\'s MAC is on P4 (i.e. Bridge 1\'s MAC-table gets an entry mapping Server 1\'s MAC to P4).</li>\n    <li>Normally, the Bridge\'s ARP table has entries for the IPs and MACs of VM-facing ports (this was explained in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> and <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a>). However, for the sake of this example, let\'s assume that you\'ve disabled the virtual bridge\'s ARP table. The Simulation on Host 1 therefore determines that Bridge 1 would flood the packet from all its port (except P4, the ingress port). The MN Agent on Host 1 therefore has the datapath emit one copy of the packet locally (from the datapath port corresponding to P1 and therefore also VM1\'s software interface/tap) , and tunnel one copy to Host 2 to be emitted from the datapath ports corresponding to Bridge1\'s ports P2 and P3.</li>\n    <li>VM2 will answer with an ARP reply directed at Server 1\'s MAC. The MN Agent at Host 2 interprets the packet as arriving at port P2 of Bridge 1. The MAC-table entry for Server 1\'s MAC should have propagated from the MN Agent on Host 1 to MN\'s State Database and from there to MN Agents on other hosts, and particularly Host 2. The Simulation on Host 2 will therefore determine that ARP reply (a unicast) should be emitted from Bridge 1\'s port P4. As a result the packet is tunneled from Host 2 to Host 1, with a tunnel key that encodes P4\'s UUID.</li>\n    <li>Upon receiving the tunneled packet, Host 5 will decapsulate it and decode the tunnel key to P4, map P4 to the datapath port for eth1, and consequently emit the ARP reply from eth1.</li>\n    <li>The physical switch will receive the ARP reply, learn VM2\'s MAC, and the forward the ARP reply to Server 1.</li>\n</ol>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\n\nSo this setup accomplishes L2 connectivity between the physical servers and the VM instances. But there are several problems with this approach:\n\n<ul>\n    <li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\n    <li>one physical switch port will be used for each Network that needs to be connected to physical servers</li>\n    <li>you have to make manual changes to the physical network each time you have another request to bridge virtual and physical workloads.</li>\n</ul>\n\n<strong>MidoNet\'s VLAN L2 Gateway</strong> is meant to address these problems while continuing to work with the 802.1q physical switch fabric. <em>In a future post we\'ll describe the VXLAN L2 Gateway which solves these problems and more but requires the network operator to have a VTEP (VXLAN Tunnel Endpoint) connected to the physical workloads.</em>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\n\nIn the diagram above, you can see that a new overlay/virtual device has been introduced: a VLAN-aware (802.1Q compatible) Bridge2. Bridge1\'s P4 is now an <em>interior port</em> (meaning it connects to another overlay device and emits traffic that remains in the overlay topology) and links to Bridge2 at an untagged VLAN 10 port. Bridge2 has a trunk port which is <em>bound</em> to Host1/eth1 (where P4 was bound in the earlier/naive setup. Also, and importantly,<strong> the physical switch port facing Host1/eth1 must be put in TRUNK mode</strong> (earlier it was untagged VLAN 10).\n\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\n\n<ol>\n    <li>The physical switch floods the packet, it\'s tagged with VLAN10 and emitted to Host1/eth1.</li>\n    <li>MN interprets the packet as arriving on Bridge2\'s trunk port; Bridge2 learns Server1\'s MAC. The packet must be untagged and flooded from all VLAN 10 untagged ports (MN will only ever allow one, in order to prevent bridging loops). The packet is therefore emitted by Bridge2 on the port facing Bridge1.</li>\n    <li>Bridge1 receives the packet on P4 and learns Server1\'s MAC, floods it to all its port (except the ingress). All VMs on Bridge1 receive the packet. Only VM2 constructs an ARP reply.</li>\n    <li>VM2\'s ARP reply packet ingresses Bridge1 on port P2. The unicast destination MAC matches the just-learned MAC-table entry, so Bridge1 emits the packet from P4.</li>\n    <li>The packet ingresses Bridge2 at the VLAN 10 untagged port. The unicast destination MAC matches the MAC-table entry from step #2, so Bridge2 tags the packet with VLAN 10 and emits it from the trunk port.</li>\n    <li>Since the trunk port is bound to Host1/eth1, the tagged packet is emitted from that interface and arrives at the physical switch\'s trunk port. The physical switch learns VM2\'s MAC on VLAN10, untags the packet and forwards it to Server1.</li>\n</ol>\n\nThe introduction of Bridge2 now allows the following:\n\n<ul>\n    <li>Connect other vlan-agnostic virtual bridges (Neutron networks) to Bridge2. Each one will require a new untagged port on Bridge2 with a unique VLAN id. These are overlay objects and are therefore instantly provisioned.</li>\n    <li>Add a second trunk port to Bridge 2, bound to Host3/eth1. Host3/eth1 is connected via physical cable to another trunk port on the physical switch. This removes the SPOFs. The setup is manual but is only performed once. Thereafter, new vlan gateways require only new ports on Bridge2.</li>\n</ul>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>\n\n&nbsp;\n\nFinally, what about STP (Spanning Tree Protocol) and its variants? MidoNet\'s VLAN-aware bridge does not implement STP. However, it recognizes STP packets and:\n\n<ul>\n    <li>drops any arriving on a non-trunk port (i.e. on the untagged VLAN ports)</li>\n    <li>if an STP packet arrives on Trunk1, it\'s forwarded from Trunk2, and vice-versa.</li>\n</ul>\n\nThis <em>STP pass-through</em> allows connecting the VLAN L2 Gateway Nodes/interfaces (Host1/eth1 and Host3/eth1 in our example) to different physical switches that are part of the same L2 fabric. That eliminates the physical switch as a SPOF. STP in the physical fabric, combined with mac-learning on the vlan-aware overlay bridge\'s trunk ports allows outgoing overlay traffic to choose the correct trunk port.','Introduction to MN (Part 6 – VLAN L2 Gateway)','','publish','open','open','','intro-to-mn-part-6-vlan-l2gateways','','','2014-12-09 12:04:54','2014-12-09 12:04:54','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n</ul>\r\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\r\n\r\nFor example, imagine you have an already installed/running cloud and have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1 and you\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\r\n\r\nHow you configure this in MidoNet and in your physical network depends on what your physical network design and the physical switch capabilities. We\'ll start by trying to use concepts we\'ve already learned in previous posts and imagining that the servers\' physical L2 segment is VLAN 10 on an 802.1q compatible switch. The diagram below illustrates one possible physical setup as well as the mapping to the overlay topology.\r\n<ul>\r\n	<li>Let\'s assume that the physical switch has a free port and is within distance to run an Ethernet cable from that port to a commodity x86 server where MidoNet is installed. The MidoNet host need not be a compute host, but in this example let\'s assume it\'s Compute Host 1.</li>\r\n	<li>Let\'s also imagine that Host 1 has a free NIC, eth1. Host 1 (and Host 2 as well) has a NIC, eth0, that it uses for to send/receive tunneled overlay traffic to/from other compute hosts.<em> If Host 1 were single-homed we could set up some software bridging to accomplish the setup. For simplicity, we\'ll assume Host 1 has an extra NIC.</em></li>\r\n	<li>You run a cable from the physical switch\'s free port to Host1\'s eth1 and you configure the switch\'s port to carry untagged VLAN 10 traffic.</li>\r\n	<li>You use MidoNet\'s API (Neutron does not yet have a L2 Gateway API) to create a free port, P4, on Virtual Bridge 1, and to <em>bind</em> P4 to Host1/eth1. MidoNet Agent on Host 1 learns about that port-interface binding from MidoNet\'s state cluster (powered by Apache ZooKeeper) and plugs eth1 directly into the local datapath.</li>\r\n	<li>For the sake of this example, let\'s also imagine that VM1 is running on Host 1, while VM2 and VM3 are running on Host 2. As usual for MidoNet hosts, Host 1 and Host 2 both have access to an IP fabric and can tunnel (GRE/VXLAN) overlay traffic to each other.</li>\r\n</ul>\r\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\r\n<ol>\r\n	<li>The switch will flood the broadcast packet to all ports in VLAN 10. The packet will therefore be emitted (untagged) from the port towards Host 1\'s eth1.</li>\r\n	<li>When the packet arrives at Host 1\'s eth1, MidoNet interprets it as arriving at Bridge 1\'s P4. Bridge 1 learns that Server 1\'s MAC is on P4 (i.e. Bridge 1\'s MAC-table gets an entry mapping Server 1\'s MAC to P4).</li>\r\n	<li>Normally, the Bridge\'s ARP table has entries for the IPs and MACs of VM-facing ports (this was explained in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> and <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a>). However, for the sake of this example, let\'s assume that you\'ve disabled the virtual bridge\'s ARP table. The Simulation on Host 1 therefore determines that Bridge 1 would flood the packet from all its port (except P4, the ingress port). The MN Agent on Host 1 therefore has the datapath emit one copy of the packet locally (from the datapath port corresponding to P1 and therefore also VM1\'s software interface/tap) , and tunnel one copy to Host 2 to be emitted from the datapath ports corresponding to Bridge1\'s ports P2 and P3.</li>\r\n	<li>VM2 will answer with an ARP reply directed at Server 1\'s MAC. The MN Agent at Host 2 interprets the packet as arriving at port P2 of Bridge 1. The MAC-table entry for Server 1\'s MAC should have propagated from the MN Agent on Host 1 to MN\'s State Database and from there to MN Agents on other hosts, and particularly Host 2. The Simulation on Host 2 will therefore determine that ARP reply (a unicast) should be emitted from Bridge 1\'s port P4. As a result the packet is tunneled from Host 2 to Host 1, with a tunnel key that encodes P4\'s UUID.</li>\r\n	<li>Upon receiving the tunneled packet, Host 5 will decapsulate it and decode the tunnel key to P4, map P4 to the datapath port for eth1, and consequently emit the ARP reply from eth1.</li>\r\n	<li>The physical switch will receive the ARP reply, learn VM2\'s MAC, and the forward the ARP reply to Server 1.</li>\r\n</ol>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\r\n\r\nSo this setup accomplishes L2 connectivity between the physical servers and the VM instances. But there are several problems with this approach:\r\n<ul>\r\n	<li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\r\n	<li>one physical switch port will be used for each Network that needs to be connected to physical servers</li>\r\n	<li>you have to make manual changes to the physical network each time you have another request to bridge virtual and physical workloads.</li>\r\n</ul>\r\n<strong>MidoNet\'s VLAN L2 Gateway</strong> is meant to address these problems while continuing to work with the 802.1q physical switch fabric. <em>In a future post we\'ll describe the VXLAN L2 Gateway which solves these problems and more but requires the network operator to have a VTEP (VXLAN Tunnel Endpoint) connected to the physical workloads.</em>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\r\n\r\nIn the diagram above, you can see that a new overlay/virtual device has been introduced: a VLAN-aware (802.1Q compatible) Bridge2. Bridge1\'s P4 is now an <em>interior port</em> (meaning it connects to another overlay device and emits traffic that remains in the overlay topology) and links to Bridge2 at an untagged VLAN 10 port. Bridge2 has a trunk port which is <em>bound</em> to Host1/eth1 (where P4 was bound in the earlier/naive setup. Also, and importantly,<strong> the physical switch port facing Host1/eth1 must be put in TRUNK mode</strong> (earlier it was untagged VLAN 10).\r\n\r\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\r\n<ol>\r\n	<li>The physical switch floods the packet, it\'s tagged with VLAN10 and emitted to Host1/eth1.</li>\r\n	<li>MN interprets the packet as arriving on Bridge2\'s trunk port; Bridge2 learns Server1\'s MAC. The packet must be untagged and flooded from all VLAN 10 untagged ports (MN will only ever allow one, in order to prevent bridging loops). The packet is therefore emitted by Bridge2 on the port facing Bridge1.</li>\r\n	<li>Bridge1 receives the packet on P4 and learns Server1\'s MAC, floods it to all its port (except the ingress). All VMs on Bridge1 receive the packet. Only VM2 constructs an ARP reply.</li>\r\n	<li>VM2\'s ARP reply packet ingresses Bridge1 on port P2. The unicast destination MAC matches the just-learned MAC-table entry, so Bridge1 emits the packet from P4.</li>\r\n	<li>The packet ingresses Bridge2 at the VLAN 10 untagged port. The unicast destination MAC matches the MAC-table entry from step #2, so Bridge2 tags the packet with VLAN 10 and emits it from the trunk port.</li>\r\n	<li>Since the trunk port is bound to Host1/eth1, the tagged packet is emitted from that interface and arrives at the physical switch\'s trunk port. The physical switch learns VM2\'s MAC on VLAN10, untags the packet and forwards it to Server1.</li>\r\n</ol>\r\nThe introduction of Bridge2 now allows the following:\r\n<ul>\r\n	<li>Connect other vlan-agnostic virtual bridges (Neutron networks) to Bridge2. Each one will require a new untagged port on Bridge2 with a unique VLAN id. These are overlay objects and are therefore instantly provisioned.</li>\r\n	<li>Add a second trunk port to Bridge 2, bound to Host3/eth1. Host3/eth1 is connected via physical cable to another trunk port on the physical switch. This removes the SPOFs. The setup is manual but is only performed once. Thereafter, new vlan gateways require only new ports on Bridge2.</li>\r\n</ul>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>\r\n\r\n&nbsp;\r\n\r\nFinally, what about STP (Spanning Tree Protocol) and its variants? MidoNet\'s VLAN-aware bridge does not implement STP. However, it recognizes STP packets and:\r\n<ul>\r\n	<li>drops any arriving on a non-trunk port (i.e. on the untagged VLAN ports)</li>\r\n	<li>if an STP packet arrives on Trunk1, it\'s forwarded from Trunk2, and vice-versa.</li>\r\n</ul>\r\nThis <em>STP pass-through</em> allows connecting the VLAN L2 Gateway Nodes/interfaces (Host1/eth1 and Host3/eth1 in our example) to different physical switches that are part of the same L2 fabric. That eliminates the physical switch as a SPOF. STP in the physical fabric, combined with mac-learning on the vlan-aware overlay bridge\'s trunk ports allows outgoing overlay traffic to choose the correct trunk port.',0,'http://blog.midonet.org/?p=114',0,'post','',0),(115,5,'2014-12-02 16:34:47','2014-12-02 16:34:47','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule. In <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-02 16:34:47','2014-12-02 16:34:47','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule. In <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(116,5,'2014-12-02 16:37:27','2014-12-02 16:37:27','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router,</li>\n    <li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks,</li>\n    <li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.</li>\n    <li>in Part 5 we discussed Floating IPs</li>\n</ul>\n\n&nbsp;','Introduction to MN’s Overlay Network Models (Part 6 – OpenStack Metadata Service)','','inherit','open','open','','113-revision-v1','','','2014-12-02 16:37:27','2014-12-02 16:37:27','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router,</li>\r\n	<li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks,</li>\r\n	<li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li>in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> we discussed Security Groups.</li>\r\n	<li>in Part 5 we discussed Floating IPs</li>\r\n</ul>\r\n&nbsp;',113,'http://blog.midonet.org/113-revision-v1/',0,'revision','',0),(119,9,'2014-12-02 17:42:50','0000-00-00 00:00:00','Here is the main content.','Test','','draft','open','open','','','','','2014-12-02 17:42:50','2014-12-02 17:42:50','Here is the main content.',0,'http://blog.midonet.org/?p=119',0,'post','',0),(120,5,'2014-12-02 16:43:20','2014-12-02 16:43:20','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\n</ul>\n\n&nbsp;\n\n&nbsp;','Introduction to MN’s Overlay Network Models (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-02 16:43:20','2014-12-02 16:43:20','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\r\n</ul>\r\n&nbsp;\r\n\r\n&nbsp;',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(121,5,'2014-12-02 16:44:06','2014-12-02 16:44:06','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n</ul>\n\n&nbsp;','Introduction to MN’s Overlay Network Models (Part 6 – OpenStack Metadata Service)','','inherit','open','open','','113-revision-v1','','','2014-12-02 16:44:06','2014-12-02 16:44:06','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n</ul>\r\n&nbsp;',113,'http://blog.midonet.org/113-revision-v1/',0,'revision','',0),(122,5,'2014-12-02 16:44:39','2014-12-02 16:44:39','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\n</ul>\n\n&nbsp;\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-02 16:44:39','2014-12-02 16:44:39','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\n&nbsp;\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(123,5,'2014-12-02 17:03:35','2014-12-02 17:03:35','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-02 17:03:35','2014-12-02 17:03:35','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\"><img class=\"wp-image-75 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys-300x225.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"300\" height=\"225\" /></a> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\r\n\r\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(124,5,'2014-12-02 17:07:12','2014-12-02 17:07:12','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n</ul>\n\n&nbsp;','Introduction to MN (Part 6 – OpenStack Metadata Service)','','inherit','open','open','','113-revision-v1','','','2014-12-02 17:07:12','2014-12-02 17:07:12','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n</ul>\r\n&nbsp;',113,'http://blog.midonet.org/113-revision-v1/',0,'revision','',0),(125,5,'2014-12-02 17:08:24','2014-12-02 17:08:24','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\n</ul>\n\n&nbsp;\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-02 17:08:24','2014-12-02 17:08:24','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\n&nbsp;\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(126,5,'2014-12-02 17:08:31','2014-12-02 17:08:31','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\n</ul>\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-02 17:08:31','2014-12-02 17:08:31','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\r\n</ul>\r\n&nbsp;\r\n\r\n&nbsp;',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(127,5,'2014-12-02 17:09:37','2014-12-02 17:09:37','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\n\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-02 17:09:37','2014-12-02 17:09:37','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router, in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks, and in <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> we discussed how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.\r\n\r\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(128,5,'2014-12-02 17:11:50','2014-12-02 17:11:50','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a title=\"Introduction to MN (Part 4 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mn-part-4-security-groups/\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\n</ul>\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-02 17:11:50','2014-12-02 17:11:50','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a title=\"Introduction to MN (Part 4 – Security Groups)\" href=\"http://blog.midonet.org/introduction-mn-part-4-security-groups/\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\r\n</ul>\r\n&nbsp;\r\n\r\n&nbsp;',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(129,5,'2014-12-02 17:14:40','2014-12-02 17:14:40','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\n</ul>\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-02 17:14:40','2014-12-02 17:14:40','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\r\n</ul>\r\n&nbsp;\r\n\r\n&nbsp;',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(130,5,'2014-12-02 17:16:24','2014-12-02 17:16:24','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n</ul>\n\n&nbsp;','Introduction to MN (Part 6 – OpenStack Metadata Service)','','inherit','open','open','','113-revision-v1','','','2014-12-02 17:16:24','2014-12-02 17:16:24','In this article we discuss how OpenStack\'s Metadata Service is reachable via MidoNet\'s overlay. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n</ul>\r\n&nbsp;',113,'http://blog.midonet.org/113-revision-v1/',0,'revision','',0),(131,5,'2014-12-02 17:17:02','2014-12-02 17:17:02','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n</ul>\n\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-02 17:17:02','2014-12-02 17:17:02','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n</ul>\r\nEach Tenant has a unique “default security group”. This SecGroup is created when the first network is created and it’s empty to start. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(132,5,'2014-12-02 17:18:35','2014-12-02 17:18:35','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n</ul>\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-02 17:18:35','2014-12-02 17:18:35','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(133,5,'2014-12-02 17:44:05','2014-12-02 17:44:05','In this article we discuss how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in Part 2, the VM has obtained its IP address and default route (and possibly some other info) via DHCP. If multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\n\n<ul>\n    <li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254\n<ul>\n    <li>Assume the</li>\n</ul>\n</li>\n    <li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\n</ul>\n\nSince the two Networks belong to the same Tenant, they will be connected to the same Neutron router. Equivalently, MN\'s overlay will have the corresponding virtual bridges linked to a single virtual router.\n\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\n\n<ol>\n    <li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\n<ul>\n    <li>The packet would arrive on P10 and hit Bridge10 (corresponding to Network10).</li>\n    <li>I alluded to this in the previous post, MN\'s virtual bridge has a MAC-IP table and it answers ARPs when it can. The bridge\'s ARP table, <em>as well as its MAC-table</em>, is seeded as ports are created on the corresponding Neutron network (MAC and IPs are generated and associated with the port ID in Neutron\'s DB). The virtual bridge will therefore <em>consume</em> the ARP request, generate an ARP reply and emit it from P10.</li>\n    <li>The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101.</li>\n    <li>It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the consumed ARP request because the datapath doesn\'t buffer missed packets.</li>\n</ul>\n</li>\n    <li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\n<ul>\n    <li>The packet would arrive on P10 and hit Bridge10. Since the bridge\'s MAC-table was already seeded with an entry for the Router\'s</li>\n    <li></li>\n</ul>\n</li>\n    <li>TO BE COMPLETED</li>\n</ol>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-02 17:44:05','2014-12-02 17:44:05','In this article we discuss how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in Part 2, the VM has obtained its IP address and default route (and possibly some other info) via DHCP. If multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\r\n<ul>\r\n	<li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254\r\n<ul>\r\n	<li>Assume the</li>\r\n</ul>\r\n</li>\r\n	<li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\r\n</ul>\r\nSince the two Networks belong to the same Tenant, they will be connected to the same Neutron router. Equivalently, MN\'s overlay will have the corresponding virtual bridges linked to a single virtual router.\r\n\r\nThen, if you VNC (you can\'t SSH because we haven\'t assigned FloatingIP\'s yet) into VM1 and run <em>ping 10.10.20.1</em>, the following occurs:\r\n<ol>\r\n	<li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\r\n<ul>\r\n	<li>The packet would arrive on P10 and hit Bridge10 (corresponding to Network10).</li>\r\n	<li>I alluded to this in the previous post, MN\'s virtual bridge has a MAC-IP table and it answers ARPs when it can. The bridge\'s ARP table, <em>as well as its MAC-table</em>, is seeded as ports are created on the corresponding Neutron network (MAC and IPs are generated and associated with the port ID in Neutron\'s DB). The virtual bridge will therefore <em>consume</em> the ARP request, generate an ARP reply and emit it from P10.</li>\r\n	<li>The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101.</li>\r\n	<li>It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the consumed ARP request because the datapath doesn\'t buffer missed packets.</li>\r\n</ul>\r\n</li>\r\n	<li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\r\n<ul>\r\n	<li>The packet would arrive on P10 and hit Bridge10. Since the bridge\'s MAC-table was already seeded with an entry for the Router\'s</li>\r\n	<li></li>\r\n</ul>\r\n</li>\r\n	<li>TO BE COMPLETED</li>\r\n</ol>\r\n&nbsp;',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(135,10,'2014-12-03 20:46:11','2014-12-03 20:46:11','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone size-medium wp-image-148\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-300x206.png\" alt=\"Drawing1\" width=\"300\" height=\"206\" /></a>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-autosave-v1','','','2014-12-03 20:46:11','2014-12-03 20:46:11','',79,'http://blog.midonet.org/79-autosave-v1/',0,'revision','',0),(136,2,'2014-12-03 01:57:16','2014-12-03 01:57:16','http://www.meetup.com/cloud-sg/events/218729618/','Singapore MidoNet Meetup: December 4th','','publish','open','open','','singapore-midonet-meetup-december-4th','','','2014-12-03 02:26:53','2014-12-03 02:26:53','http://www.meetup.com/cloud-sg/events/218729618/',0,'http://blog.midonet.org/?p=136',0,'post','',0),(137,2,'2014-12-03 01:57:16','2014-12-03 01:57:16','Join us in Singapore on December 4th for a Meetup covering OpenStack Networking and MidoNet\nhttp://www.meetup.com/cloud-sg/events/218729618/','Singapore MidoNet Meetup: December 4th','','inherit','open','open','','136-revision-v1','','','2014-12-03 01:57:16','2014-12-03 01:57:16','Join us in Singapore on December 4th for a Meetup covering OpenStack Networking and MidoNet\r\nhttp://www.meetup.com/cloud-sg/events/218729618/',136,'http://blog.midonet.org/136-revision-v1/',0,'revision','',0),(138,2,'2014-12-03 01:57:58','2014-12-03 01:57:58','http://www.meetup.com/cloud-sg/events/218729618/','Singapore MidoNet Meetup: December 4th','','inherit','open','open','','136-revision-v1','','','2014-12-03 01:57:58','2014-12-03 01:57:58','http://www.meetup.com/cloud-sg/events/218729618/',136,'http://blog.midonet.org/136-revision-v1/',0,'revision','',0),(139,2,'2014-12-03 02:19:19','2014-12-03 02:19:19','','meetup','','inherit','open','open','','meetup','','','2014-12-03 02:19:19','2014-12-03 02:19:19','',136,'http://blog.midonet.org/wp-content/uploads/2014/12/meetup.jpeg',0,'attachment','image/jpeg',0),(140,2,'2014-12-03 02:26:04','2014-12-03 02:26:04','<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/meetup.jpeg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/12/meetup-150x150.jpeg\" alt=\"meetup\" width=\"150\" height=\"150\" class=\"alignnone size-thumbnail wp-image-139\" /></a>http://www.meetup.com/cloud-sg/events/218729618/','Singapore MidoNet Meetup: December 4th','','inherit','open','open','','136-autosave-v1','','','2014-12-03 02:26:04','2014-12-03 02:26:04','',136,'http://blog.midonet.org/136-autosave-v1/',0,'revision','',0),(141,2,'2014-12-03 02:26:05','2014-12-03 02:26:05','<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/meetup.jpeg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/12/meetup-150x150.jpeg\" alt=\"meetup\" width=\"150\" height=\"150\" class=\"alignnone size-thumbnail wp-image-139\" /></a>http://www.meetup.com/cloud-sg/events/218729618/','Singapore MidoNet Meetup: December 4th','','inherit','open','open','','136-revision-v1','','','2014-12-03 02:26:05','2014-12-03 02:26:05','<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/meetup.jpeg\"><img src=\"http://blog.midonet.org/wp-content/uploads/2014/12/meetup-150x150.jpeg\" alt=\"meetup\" width=\"150\" height=\"150\" class=\"alignnone size-thumbnail wp-image-139\" /></a>http://www.meetup.com/cloud-sg/events/218729618/',136,'http://blog.midonet.org/136-revision-v1/',0,'revision','',0),(142,2,'2014-12-03 02:26:53','2014-12-03 02:26:53','http://www.meetup.com/cloud-sg/events/218729618/','Singapore MidoNet Meetup: December 4th','','inherit','open','open','','136-revision-v1','','','2014-12-03 02:26:53','2014-12-03 02:26:53','http://www.meetup.com/cloud-sg/events/218729618/',136,'http://blog.midonet.org/136-revision-v1/',0,'revision','',0),(143,5,'2014-12-03 16:54:13','2014-12-03 16:54:13','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (e.g. 20.20.0.20), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-03 16:54:13','2014-12-03 16:54:13','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (e.g. 20.20.0.20), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(144,5,'2014-12-03 16:55:22','2014-12-03 16:55:22','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-03 16:55:22','2014-12-03 16:55:22','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network, the gateway address and some DHCP options. Multiple Subnets are allowed. IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet; a /32 <em>Local</em> route and a route to the prefix, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate options, including any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(145,5,'2014-12-03 17:01:24','2014-12-03 17:01:24','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-03 17:01:24','2014-12-03 17:01:24','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(146,10,'2014-12-03 20:42:08','2014-12-03 20:42:08','','Arch blog','','inherit','open','open','','arch-blog','','','2014-12-03 20:42:08','2014-12-03 20:42:08','',79,'http://blog.midonet.org/wp-content/uploads/2014/12/Arch-blog.jpg',0,'attachment','image/jpeg',0),(147,10,'2014-12-03 20:42:44','2014-12-03 20:42:44','','Arch blog','','inherit','open','open','','arch-blog-2','','','2014-12-03 20:42:44','2014-12-03 20:42:44','',79,'http://blog.midonet.org/wp-content/uploads/2014/12/Arch-blog1.jpg',0,'attachment','image/jpeg',0),(148,10,'2014-12-03 20:45:53','2014-12-03 20:45:53','','Drawing1','','inherit','open','open','','drawing1','','','2014-12-03 20:45:53','2014-12-03 20:45:53','',79,'http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png',0,'attachment','image/png',0),(149,10,'2014-12-03 20:47:06','2014-12-03 20:47:06','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone size-medium wp-image-148\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-300x206.png\" alt=\"Drawing1\" width=\"300\" height=\"206\" /></a>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-03 20:47:06','2014-12-03 20:47:06','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone size-medium wp-image-148\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-300x206.png\" alt=\"Drawing1\" width=\"300\" height=\"206\" /></a>\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(150,2,'2014-12-04 07:56:55','2014-12-04 07:56:55','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" /></a>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-autosave-v1','','','2014-12-04 07:56:55','2014-12-04 07:56:55','',79,'http://blog.midonet.org/79-autosave-v1/',0,'revision','',0),(151,2,'2014-12-04 07:54:23','2014-12-04 07:54:23','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone wp-image-148\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-300x206.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" /></a>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-04 07:54:23','2014-12-04 07:54:23','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone wp-image-148\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-300x206.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" /></a>\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(152,2,'2014-12-04 07:56:58','2014-12-04 07:56:58','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" /></a>\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-04 07:56:58','2014-12-04 07:56:58','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1.png\"><img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" /></a>\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(153,2,'2014-12-04 07:57:18','2014-12-04 07:57:18','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" />\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-04 07:57:18','2014-12-04 07:57:18','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" />\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(154,2,'2014-12-04 08:01:37','2014-12-04 08:01:37','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[gallery ids=\"73,72\"]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-04 08:01:37','2014-12-04 08:01:37','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[gallery ids=\"73,72\"]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\r\n\r\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(155,5,'2014-12-06 21:40:36','2014-12-06 21:40:36','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n</ul>\n\n<h2>Introduction</h2>\n\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\n\nTraditionally, in physical networking, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\n\nThe single location was a SPOF (single point of failure), so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster (e.g. of Load-balancers).\n\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\n\n<ul>\n    <li>it allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.</li>\n    <li>when flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.</li>\n</ul>\n\n<h2>The Old Approach</h2>\n\n<em>Recap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.</em>\n\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\n\n<ul>\n    <li>is this a forward or return flow?</li>\n    <li>in what state is this TCP connection?</li>\n    <li>where should load-balancer LB1 send this flow?</li>\n    <li>what Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?</li>\n    <li>FYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case because once a single route is chosen it continues to be used during the lifetime of the flow installed by the Simulation that ran the ECMP algorithm.</li>\n</ul>\n\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. <em>Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.</em>\n\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\n\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\n\nDiagram 1:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\n\nDiagram 2:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\n\nDiagram 3:\n\n<h2><a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\nThe New Approach</h2>\n\n<em>Summary: the ingress node pushes the flow-state directly to the Interested Set of nodes.</em>\n\nWe reasoned that although each flow\'s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\n\nTwo improvements were therefore feasible:\n\n<ul>\n    <li>reduce latency because flow computation only requires locally available state</li>\n    <li>remove a dependency on a database</li>\n</ul>\n\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\n\nDiagram 4:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\n\nDiagram 5:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\n\nDiagram 6:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\n\n<h3>Algorithm/protocol for pushing flow-state between Agents</h3>\n\nTunnel packet (currently using GRE/UDP) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\n\n<em>The upcoming release of MidoNet will have the option of setting DSCP values in the outer header of the state packets. Operators can map this DSCP value to a higher-priority traffic class in the underlay switches. This is important since the MN Agent cannot always detect when the state packet needs to be retransmitted. Failure to receive a state packet can result in a failed connection, for example, because the MN Agent at the host where the return flow ingresses does not have the forward flow state.</em>\n\n<h3>Interested Set Hinting</h3>\n\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\n\n<ul>\n    <li>Depending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.</li>\n    <li>A tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.</li>\n    <li>A VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.</li>\n    <li>The VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\n<ul>\n    <li>The physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.</li>\n</ul>\n</li>\n</ul>\n\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.','Introduction to MN (Part 5 – Flow State)','','publish','open','open','','introduction-to-mn-part-5-flow-state','','http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\nhttp://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/','2014-12-06 21:42:55','2014-12-06 21:42:55','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\n<h2>Introduction</h2>\r\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\r\n\r\nTraditionally, in physical networking, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\r\n\r\nThe single location was a SPOF (single point of failure), so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster (e.g. of Load-balancers).\r\n\r\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\r\n<ul>\r\n	<li>it allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.</li>\r\n	<li>when flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.</li>\r\n</ul>\r\n<h2>The Old Approach</h2>\r\n<em>Recap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.</em>\r\n\r\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\r\n<ul>\r\n	<li>is this a forward or return flow?</li>\r\n	<li>in what state is this TCP connection?</li>\r\n	<li>where should load-balancer LB1 send this flow?</li>\r\n	<li>what Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?</li>\r\n	<li>FYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case because once a single route is chosen it continues to be used during the lifetime of the flow installed by the Simulation that ran the ECMP algorithm.</li>\r\n</ul>\r\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. <em>Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.</em>\r\n\r\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\r\n\r\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\r\n\r\nDiagram 1:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\r\n\r\nDiagram 2:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\r\n\r\nDiagram 3:\r\n<h2><a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\r\nThe New Approach</h2>\r\n<em>Summary: the ingress node pushes the flow-state directly to the Interested Set of nodes.</em>\r\n\r\nWe reasoned that although each flow\'s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\r\n\r\nTwo improvements were therefore feasible:\r\n<ul>\r\n	<li>reduce latency because flow computation only requires locally available state</li>\r\n	<li>remove a dependency on a database</li>\r\n</ul>\r\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\r\n\r\nDiagram 4:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\r\n\r\nDiagram 5:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\r\n\r\nDiagram 6:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\r\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\r\n<h3>Algorithm/protocol for pushing flow-state between Agents</h3>\r\nTunnel packet (currently using GRE/UDP) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\r\n\r\n<em>The upcoming release of MidoNet will have the option of setting DSCP values in the outer header of the state packets. Operators can map this DSCP value to a higher-priority traffic class in the underlay switches. This is important since the MN Agent cannot always detect when the state packet needs to be retransmitted. Failure to receive a state packet can result in a failed connection, for example, because the MN Agent at the host where the return flow ingresses does not have the forward flow state.</em>\r\n<h3>Interested Set Hinting</h3>\r\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\r\n<ul>\r\n	<li>Depending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.</li>\r\n	<li>A tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.</li>\r\n	<li>A VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.</li>\r\n	<li>The VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\r\n<ul>\r\n	<li>The physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.',0,'http://blog.midonet.org/?p=155',0,'post','',0),(156,5,'2014-12-04 12:05:07','2014-12-04 12:05:07','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the eighth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1Q\">Part 7</a> covered L2 Gateways</li>\n</ul>\n\nIntroduction\n\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\n\nTraditionally, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\n\nThe single location was a SPOF, so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster of Load-balancers.\n\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\nit allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.\nwhen flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.\n\nThe Old Approach\n\nRecap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.\n\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\nis this a forward or return flow?\nin what state is this TCP connection?\nwhere should load-balancer LB1 send this flow?\nwhat Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?\nFYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case.\n\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.\n\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\n\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\n\nDiagram 1:\n\nDiagram 2:\n\nDiagram 3:\nThe New Approach\n\nSummary: the ingress node pushes the flow-state directly to the Interested Set of nodes.\n\nWe reasoned that although each flow-s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\n\nTwo improvements were therefore feasible:\nreduce latency because flow computation only requires locally available state\nremove a dependency on a database\n\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\n\nDiagram 4:\n\nDiagram 5:\n\nDiagram 6:\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\nAlgorithm/protocol for pushing flow-state between Agents\n\nTunnel packet (currently using GRE/UDP - unreliable, but later will use DSCP or PFC) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\nInterested Set Hinting\n\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\nDepending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.\nA tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.\nA VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.\nThe VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\nThe physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.\n\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.\nPort Migration\n\nThe typical use-case for port migration is VM migration. In the old approach to flow-state management, the MN Agent at the virtual port’s New Node didn’t have to do anything special. For every flow computation that required state, there would be a lookup in Cassandra.\n\nIn the new approach, flow computation at the New Node works as described above. However, for a few seconds after the port has migrated, flows will not find local state and flow computations will therefore be performed incorrectly. However, in the background the MN Agent knows that this port was recently owned/hosted by a different, Old Node. The New Node therefore pulls all the required flow state from the Old Node.\n\nConsider that when port1 migrates from Old Node to New Node, Old Node has all the flow-state required for (forward and return) flows that potentially ingress port1. Consider also that:\nif port1 is not in an Ingress Set, then no other node has all of port1’s ingress flows’ state. If Old Node is unavailable, the flow state may not be recoverable - this is acceptable because VM migration itself requires Old Node to be available for memory transfer.\nif port1 is in an Ingress Set, then nodes other than Old Node may be able to help recover the ingress flow state for the port.\n\nFlow state is indexed (and laid out contiguously in memory) by ingress port, and can be transferred from Old to New Node within a matter of seconds (this needs to be clarified in terms of how much data per flow, how many flows, and in exactly what time delay).\nAgent restart\n\nIn the old approach, the MN Agent would flush all its state (including flow rules) on every restart. When recomputing flows, the Agent would simply do lookups in Cassandra.\n\nIn the new approach, the MN Agent either flushes its per-flow state to disk periodically, or, in the case of ports belonging to Ingress Sets, may avoid disk-writes by recovering the flow-state from another Node that hosts a port in the same Ingress Set. Recovering from peers avoids disk writes on heavily loaded nodes like L3 Gateways.\n\nMore Advanced Cases: real connection tracking, DPI information.\n\nIn the new Distributed Flow State management, many nodes may potentially update flow state after it’s created...\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 8 – Flow State)','','inherit','open','open','','155-revision-v1','','','2014-12-04 12:05:07','2014-12-04 12:05:07','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the eighth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 5</a> covered Floating IPs.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1P\">Part 6</a> covered OpenStack\'s Metadata Service.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1Q\">Part 7</a> covered L2 Gateways</li>\r\n</ul>\r\nIntroduction\r\n\r\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\r\n\r\nTraditionally, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\r\n\r\nThe single location was a SPOF, so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster of Load-balancers.\r\n\r\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\r\nit allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.\r\nwhen flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.\r\n\r\nThe Old Approach\r\n\r\nRecap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.\r\n\r\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\r\nis this a forward or return flow?\r\nin what state is this TCP connection?\r\nwhere should load-balancer LB1 send this flow?\r\nwhat Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?\r\nFYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case.\r\n\r\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.\r\n\r\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\r\n\r\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\r\n\r\nDiagram 1:\r\n\r\nDiagram 2:\r\n\r\nDiagram 3:\r\nThe New Approach\r\n\r\nSummary: the ingress node pushes the flow-state directly to the Interested Set of nodes.\r\n\r\nWe reasoned that although each flow-s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\r\n\r\nTwo improvements were therefore feasible:\r\nreduce latency because flow computation only requires locally available state\r\nremove a dependency on a database\r\n\r\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\r\n\r\nDiagram 4:\r\n\r\nDiagram 5:\r\n\r\nDiagram 6:\r\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\r\nAlgorithm/protocol for pushing flow-state between Agents\r\n\r\nTunnel packet (currently using GRE/UDP - unreliable, but later will use DSCP or PFC) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\r\nInterested Set Hinting\r\n\r\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\r\nDepending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.\r\nA tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.\r\nA VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.\r\nThe VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\r\nThe physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.\r\n\r\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.\r\nPort Migration\r\n\r\nThe typical use-case for port migration is VM migration. In the old approach to flow-state management, the MN Agent at the virtual port’s New Node didn’t have to do anything special. For every flow computation that required state, there would be a lookup in Cassandra.\r\n\r\nIn the new approach, flow computation at the New Node works as described above. However, for a few seconds after the port has migrated, flows will not find local state and flow computations will therefore be performed incorrectly. However, in the background the MN Agent knows that this port was recently owned/hosted by a different, Old Node. The New Node therefore pulls all the required flow state from the Old Node.\r\n\r\nConsider that when port1 migrates from Old Node to New Node, Old Node has all the flow-state required for (forward and return) flows that potentially ingress port1. Consider also that:\r\nif port1 is not in an Ingress Set, then no other node has all of port1’s ingress flows’ state. If Old Node is unavailable, the flow state may not be recoverable - this is acceptable because VM migration itself requires Old Node to be available for memory transfer.\r\nif port1 is in an Ingress Set, then nodes other than Old Node may be able to help recover the ingress flow state for the port.\r\n\r\nFlow state is indexed (and laid out contiguously in memory) by ingress port, and can be transferred from Old to New Node within a matter of seconds (this needs to be clarified in terms of how much data per flow, how many flows, and in exactly what time delay).\r\nAgent restart\r\n\r\nIn the old approach, the MN Agent would flush all its state (including flow rules) on every restart. When recomputing flows, the Agent would simply do lookups in Cassandra.\r\n\r\nIn the new approach, the MN Agent either flushes its per-flow state to disk periodically, or, in the case of ports belonging to Ingress Sets, may avoid disk-writes by recovering the flow-state from another Node that hosts a port in the same Ingress Set. Recovering from peers avoids disk writes on heavily loaded nodes like L3 Gateways.\r\n\r\nMore Advanced Cases: real connection tracking, DPI information.\r\n\r\nIn the new Distributed Flow State management, many nodes may potentially update flow state after it’s created...\r\n\r\n&nbsp;\r\n\r\n&nbsp;',155,'http://blog.midonet.org/155-revision-v1/',0,'revision','',0),(157,5,'2014-12-04 12:57:31','2014-12-04 12:57:31','','MidoNetNeutronOverlay','','inherit','open','open','','midonetneutronoverlay','','','2014-12-04 12:57:31','2014-12-04 12:57:31','',90,'http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png',0,'attachment','image/png',0),(158,5,'2014-12-04 14:06:45','2014-12-04 14:06:45','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" />\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-04 14:06:45','2014-12-04 14:06:45','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<img class=\"alignnone wp-image-148 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Drawing1-e1417679800602.png\" alt=\"Drawing1\" width=\"600\" height=\"414\" />\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(159,5,'2014-12-04 14:09:40','2014-12-04 14:09:40','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone size-medium wp-image-157\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay-300x207.png\" alt=\"MidoNetNeutronOverlay\" width=\"300\" height=\"207\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet and context get kicked up to the Simulation stage of the MN Agent. The Simulation stage first translates physical layer concepts to overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, in the virtual layer that translates to ingressing virtual bridge port P5.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\n<ul>\n    <li>The packet would arrive on P10 and hit Bridge10 (corresponding to Network10).</li>\n    <li>I alluded to this in the previous post, MN\'s virtual bridge has a MAC-IP table and it answers ARPs when it can. The bridge\'s ARP table, <em>as well as its MAC-table</em>, is seeded as ports are created on the corresponding Neutron network (MAC and IPs are generated and associated with the port ID in Neutron\'s DB). The virtual bridge will therefore <em>consume</em> the ARP request, generate an ARP reply and emit it from P10.</li>\n    <li>The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101.</li>\n    <li>It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the consumed ARP request because the datapath doesn\'t buffer missed packets.</li>\n</ul>\n</li>\n    <li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\n<ul>\n    <li>The packet would arrive on P10 and hit Bridge10. Since the bridge\'s MAC-table was already seeded with an entry for the Router\'s</li>\n    <li></li>\n</ul>\n</li>\n    <li>TO BE COMPLETED</li>\n</ol>\n\n&nbsp;\n\n&nbsp;\n\nIf multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\n\n<ul>\n    <li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254\n<ul>\n    <li>Assume the</li>\n</ul>\n</li>\n    <li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\n</ul>\n\nSince the two Networks belong to the same Tenant, they will be connected to the same Neutron router. Equivalently, MN\'s overlay will have the corresponding virtual bridges linked to a single virtual router.\n\n<ol>\n    <li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\n<ul>\n    <li>The packet would arrive on P10 and hit Bridge10 (corresponding to Network10).</li>\n    <li>I alluded to this in the previous post, MN\'s virtual bridge has a MAC-IP table and it answers ARPs when it can. The bridge\'s ARP table, <em>as well as its MAC-table</em>, is seeded as ports are created on the corresponding Neutron network (MAC and IPs are generated and associated with the port ID in Neutron\'s DB). The virtual bridge will therefore <em>consume</em> the ARP request, generate an ARP reply and emit it from P10.</li>\n    <li>The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101.</li>\n    <li>It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the consumed ARP request because the datapath doesn\'t buffer missed packets.</li>\n</ul>\n</li>\n    <li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\n    <li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\n<ul>\n    <li>The packet would arrive on P10 and hit Bridge10. Since the bridge\'s MAC-table was already seeded with an entry for the Router\'s</li>\n    <li></li>\n</ul>\n</li>\n    <li>TO BE COMPLETED</li>\n</ol>\n\n&nbsp;','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-04 14:09:40','2014-12-04 14:09:40','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone size-medium wp-image-157\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay-300x207.png\" alt=\"MidoNetNeutronOverlay\" width=\"300\" height=\"207\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet and context get kicked up to the Simulation stage of the MN Agent. The Simulation stage first translates physical layer concepts to overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, in the virtual layer that translates to ingressing virtual bridge port P5.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\r\n<ul>\r\n	<li>The packet would arrive on P10 and hit Bridge10 (corresponding to Network10).</li>\r\n	<li>I alluded to this in the previous post, MN\'s virtual bridge has a MAC-IP table and it answers ARPs when it can. The bridge\'s ARP table, <em>as well as its MAC-table</em>, is seeded as ports are created on the corresponding Neutron network (MAC and IPs are generated and associated with the port ID in Neutron\'s DB). The virtual bridge will therefore <em>consume</em> the ARP request, generate an ARP reply and emit it from P10.</li>\r\n	<li>The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101.</li>\r\n	<li>It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the consumed ARP request because the datapath doesn\'t buffer missed packets.</li>\r\n</ul>\r\n</li>\r\n	<li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\r\n<ul>\r\n	<li>The packet would arrive on P10 and hit Bridge10. Since the bridge\'s MAC-table was already seeded with an entry for the Router\'s</li>\r\n	<li></li>\r\n</ul>\r\n</li>\r\n	<li>TO BE COMPLETED</li>\r\n</ol>\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\nIf multiple VM instances have been launched on one or more Networks/Subnets of the same Tenant, then at this point they can already communicate with each other. For example, let\'s say that:\r\n<ul>\r\n	<li>VM1 on port P10 of Network10 of TenantA (mapped to tap101 on Host1) has been assigned address 10.10.10.1 and its gateway is 10.10.10.254\r\n<ul>\r\n	<li>Assume the</li>\r\n</ul>\r\n</li>\r\n	<li>VM2 on port P11 of Network20 of same TenantA (mapped to tap201 on Host2) has been assigned address 10.10.20.1 and its gateway is 10.10.20.254</li>\r\n</ul>\r\nSince the two Networks belong to the same Tenant, they will be connected to the same Neutron router. Equivalently, MN\'s overlay will have the corresponding virtual bridges linked to a single virtual router.\r\n<ol>\r\n	<li>VM1 will ARP for 10.10.10.254. The ARP packet will miss in the datapath and be kicked up to the MN Agent on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\r\n<ul>\r\n	<li>The packet would arrive on P10 and hit Bridge10 (corresponding to Network10).</li>\r\n	<li>I alluded to this in the previous post, MN\'s virtual bridge has a MAC-IP table and it answers ARPs when it can. The bridge\'s ARP table, <em>as well as its MAC-table</em>, is seeded as ports are created on the corresponding Neutron network (MAC and IPs are generated and associated with the port ID in Neutron\'s DB). The virtual bridge will therefore <em>consume</em> the ARP request, generate an ARP reply and emit it from P10.</li>\r\n	<li>The simulation would conclude and the MN Agent on Host1, knowing that P10 is bound to a local interface, would interpret that the ARP reply packet should be emitted from tap101.</li>\r\n	<li>It would issue a Netlink request to its datapath to do so. Nothing needs to be done for the consumed ARP request because the datapath doesn\'t buffer missed packets.</li>\r\n</ul>\r\n</li>\r\n	<li>VM1 would receive the ARP reply and add an entry to its neighbor table. Then it would emit the ping (ICMP echo request) packet to 10.10.20.1 with Ethernet destination set to the virtual router\'s MAC. The ping will miss in the datapath and be kicked up to the MN Agent, again on Host1.</li>\r\n	<li>The MN Agent will run a simulation to figure out how the overlay topology would treat this packet.\r\n<ul>\r\n	<li>The packet would arrive on P10 and hit Bridge10. Since the bridge\'s MAC-table was already seeded with an entry for the Router\'s</li>\r\n	<li></li>\r\n</ul>\r\n</li>\r\n	<li>TO BE COMPLETED</li>\r\n</ol>\r\n&nbsp;',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(160,5,'2014-12-04 16:01:26','2014-12-04 16:01:26','','MNPacketFlow4','','inherit','open','open','','mnpacketflow4','','','2014-12-04 16:01:26','2014-12-04 16:01:26','',90,'http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow4.png',0,'attachment','image/png',0),(161,5,'2014-12-04 16:01:27','2014-12-04 16:01:27','','MNPacketFlow3','This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.','inherit','open','open','','mnpacketflow3','','','2014-12-04 16:40:32','2014-12-04 16:40:32','',90,'http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png',0,'attachment','image/png',0),(162,5,'2014-12-04 16:01:28','2014-12-04 16:01:28','','MNPacketFlow2','This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.','inherit','open','open','','mnpacketflow2','','','2014-12-04 16:39:18','2014-12-04 16:39:18','',90,'http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png',0,'attachment','image/png',0),(163,5,'2014-12-04 16:01:29','2014-12-04 16:01:29','','MNPacketFlow1','','inherit','open','open','','mnpacketflow1','','','2014-12-04 16:01:29','2014-12-04 16:01:29','',90,'http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png',0,'attachment','image/png',0),(164,5,'2014-12-04 16:56:55','2014-12-04 16:56:55','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone size-medium wp-image-157\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay-300x207.png\" alt=\"MidoNetNeutronOverlay\" width=\"300\" height=\"207\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1-300x269.png\" alt=\"MNPacketFlow1\" width=\"300\" height=\"269\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n[gallery ids=\"162,161\"]\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-04 16:56:55','2014-12-04 16:56:55','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone size-medium wp-image-157\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay-300x207.png\" alt=\"MidoNetNeutronOverlay\" width=\"300\" height=\"207\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\r\n<ul>\r\n	<li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\r\n	<li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\r\n	<li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\r\n<ul>\r\n	<li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\r\n	<li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\r\n	<li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\r\n	<li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\r\n	<li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\r\n	<li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\r\n</ol>\r\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1-300x269.png\" alt=\"MNPacketFlow1\" width=\"300\" height=\"269\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\r\n\r\n&nbsp;\r\n\r\n[gallery ids=\"162,161\"]\r\n\r\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\r\n<ol>\r\n	<li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\r\n	<li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\r\n	<li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\r\n	<li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\r\n<ul>\r\n	<li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\r\n</ul>\r\n</li>\r\n	<li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\r\n</ol>\r\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\r\n\r\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(165,5,'2014-12-04 16:59:25','2014-12-04 16:59:25','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone  wp-image-157\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay-300x207.png\" alt=\"MidoNetNeutronOverlay\" width=\"542\" height=\"374\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1-300x269.png\" alt=\"MNPacketFlow1\" width=\"300\" height=\"269\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n[gallery ids=\"162,161\"]\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-04 16:59:25','2014-12-04 16:59:25','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone  wp-image-157\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay-300x207.png\" alt=\"MidoNetNeutronOverlay\" width=\"542\" height=\"374\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\r\n<ul>\r\n	<li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\r\n	<li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\r\n	<li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\r\n<ul>\r\n	<li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\r\n	<li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\r\n	<li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\r\n	<li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\r\n	<li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\r\n	<li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\r\n</ol>\r\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1-300x269.png\" alt=\"MNPacketFlow1\" width=\"300\" height=\"269\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\r\n\r\n&nbsp;\r\n\r\n[gallery ids=\"162,161\"]\r\n\r\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\r\n<ol>\r\n	<li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\r\n	<li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\r\n	<li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\r\n	<li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\r\n<ul>\r\n	<li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\r\n</ul>\r\n</li>\r\n	<li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\r\n</ol>\r\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\r\n\r\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(166,5,'2014-12-04 17:01:21','2014-12-04 17:01:21','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1-300x269.png\" alt=\"MNPacketFlow1\" width=\"300\" height=\"269\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n[gallery ids=\"162,161\"]\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-04 17:01:21','2014-12-04 17:01:21','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\r\n<ul>\r\n	<li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\r\n	<li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\r\n	<li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\r\n<ul>\r\n	<li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\r\n	<li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\r\n	<li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\r\n	<li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\r\n	<li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\r\n	<li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\r\n</ol>\r\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1-300x269.png\" alt=\"MNPacketFlow1\" width=\"300\" height=\"269\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\r\n\r\n&nbsp;\r\n\r\n[gallery ids=\"162,161\"]\r\n\r\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\r\n<ol>\r\n	<li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\r\n	<li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\r\n	<li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\r\n	<li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\r\n<ul>\r\n	<li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\r\n</ul>\r\n</li>\r\n	<li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\r\n</ol>\r\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\r\n\r\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(167,2,'2014-12-04 17:35:29','0000-00-00 00:00:00','&nbsp;\n\n<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>\n\nAt the last <a href=\"http://openstack.jp/\">OpenStack Japan</a> user meetup,\n\nhttp://www.slideshare.net/enakai/midonet-technology-internalsv10','MidoNet Community Newsletter Issue #1','','draft','open','open','','','','','2014-12-04 17:35:29','2014-12-04 17:35:29','&nbsp;\r\n### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan\r\nAt the last <a href=\"http://openstack.jp/\">OpenStack Japan</a> user meetup, \r\n\r\nhttp://www.slideshare.net/enakai/midonet-technology-internalsv10',0,'http://blog.midonet.org/?p=167',0,'post','',0),(168,5,'2014-12-04 17:25:20','2014-12-04 17:25:20','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n[gallery columns=\"2\" ids=\"162,161\"]\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-04 17:25:20','2014-12-04 17:25:20','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\r\n<ul>\r\n	<li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\r\n	<li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\r\n	<li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\r\n<ul>\r\n	<li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\r\n	<li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\r\n	<li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\r\n	<li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\r\n	<li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\r\n	<li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\r\n</ol>\r\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\r\n\r\n&nbsp;\r\n\r\n[gallery columns=\"2\" ids=\"162,161\"]\r\n\r\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\r\n<ol>\r\n	<li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\r\n	<li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\r\n	<li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\r\n	<li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\r\n<ul>\r\n	<li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\r\n</ul>\r\n</li>\r\n	<li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\r\n</ol>\r\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\r\n\r\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(169,2,'2014-12-04 17:27:03','2014-12-04 17:27:03','<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>','MidoNet Community Newsletter Issue #1','','inherit','open','open','','167-revision-v1','','','2014-12-04 17:27:03','2014-12-04 17:27:03','### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan',167,'http://blog.midonet.org/167-revision-v1/',0,'revision','',0),(170,5,'2014-12-04 17:27:55','2014-12-04 17:27:55','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\n\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n<ul>\n    <li>The Provider Router port P2 has been linked to the Tenant Router port P1.\n<ul>\n    <li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\n    <li>The Tenant router has a default route to the Provider via P1.</li>\n</ul>\n</li>\n    <li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\n    <li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\n    <li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\n<ul>\n    <li>tap123 inserted into the OVS datapath as port #10.</li>\n</ul>\n</li>\n    <li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\n    <li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\n</ul>\n\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\n\n<ol>\n    <li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\n    <li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\n    <li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\n    <li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\n<ul>\n    <li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\n    <li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\n</ul>\n</li>\n    <li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\n    <li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\n<ul>\n    <li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\n    <li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\n    <li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\n    <li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\n    <li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\n</ul>\n</li>\n    <li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\n    <li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\n</ol>\n\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\n\n&nbsp;\n\n[caption id=\"attachment_162\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\"><img class=\"wp-image-162 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\" alt=\"This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.\" width=\"415\" height=\"373\" /></a> This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.[/caption]\n\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\n\n<ol>\n    <li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\n    <li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\n    <li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\n    <li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\n<ul>\n    <li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\n</ul>\n</li>\n    <li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\n</ol>\n\n[caption id=\"attachment_161\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\"><img class=\"wp-image-161 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\" alt=\"This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.[/caption]\n\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\n\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".','Introduction to MN\'s Overlay Network Models (Part 3 - Simulations)','','inherit','open','open','','90-revision-v1','','','2014-12-04 17:27:55','2014-12-04 17:27:55','In this article we explain how the MN Agent <em>simulates</em> the overlay topology. This post is the third in a series intended to familiarize users with MidoNet\'s overlay virtual networking models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router and in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we discussed Tenant Routers and Networks.\r\n\r\nWhere we left off in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a>, the following had been set up:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\r\n<ul>\r\n	<li>The Provider Router port P2 has been linked to the Tenant Router port P1.\r\n<ul>\r\n	<li>The Provider router has a route all traffic to 20.20.0.20 to the Tenant via P2.</li>\r\n	<li>The Tenant router has a default route to the Provider via P1.</li>\r\n</ul>\r\n</li>\r\n	<li>The Tenant Router uses the 20.20.0.20 address for port masquerading.</li>\r\n	<li>The Tenant Router has a port on the 10.10.0.0/24 subnet with address 10.10.0.254.</li>\r\n	<li>A VM instance launched on Compute Host 5 has its vNIC connected to tap123.\r\n<ul>\r\n	<li>tap123 inserted into the OVS datapath as port #10.</li>\r\n</ul>\r\n</li>\r\n	<li>The VM is on the 10.10.0.0/24 subnet and has learned its IP, 10.10.0.1, and its gateway IP, 10.10.0.254, via DHCP.</li>\r\n	<li>The MidoNet virtual bridge that implements the Neutron Network has been seeded with MAC-table and ARP-table entries for both the Router\'s and VM\'s MAC and IP.</li>\r\n</ul>\r\nWhat happens when the VM sends an ARP request to resolve the gateway IP, 10.10.0.254?\r\n<ol>\r\n	<li>The ARP packet misses in the datapath (OVS/kernel or DPDK-based flow switch) and is kicked up (via Netlink) to the MN Agent on Host5. <em>This is exactly like Open vSwitch in-kernel datapath kicking up a packet to the vSwitch user-space daemon.</em></li>\r\n	<li>The MN Agent checks its in-process copy of the datapath flow table (this part is the same as with Open vSwitch daemon) for a match. A match occurs when a second or subsequent packet in a flow races with a request from the user-space agent to the datapath (via Netlink) to install a flow rule. If there\'s a flow with no actions, the packet is dropped. If there\'s a flow with a non-empty list of actions, the MN Agent sends a request to the datapath to apply those actions to the packet (the actions may change header fields and emit the packet from datapath ports).</li>\r\n	<li>If there was no match in the MN Agent\'s copy of the datapath flow table, then a user-space Wildcard flow table is checked. <em>Note that even when the datapath supports wildcarding features, MN Agent may not fully use them (e.g. prefers more granular flows for statistics/counting purposes).</em> If there\'s a match in the Wildcard flow table, the MN Agent copies the actions and sends two requests to the datapath: 1) to install a flow with the packet\'s header and applying those actions (even if the actions are an empty-list, meaning \"drop\", and 2) if the actions are a non-empty list, to apply the actions to the packet that missed in the datapath (this is necessary because the datapath does not buffer packets it kicks up to user-space).</li>\r\n	<li>If there was no match in the Wildcard flow table, then the packet should go to the Simulation stage of the MN Agent. Two things have to happen first:\r\n<ul>\r\n	<li>There may already be a packet with an identical header (if the sender is sending fast enough) in the Simulation phase. A prep phase performs de-duplication: if an identical Simulation is already in progress, this packet gets buffered and will receive the same treatment determined by the in-progress Simulation. There is a limit to the number of packets the MN Agent will buffer this way.</li>\r\n	<li>The Simulation only understands overlay topology elements. So the prep-stage also translates physical layer concepts into overlay/virtual layer concepts. For example, the packet ingressed datapath-port #10, that translates to ingressing the overlay topology at virtual bridge port P5. Now the packet is ready for Simulation.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation fetches an local representation of port P5. If there\'s a port-level firewall, it would be a filter inside the overlay port object. This is where MidoNet implements Neutron Security Groups and Anti-spoofing. That\'s a topic of <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> so for now let\'s assume there\'s no port-level firewall. The Simulation knows the packet traverses the port in-bound (from the perspective of the virtual bridge) and therefore the packet enters the virtual bridge.</li>\r\n	<li>The Simulation fetches an internal representation of the virtual bridge, along with its MAC table and ARP table.\r\n<ul>\r\n	<li>MAC learning is performed on the source address. If the MAC table does not already contains an entry mapping the packet\'s source MAC to P5, then the Simulation registers a device state change to add such an entry (possibly replacing an older entry for that MAC). The change is processed after the Simulation completes, and propagates to any other host that is caching this virtual bridge.</li>\r\n	<li>The bridge\'s pre-forwarding filters are evaluated/simulated. We\'ll assume no pre-forwarding filters are set.</li>\r\n	<li>The bridge recognizes this is an ARP request and checks its ARP table to see if there\'s an entry for 10.10.0.254. There is and it resolves to the P4\'s MAC address. This entry was added when the Router\'s Neutron port was created. At that time, the MidoNet\'s virtual router\'s P4 port was created, as was the virtual bridge\'s P3 port, and the two were linked (MidoNet stores a \"peer\" port ID in each of P3 and P4\'s configuration that represents a <em>linked-to-port</em>).</li>\r\n	<li>The bridge generates the corresponding ARP reply packet and queues it for emission from port P5. This will be handled in a separate simulation.</li>\r\n	<li>Since the bridge is able to answer the ARP it <em>consumes</em> the packet.</li>\r\n</ul>\r\n</li>\r\n	<li>The Simulation terminates indicating that the packet was consumed. The packet+results are transferred back to the prep stage to reverse the overlay/underlay translation and then to apply the same result any buffered packets with the same header. In this case, since the packet was \"consumed\" we would just discard those packets (as a form of DOS protection).</li>\r\n	<li>Normally packet+results are passed back to the Wildcard Flow Table stage. In this case the Simulation did not install any flow, so handling for this packet is already complete.</li>\r\n</ol>\r\n[caption id=\"attachment_163\" align=\"aligncenter\" width=\"415\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\"><img class=\"wp-image-163 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow1.png\" alt=\"MNPacketFlow1\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the VM, datapath, and MN Agent are connected.[/caption]\r\n\r\n&nbsp;\r\n\r\n[caption id=\"attachment_162\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\"><img class=\"wp-image-162 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow2.png\" alt=\"This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.\" width=\"415\" height=\"373\" /></a> This illustrates the path of the ARP request from VM through datapath to MN Agent and through its packet processing stages.[/caption]\r\n\r\nLet\'s briefly describe what happens to that ARP reply packet that was queued for Simulation:\r\n<ol>\r\n	<li>This packet was generated by the virtual topology and starts its life in the Simulation stage. Its context indicates that the virtual bridge is emitting it from P5.</li>\r\n	<li>The Simulation fetches the local representation of P5. If there were an egress/out-bound (from the perspective of the port\'s owner, the bridge) filter, the Simulation would evaluate it. There isn\'t so the Simulation determines that the packet should be emitted from P5.</li>\r\n	<li>The Simulation realizes that P5 is an <em>exterior port</em>, in the sense that it\'s at the boundary of the overlay and physical worlds. The Simulation therefore terminates with the result being an action \"Forward(P5)\".</li>\r\n	<li>Control passes to the Simulation prep phase where reverse translation of overlay-to-physical concepts occurs. The result action becomes \"Forward(datapath-port #10)\". Since this is a topology-generated packet, there are no buffered packets with identical headers.\r\n<ul>\r\n	<li>This stage also realizes there are no flows to install. It enqueues a Netlink request that sends the packet and the now datapath-compatible action to the datapath.</li>\r\n</ul>\r\n</li>\r\n	<li>When the datapath receives the Netlink request, it will apply the Forward action to the packet included in the request. As a result the VM will receive the ARP reply.</li>\r\n</ol>\r\n[caption id=\"attachment_161\" align=\"aligncenter\" width=\"415\" class=\" \"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\"><img class=\"wp-image-161 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNPacketFlow3.png\" alt=\"This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.\" width=\"415\" height=\"373\" /></a> This diagram illustrates how the ARP reply generated by the virtual bridge starts its life in the Simulation Stage, traverses MN\'s packet-processing stages, and is forwarded to the datapath for emission towards the VM.[/caption]\r\n\r\nIf the VM were to send a Ping request to the gateway IP, 10.10.0.254, the description would be very similar, with the difference that the Simulation would have the packet actually reach the Router, and the Router would consume the packet and emit a Ping reply.\r\n\r\nWe haven\'t illustrated a case where a packet results in a flow being installed to drop packets, or to forward packets to another local VM, or to forward packets to a remote VM via a tunnel. We\'ll leave that for a future post. Our goal in this article was to give the reader an understanding of MN Agent\'s packet processing workflow and what we mean by topology \"Simulation\".',90,'http://blog.midonet.org/90-revision-v1/',0,'revision','',0),(171,2,'2014-12-04 17:28:18','2014-12-04 17:28:18','&nbsp; <br />\n\n<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>','MidoNet Community Newsletter Issue #1','','inherit','open','open','','167-revision-v1','','','2014-12-04 17:28:18','2014-12-04 17:28:18','&nbsp; <br />\r\n\r\n\r\n### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan',167,'http://blog.midonet.org/167-revision-v1/',0,'revision','',0),(172,2,'2014-12-04 17:28:30','2014-12-04 17:28:30','&nbsp;\n\n<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>','MidoNet Community Newsletter Issue #1','','inherit','open','open','','167-revision-v1','','','2014-12-04 17:28:30','2014-12-04 17:28:30','&nbsp; \r\n\r\n### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan',167,'http://blog.midonet.org/167-revision-v1/',0,'revision','',0),(173,2,'2014-12-04 17:28:52','2014-12-04 17:28:52','<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>','MidoNet Community Newsletter Issue #1','','inherit','open','open','','167-revision-v1','','','2014-12-04 17:28:52','2014-12-04 17:28:52','\r\n### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan',167,'http://blog.midonet.org/167-revision-v1/',0,'revision','',0),(174,5,'2014-12-04 17:30:02','2014-12-04 17:30:02','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\n\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\n\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\n\n<ol>\n    <li>Create a Neutron Router.</li>\n    <li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\n    <li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\n    <li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\n    <li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\n    <li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\n</ol>\n\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\n\n<ol>\n    <li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\n    <li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\n<ul>\n    <li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\n    <li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\n</ul>\n</li>\n    <li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\n<ul>\n    <li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\n    <li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\n    <li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\n</ul>\n</li>\n    <li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\n    <li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\n    <li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\n<ul>\n    <li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\n    <li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\n</ul>\n</li>\n</ol>\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\n\n&nbsp;\n\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).','Introduction to MN\'s Overlay Network Models (Part 2 - Tenant Routers and Bridges)','','inherit','open','open','','79-revision-v1','','','2014-12-04 17:30:02','2014-12-04 17:30:02','This post is the second in a series intended to familiarize users with MidoNet\'s overlay virtual networking approach and models. In <a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">part 1</a> we discussed MN\'s Provider Router. In this article we discuss Tenant Routers and Bridges.\r\n\r\nA <em>Tenant</em> (or <em>Project</em> in OpenStack\'s terminology) is an organizational unit that shares ownership of a set of virtual devices. For example, in MidoNet a single Tenant may own a set of virtual routers, virtual bridges and rules/chains; similarly, in Neutron a Project may own a set of routers, networks, subnets, security groups in Neutron.\r\n\r\nIn OpenStack/Neutron, typically each Project owns a single Router and one or more Networks. One possible Tenant workflow is:\r\n<ol>\r\n	<li>Create a Neutron Router.</li>\r\n	<li>Set the Router\'s gateway - this is an External Network, and an IP address in one of that network\'s prefixes.</li>\r\n	<li>(Implicit/automatic unfortunately) An IP from the FloatingIP (e.g. 20.20.0.20) range is allocated and port-masquerading set up for traffic traversing the Router\'s uplink.</li>\r\n	<li>Create a Neutron Network and name it. Create a Subnet and associate it with that Network. This specifies the IP address range for that network (e.g. 10.10.0.0/24), the gateway address (e.g. 10.10.0.254) and some DHCP options. Multiple Subnets are allowed; both IPv4 and IPv6 ranges are allowed.</li>\r\n	<li>Add an interface to the Router (<em>neutron router-interface-add</em> CLI command) on the Subnet(s) - this connects the Router to the Subnet, and assigns it the specified gateway address. A single port will be created on the Subnet\'s Network.  If the router has an interface on multiple Subnets of the same Network, the same port will be re-used.</li>\r\n	<li>Launch VM instances. For each instance, specify the number of vNICs, and for each vNIC what Network it should be attached to. Neutron will automatically create one port per vNIC on the appropriate Network. For each Network port created, Neutron generates one MAC address and chooses one IP address from each Subnet range. Continuing the example above, assume IP 10.10.0.1 is chosen. The MAC and IP addresses are stored in Neutron DB, typically MySQL. Only then will Nova scheduler choose a compute host to spin up the instance and the Nova agent local to that host will create the VM with the appropriate number of vNICs.</li>\r\n</ol>\r\nHere\'s what happens in MidoNet\'s low-level models for each of those workflow steps:\r\n<ol>\r\n	<li>A MidoNet virtual router is created and stored in ZooKeeper. MN virtual routers are completely distributed and <em>simulated</em> at the agent/software switch at flow computation/installation time.</li>\r\n	<li>A virtual port P1, is created on the tenant virtual router to serve as an uplink; a port P2 is created on the Provider Router. P1 and P2 are linked, and the virtual router\'s routing table gets a default route to the Provider Router via that link.\r\n<ul>\r\n	<li>Neutron doesn\'t have explicit IPAM, just default behavior. The Tenant is meant to be a private domain. Therefore, at this step the Provider Router has no route via that link. Outside traffic will not be forwarded to the Tenant\'s router until a FloatingIP is allocated to the Tenant.</li>\r\n	<li>In MidoNet\'s terminology P1 and P2 are <em>interior</em> virtual ports. Interior virtual ports exist entirely within the overlay network and don\'t map to any physical device/port. In contrast <em>exterior</em> virtual ports are considered to be at the edge of the overlay and connect the overlay to a VM instance or to external L2 or L3 networks. Exterior ports must be associated with network interfaces (physical or logical) on physical hosts where MN Agents are installed.</li>\r\n</ul>\r\n</li>\r\n	<li>When the port-masquerading IP is allocated from the FloatingIP range (20.20.0.20 in this example), the Provider Router gets a /32 route (to that FloatingIP) via the link to the Tenant\'s router. The Tenant Router\'s Post-routing Chain gets a rule that matches packets egressing the uplink and with private source address and applies a SNAT: the source IP is translated to the FloatingIP, the source L4 port is translated to a dynamically chosen value in the privileged or ephemeral port range according to whether the original source port was privileged or not. The Tenant router\'s Pre-routing Chain gets a rule that matches packets ingressing the uplink and with destination IP matching the FloatingIP and that reverses the SNAT by looking up the translation in the forward flow\'s state.\r\n<ul>\r\n	<li>MidoNet\'s port-masquerading is entirely distributed and is decided flow-by-flow by the MN Agent local to the flow. It does not require forwarding the packets through an L3 namespace or router appliance.</li>\r\n	<li>MidoNet\'s Chains and Rules will be described in detail in a subsequent post.</li>\r\n	<li>FloatingIP\'s used in the normal way (statically mapped to a single VM/private IP) result in static NAT rules in the Tenant router\'s Pre-routing and Post-routing Chains. This will be described in a subsequent post.</li>\r\n</ul>\r\n</li>\r\n	<li>When the Neutron Network is created, a corresponding MidoNet virtual bridge is created. When a Subnet is created, a corresponding MidoNet DHCPSubnet object is created. Any information related to the Subnet is stored in the DHCPSubnet object in ZooKeeper.</li>\r\n	<li>When the Router is connected to the Network/Subnets, a port P3 is created on the corresponding MN virtual bridge, it will serve as the bridge\'s uplink. A port P4 is created on the Router with IP address/prefix equal to the gateway IP specified in the Subnet (10.10.0.254 in the example above); a 10.10.0.254/32 <em>Local</em> route and a route to the prefix 10.10.0.0/24, both via P4, are added to the Tenant Router\'s routing table. The /32 route allows the router to recognize traffic to P4 that arrives via a different port. The virtual bridge gets two static entries: in the mac-table, the router P4 port\'s MAC must map to bridge port P3; in the ARP table (the bridge answers ARPs when it can) the router P4 port\'s IP must map to its MAC.</li>\r\n	<li>When a Neutron port is created for a VM, MidoNet creates a corresponding <em>exterior</em> port, let\'s call it P5, on the appropriate virtual bridge. MidoNet stores the selected (by Neutron) MAC and IP addresses in the DHCPSubnet object associated with that bridge. When Nova agent, let\'s say on Compute Host5, launches a VM instance (typically via libvirt and KVM) it creates software interfaces (taps) for each of the VM\'s vNICs and then invokes a Python hook that enables a Neutron-plugin-or-driver-specific callbacks. Let\'s assume <em>tap123</em> was created for P5. MidoNet\'s hook code makes a call to the MidoNet API to tell it that \"tap123 on Host5 is bound to P5\". MN API stores this information in Apache ZooKeeper in a directory specific to Host5. The MN Agent on Host5 is watching that directory and realizes that it needs to plug tap123 into its datapath. MN Agent therefore makes a netlink call to the OVS datapath to insert tap123 as a netdev device, and tap123 appears as port #10 in the datapath.\r\n<ul>\r\n	<li>When the VM finishes booting it will issue a DHCP message of type <em>Discover</em>. The packet will miss in the datapath and will be kicked up to MN Agent in userspace (same as OVS kmod kicking missed packets up to OVS vSwitchd). The MN Agent realizes that the packet came from tap123 and therefore from P5 in the overlay topology. The Agent checks whether it can generate the DHCP reply (an <em>Offer</em> message) by looking for the <em>Discover</em>\'s source MAC in the Bridge\'s DHCPSubnet. In this case it will find the MAC-IP mapping and therefore generates a DHCP Offer with the appropriate 10.10.0.1 IP address offer, and any additional options (default routes, non-default routes, DNS servers) specified in the Subnet. (And similarly for the DHCP <em>Request</em> and <em>Acknowledge</em> that will soon follow).</li>\r\n	<li>Note that the DHCP responses are generated by the MN Agent local to the VM\'s host. This is a common theme in MidoNet, we try to do as much work as possible at the edge. The MN Agent is aware of the overlay topology model, that\'s why we refer to this approach as <em>Topology-Aware Switch</em>.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\"><img class=\"alignnone wp-image-157 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MidoNetNeutronOverlay.png\" alt=\"MidoNetNeutronOverlay\" width=\"743\" height=\"513\" /></a>\r\n\r\n&nbsp;\r\n\r\nReaders familiar with Neutron will have noticed that I omitted Security Groups. Part 3 of this series will discuss Security Groups as well as Floating IPs (not the port-masquerading kind).',79,'http://blog.midonet.org/79-revision-v1/',0,'revision','',0),(175,5,'2014-12-04 17:32:36','2014-12-04 17:32:36','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\n\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\n\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\n\n<h1>MidoNet’s Provider Router</h1>\n\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\n\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\n\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\n\n<ol>\n    <li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\n    <li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\n</ol>\n\n[caption id=\"attachment_73\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\"><img class=\"wp-image-73 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\" alt=\"Inter-tenant connectivity via Neutron External Network\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via Neutron External Network[/caption]\n\n[caption id=\"attachment_72\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\"><img class=\"wp-image-72 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\" alt=\"Inter-tenant connectivity via MN Provider Router\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via MN Provider Router[/caption]\n\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\n\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\n\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\n\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\n\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.','Introduction to MN\'s Overlay Network Models (Part 1 - Provider Router)','','inherit','open','open','','71-revision-v1','','','2014-12-04 17:32:36','2014-12-04 17:32:36','In this series of articles we’ll discuss how the overlay network is modeled in MidoNet. Although the concepts also apply to non-OpenStack setups, we’ll focus on OpenStack and point out how Neutron concepts relate to MidoNet concepts. In this post we focus on MidoNet\'s <em>Provider Router</em>, in the <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> we\'ll discuss Tenant Routers.\r\n\r\nNote that all of Neutron’s models are <em>native</em> in <a href=\"http://docs.midonet.org/docs/latest/rest-api/content/index.html\">MidoNet’s API</a>, but MidoNet’s API also has some <em>lower-level</em> models. MidoNet’s agents understand all the lower-level models, but only some of the Neutron models, so MidoNet’s API translates some Neutron models to low-level models. The Neutron models and the low-level models are stored in Apache ZooKeeper and from there propagated to the MidoNet agents as needed.\r\n\r\nFinally, as we discuss the overlay models, remember that the concepts don’t necessarily map 1-1 to physical concepts.\r\n<h1>MidoNet’s Provider Router</h1>\r\nA typical MidoNet deployment (certainly any MidoNet/OpenStack deployment) will have a single router which we call <em>Provider Router</em>. Don’t confuse this with Neutron’s “provider” concept. MidoNet’s Provider Router is an overlay (read: virtual/logical) router, owned by the cloud operator, that provides L3 connectivity between Tenants or between Tenants and the Internet.\r\n\r\nIn a typical deployment, the Provider Router has 3 uplinks. The Provider Router may have 3 ECMP default static routes, one for each of the uplink. Alternatively, BGP may be set up so that this router can dynamically learn uplink routes (and advertise its own).\r\n\r\nThe diagrams below show the difference between inter-networking Tenant Routers with MidoNet’s Provider Router vs. Neutron’s External Network. The External Network requires all Tenant Routers to be connected to the same L2 network and doesn’t have support for dynamic route learning and advertisement. In contrast, with MidoNet’s Provider Router:\r\n<ol>\r\n	<li>A flow from VM1 in Tenant A’s network to VM2 in Tenant B’s network doesn’t leave the overlay. Therefore MidoNet (and some other SDNs) can tunnel the flow directly from VM1’s host to VM2’s host.</li>\r\n	<li>All external as well inter-tenant traffic passes through the Provider Router’s uplink ports, providing a well-defined set of points to apply traffic policy, and learn or advertise routes. Note that the Provider Router can have any number of uplinks.</li>\r\n</ol>\r\n[caption id=\"attachment_73\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\"><img class=\"wp-image-73 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/ExternalNetworkRouting-e1417444328249.jpg\" alt=\"Inter-tenant connectivity via Neutron External Network\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via Neutron External Network[/caption]\r\n\r\n[caption id=\"attachment_72\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\"><img class=\"wp-image-72 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNProviderRouter.jpg\" alt=\"Inter-tenant connectivity via MN Provider Router\" width=\"720\" height=\"540\" /></a> Inter-tenant connectivity via MN Provider Router[/caption]\r\n\r\nIn a typical MidoNet deployment, the Provider Router is the first logical device created (via API) after the software has been installed. The deployer/admin chooses 3 commodity servers, one for each Provider Router uplink. Each of these servers is referred to as a <em>L3 Gateway Node</em>. In production deployments L3 Gateway Nodes are entirely dedicated to processing the North-South traffic for one Provider Router uplink, while in test deployments the L3 Gateway Node may also be an OpenStack Compute host. Each L3 Gateway Node should have a NIC dedicated to the uplink traffic.\r\n\r\nThe diagram below shows how the Provider Router uplinks are mapped to physical NICs on commodity hosts that act as L3 Gateway Nodes. For best throughput and to minimize fate-sharing, the Gateway Nodes should be placed in different racks. Not every rack needs a Gateway Node. The number of Gateway Nodes depends on the required North-South bandwidth for the entire cloud.\r\n\r\n[caption id=\"attachment_75\" align=\"alignnone\" width=\"720\"]<img class=\"wp-image-75 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PR_UplinksToPhys.jpg\" alt=\"MN_PR_UplinksToPhys\" width=\"720\" height=\"540\" /> The dashed red line shows how the Provider Router Uplinks (in the Overlay/Logical layer in the bottom half) map to physical NICs on the commodity X86 servers in each rack that act as L3 Gateway Nodes.[/caption]\r\n\r\nSo how does the Provider Router know about the operational state of its uplinks? Each uplink is explicitly bound (via an API call or CLI command) to a specific network interface on a host running the MN Agent. Assume Uplink1 is bound to eth0 on Host10. When the MN Agent on Host10 learns about the binding, it issues a call to the local datapath (e.g. a netlink call to the Open vSwitch kernel module datapath) to add the eth0 as a netdev port (the host IP network stack can no longer use eth0). When the new datapath port is correctly added to the datapath and if its operational state is UP, then the MN Agent will publish to Apache ZooKeeper that the virtual router port Uplink1 is UP and located on Host10. As a result of Uplink1 being UP, any route via Uplink1 (MN Route objects explicitly specify both their next hop gateway and their virtual router egress port) is added to the Provider Router\'s forwarding table. These routes will be automatically removed if Uplink1 goes down or if the MN Agent on Host10 fails.\r\n\r\nIn <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> of this series we\'ll discuss Tenant Routers.',71,'http://blog.midonet.org/71-revision-v1/',0,'revision','',0),(176,2,'2014-12-04 17:35:13','2014-12-04 17:35:13','&nbsp;\n\n<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>\n\nAt the last <a href=\"http://openstack.jp/\">OpenStack Japan</a> user meetup,','MidoNet Community Newsletter Issue #1','','inherit','open','open','','167-revision-v1','','','2014-12-04 17:35:13','2014-12-04 17:35:13','&nbsp;\r\n### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan\r\nAt the last <a href=\"http://openstack.jp/\">OpenStack Japan</a> user meetup, ',167,'http://blog.midonet.org/167-revision-v1/',0,'revision','',0),(177,2,'2014-12-04 17:35:29','2014-12-04 17:35:29','&nbsp;\n\n<h3>MidoNet Momentum</h3>\n\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\n\n<h3>MidoNet in Japan</h3>\n\nAt the last <a href=\"http://openstack.jp/\">OpenStack Japan</a> user meetup,\n\nhttp://www.slideshare.net/enakai/midonet-technology-internalsv10','MidoNet Community Newsletter Issue #1','','inherit','open','open','','167-revision-v1','','','2014-12-04 17:35:29','2014-12-04 17:35:29','&nbsp;\r\n### MidoNet Momentum\r\nIt\'s exciting to see momentum of MidoNet already taking off in such a short amount of time since launching. It\'s been about a month since open sourcing MidoNet, and we\'ve seen MidoStack cloned over a thousand times with hundreds of unique users. We\'ve even had a small number of contributions from new community members. We can\'t wait to see the continued growth of the MidoNet community!\r\n\r\n### MidoNet in Japan\r\nAt the last <a href=\"http://openstack.jp/\">OpenStack Japan</a> user meetup, \r\n\r\nhttp://www.slideshare.net/enakai/midonet-technology-internalsv10',167,'http://blog.midonet.org/167-revision-v1/',0,'revision','',0),(178,5,'2014-12-04 22:46:14','2014-12-04 22:46:14','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\nIn OpenStack, each Tenant has a unique default security group. This security group is initially empty and is created when the tenant\'s first network is created. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-04 22:46:14','2014-12-04 22:46:14','In this article we discuss how Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay virtual networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\nIn OpenStack, each Tenant has a unique default security group. This security group is initially empty and is created when the tenant\'s first network is created. MN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(179,5,'2014-12-06 14:30:07','2014-12-06 14:30:07','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\nIn OpenStack the term \"Security Group\" refers to two concepts:\n\n<ul>\n    <li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\n    <li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\n</ul>\n\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\n\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group. Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group begins to propagate immediately to all the ports in the Security Group.\n\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\n\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\n\n<ol>\n    <li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\n    <li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\n    <li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\n    <li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\n</ol>\n\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\n\nIn OpenStack, each Tenant has a unique default security group. This security group is initially empty and is created when the tenant\'s first network is created.\n\n&nbsp;\n\nMN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-06 14:30:07','2014-12-06 14:30:07','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\nIn OpenStack the term \"Security Group\" refers to two concepts:\r\n<ul>\r\n	<li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\r\n	<li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\r\n</ul>\r\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\r\n\r\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group. Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group begins to propagate immediately to all the ports in the Security Group.\r\n\r\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\r\n\r\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\r\n<ol>\r\n	<li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\r\n	<li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\r\n	<li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\r\n	<li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\r\n</ol>\r\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\r\n\r\nIn OpenStack, each Tenant has a unique default security group. This security group is initially empty and is created when the tenant\'s first network is created.\r\n\r\n&nbsp;\r\n\r\nMN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0);
INSERT INTO `wp_posts` VALUES (180,5,'2014-12-06 15:17:21','2014-12-06 15:17:21','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\n<h2>Intro to Security Groups</h2>\n\nIn OpenStack the term \"Security Group\" refers to two concepts:\n\n<ul>\n    <li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\n    <li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\n</ul>\n\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\n\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\n\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\n\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\n\n<ol>\n    <li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\n    <li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\n    <li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\n    <li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\n</ol>\n\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\n\n<h2>Security Groups in MidoNet</h2>\n\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\n\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\n\n<ul>\n    <li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\n    <li>clauses matching the packet\'s ingress port on a specific virtual device</li>\n    <li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\n    <li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\n</ul>\n\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\n\nRule-Chains are ordered lists of Rules.\n\nMN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\n\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\n\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-06 15:17:21','2014-12-06 15:17:21','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\n<h2>Intro to Security Groups</h2>\r\nIn OpenStack the term \"Security Group\" refers to two concepts:\r\n<ul>\r\n	<li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\r\n	<li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\r\n</ul>\r\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\r\n\r\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\r\n\r\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\r\n\r\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\r\n<ol>\r\n	<li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\r\n	<li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\r\n	<li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\r\n	<li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\r\n</ol>\r\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\r\n<h2>Security Groups in MidoNet</h2>\r\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\r\n\r\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\r\n<ul>\r\n	<li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\r\n	<li>clauses matching the packet\'s ingress port on a specific virtual device</li>\r\n	<li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\r\n	<li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\r\n</ul>\r\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\r\n\r\nRule-Chains are ordered lists of Rules.\r\n\r\nMN creates the corresponding MN-Rule-Chains (default inbound and outbound chains on the bridge port) when the Tenant is created.\r\n\r\n<i>Note about MN Chains: MN Chains are low-level models that don’t exist in Neutron. Each SecurityGroup (which includes both ingress and egress rules) is translated to two MN Chains… TO BE CONTINUED. The Chain names start with “SecGroup_” and end with the first N letters of the SecGroup name. The Chain UUIDs are unrelated to the SecGroup UUIDs (this is in contrast to MN Routers and Bridges whose UUIDs are equal to the Neutron Routers and Networks they translate, because those relationships are 1-1).</i>\r\n\r\nSecurity Groups (like Networks and Routers) belong to one Tenant. A Tenant can have any number of Security Groups and the Tenant’s VMs can be assigned to one ore more Security Groups.\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(181,5,'2014-12-06 16:06:45','2014-12-06 16:06:45','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\n<h2>Intro to Security Groups</h2>\n\nIn OpenStack the term \"Security Group\" refers to two concepts:\n\n<ul>\n    <li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\n    <li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\n</ul>\n\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\n\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\n\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\n\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\n\n<ol>\n    <li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\n    <li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\n    <li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\n    <li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\n</ol>\n\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\n\n<h2>MidoNet\'s Rule-Chains</h2>\n\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\n\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\n\n<ul>\n    <li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\n    <li>clauses matching the packet\'s ingress port on a specific virtual device</li>\n    <li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\n    <li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\n</ul>\n\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\n\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\n\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\n\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\n\n<h2>Security Groups in MidoNet</h2>\n\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\n\n<ul>\n    <li>Tenant A has two Security Groups:\n<ul>\n    <li>SG1 allows ICMP and HTTP traffic.</li>\n    <li>SG2 allows SSH traffic.</li>\n</ul>\n</li>\n    <li>Tenant A has two VMs:\n<ul>\n    <li>VM1 is bound to network Port1, which belongs to both Security Groups.</li>\n    <li>VM2 is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\n</ul>\n</li>\n</ul>\n\nHere\'s how MidoNet\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-06 16:06:45','2014-12-06 16:06:45','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\n<h2>Intro to Security Groups</h2>\r\nIn OpenStack the term \"Security Group\" refers to two concepts:\r\n<ul>\r\n	<li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\r\n	<li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\r\n</ul>\r\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\r\n\r\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\r\n\r\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\r\n\r\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\r\n<ol>\r\n	<li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\r\n	<li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\r\n	<li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\r\n	<li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\r\n</ol>\r\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\r\n<h2>MidoNet\'s Rule-Chains</h2>\r\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\r\n\r\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\r\n<ul>\r\n	<li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\r\n	<li>clauses matching the packet\'s ingress port on a specific virtual device</li>\r\n	<li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\r\n	<li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\r\n</ul>\r\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\r\n\r\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\r\n\r\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\r\n\r\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\r\n<h2>Security Groups in MidoNet</h2>\r\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\r\n<ul>\r\n	<li>Tenant A has two Security Groups:\r\n<ul>\r\n	<li>SG1 allows ICMP and HTTP traffic.</li>\r\n	<li>SG2 allows SSH traffic.</li>\r\n</ul>\r\n</li>\r\n	<li>Tenant A has two VMs:\r\n<ul>\r\n	<li>VM1 is bound to network Port1, which belongs to both Security Groups.</li>\r\n	<li>VM2 is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nHere\'s how MidoNet\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"300\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-medium\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups-300x225.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"300\" height=\"225\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(182,5,'2014-12-06 19:37:32','2014-12-06 19:37:32','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\n<h2>Intro to Security Groups</h2>\n\nIn OpenStack the term \"Security Group\" refers to two concepts:\n\n<ul>\n    <li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\n    <li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\n</ul>\n\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\n\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\n\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\n\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\n\n<ol>\n    <li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\n    <li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\n    <li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\n    <li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\n</ol>\n\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\n\n<h2>MidoNet\'s Rule-Chains</h2>\n\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\n\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\n\n<ul>\n    <li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\n    <li>clauses matching the packet\'s ingress port on a specific virtual device</li>\n    <li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\n    <li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\n</ul>\n\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\n\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\n\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\n\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\n\n<h2>Security Groups in MidoNet</h2>\n\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\n\n<ul>\n    <li>Tenant A has two Security Groups:\n<ul>\n    <li>SG1 allows ICMP and HTTP traffic.</li>\n    <li>SG2 allows SSH traffic.</li>\n</ul>\n</li>\n    <li>Tenant A has two VMs:\n<ul>\n    <li>VM1 (with MAC1 and IP1) is bound to network Port1, which belongs to both Security Groups.</li>\n    <li>VM2 (with MAC2 and IP2) is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\n</ul>\n</li>\n</ul>\n\nThe following diagram shows how the <em>ingress</em> Security Group and anti-spoofing rules are translated and organized into MidoNet Rule-Chains.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"720\" height=\"540\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\nWhen a Security Group is created, e.g. SG1, MidoNet\'s low-level topology gets two Rule-Chains: SG1-Ingress will hold translations of the Group\'s <em>ingress</em> rules; SG2-Egress will hold translations of the Group\'s <em>egress</em> rules. When a <em>ingress</em> Security Group Rule is added, a corresponding low-level MN Rule is added to SG1-Ingress. Similarly, a new <em>egress</em> Security Group Rule results in a new MN Rule in SG1-Egress. A Security Group rule is translated to a MN Rule with a condition matching the same traffic and Action set to \"ACCEPT\".\n\nWhen a Neutron port is created, MidoNet\'s low-level model gets a new port on the MN virtual bridge corresponding to the Neutron network. The virtual bridge port has two Rule-Chains: one for traffic from the VM and another for traffic to the VM. These Chains are immediately populated with anti-spoofing rules (because the port\'s MAC and IP are already known).\n\nWhen Security Groups are assigned to the Neutron port, for each Security Group added, the MN port\'s <em>VM-ingress</em> Chain gets an unconditional rule whose action causes the flow of control (during Simulation) to JUMP to the <em>ingress</em>\n\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-06 19:37:32','2014-12-06 19:37:32','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\n<h2>Intro to Security Groups</h2>\r\nIn OpenStack the term \"Security Group\" refers to two concepts:\r\n<ul>\r\n	<li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\r\n	<li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\r\n</ul>\r\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\r\n\r\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\r\n\r\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\r\n\r\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\r\n<ol>\r\n	<li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\r\n	<li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\r\n	<li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\r\n	<li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\r\n</ol>\r\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\r\n<h2>MidoNet\'s Rule-Chains</h2>\r\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\r\n\r\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\r\n<ul>\r\n	<li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\r\n	<li>clauses matching the packet\'s ingress port on a specific virtual device</li>\r\n	<li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\r\n	<li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\r\n</ul>\r\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\r\n\r\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\r\n\r\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\r\n\r\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\r\n<h2>Security Groups in MidoNet</h2>\r\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\r\n<ul>\r\n	<li>Tenant A has two Security Groups:\r\n<ul>\r\n	<li>SG1 allows ICMP and HTTP traffic.</li>\r\n	<li>SG2 allows SSH traffic.</li>\r\n</ul>\r\n</li>\r\n	<li>Tenant A has two VMs:\r\n<ul>\r\n	<li>VM1 (with MAC1 and IP1) is bound to network Port1, which belongs to both Security Groups.</li>\r\n	<li>VM2 (with MAC2 and IP2) is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe following diagram shows how the <em>ingress</em> Security Group and anti-spoofing rules are translated and organized into MidoNet Rule-Chains.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"720\" height=\"540\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\nWhen a Security Group is created, e.g. SG1, MidoNet\'s low-level topology gets two Rule-Chains: SG1-Ingress will hold translations of the Group\'s <em>ingress</em> rules; SG2-Egress will hold translations of the Group\'s <em>egress</em> rules. When a <em>ingress</em> Security Group Rule is added, a corresponding low-level MN Rule is added to SG1-Ingress. Similarly, a new <em>egress</em> Security Group Rule results in a new MN Rule in SG1-Egress. A Security Group rule is translated to a MN Rule with a condition matching the same traffic and Action set to \"ACCEPT\".\r\n\r\nWhen a Neutron port is created, MidoNet\'s low-level model gets a new port on the MN virtual bridge corresponding to the Neutron network. The virtual bridge port has two Rule-Chains: one for traffic from the VM and another for traffic to the VM. These Chains are immediately populated with anti-spoofing rules (because the port\'s MAC and IP are already known).\r\n\r\nWhen Security Groups are assigned to the Neutron port, for each Security Group added, the MN port\'s <em>VM-ingress</em> Chain gets an unconditional rule whose action causes the flow of control (during Simulation) to JUMP to the <em>ingress</em>\r\n\r\nAdding a Security Group Rule results in a MN Rule being added to the MN Inbound/Outbound Chain pair corresponding to the SecGroup. Which one depends on the rule direction: egress in Neutron becomes inbound in MN (PoV of the device owning the port); ingress rule in Neutron gets translated to a rule in the outbound chain in MN.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(183,5,'2014-12-06 20:20:22','2014-12-06 20:20:22','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\n</ul>\n\n<h2>Neutron Security Groups</h2>\n\nIn OpenStack the term \"Security Group\" refers to two concepts:\n\n<ul>\n    <li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\n    <li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\n</ul>\n\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\n\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\n\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\n\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\n\n<ol>\n    <li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\n    <li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\n    <li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\n    <li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\n</ol>\n\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\n\n<h2>MidoNet\'s Rule-Chains</h2>\n\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\n\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\n\n<ul>\n    <li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\n    <li>clauses matching the packet\'s ingress port on a specific virtual device</li>\n    <li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\n    <li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\n</ul>\n\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\n\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\n\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\n\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\n\n<h2>Security Groups in MidoNet</h2>\n\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\n\n<ul>\n    <li>Tenant A has two Security Groups:\n<ul>\n    <li>SG1 allows ICMP and HTTP traffic.</li>\n    <li>SG2 allows SSH traffic.</li>\n</ul>\n</li>\n    <li>Tenant A has two VMs:\n<ul>\n    <li>VM1 (with MAC1 and IP1) is bound to network Port1, which belongs to both Security Groups.</li>\n    <li>VM2 (with MAC2 and IP2) is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\n</ul>\n</li>\n</ul>\n\nThe following diagram shows how the <em>ingress</em> Security Group and anti-spoofing rules are translated and organized into MidoNet Rule-Chains.\n\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"720\" height=\"540\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\n\nWhen a Security Group is created, e.g. SG1, MidoNet\'s low-level topology gets two Rule-Chains: SG1-Ingress will hold translations of the Group\'s <em>ingress</em> rules; SG2-Egress will hold translations of the Group\'s <em>egress</em> rules. When a <em>ingress</em> Security Group Rule is added, a corresponding low-level MN Rule is added to SG1-Ingress. Similarly, a new <em>egress</em> Security Group Rule results in a new MN Rule in SG1-Egress. A Security Group rule is translated to a MN Rule with a condition matching the same traffic and Action set to \"ACCEPT\".\n\nWhen Neutron Port1 is created, MidoNet\'s low-level model gets a new port on the MN virtual bridge corresponding to Port1\'s network. The MN port has two Rule-Chains: Port1-Ingress filters traffic to the VM; Port1-Egress filters traffic from the VM. These Chains are immediately populated with anti-spoofing rules (because the port\'s MAC and IP are already known).\n\nWhen a Security Group is assigned to a Neutron port, for example SG1 is assigned to Port1, the Port1-Ingress gets an unconditional rule whose JUMP action causes the flow of control (during Simulation) to pass to SG1-Ingress. Similarly, Port1-Egress gets an unconditional rule whose action is <em>Jump to SG1-Egress.</em>\n\nPutting this all together, since Port1 is assigned to both SG1 and SG2, the Rule-Chain named Port1-Ingress ends up with these rules:\n\n<ol>\n    <li>DROP traffic whose destination MAC is not MAC1</li>\n    <li>DROP traffic whose destination IP is not IP1</li>\n    <li>ACCEPT reply traffic</li>\n    <li>JUMP to SG1-Ingress</li>\n    <li>JUMP to SG2-Ingress</li>\n    <li>DROP (unconditional, drops everything that gets here)</li>\n</ol>\n\nNotice that Port1-Ingress and Port1-Egress are Rule-Chains that are only used by Port1. In contrast, SG1-Ingress and SG1-Egress are both potentially shared Rule-Chains. They will be <em>jumped-to</em> by rules in every port that belongs to Security Group SG1.\n\n<h2>Rule-Chain Simulation</h2>\n\n<a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> described the MN Agent\'s packet-processing logic, and particularly the Simulation stage. So here we\'ll just jump right into how Rule-Chains behave during Simulation. Assume an SSH packet (to IP1 and TCP port 22 and of course having missed in the kernel and user-space flow tables) is in the Simulation stage. Assume the Simulation has determined that the packet would arrive at the virtual bridge (corresponding to a Neutron network) and would be emitted from Port1. The Simulation would proceed to check how Port1\'s <em>outbound</em> (outbound, from the perspective of the bridge) filter would behave. This means evaluating the packet against the Rule-Chain named <em>Port1-Ingress</em> (the chain name reflect Neutron\'s VM-based perspective). Here\'s what the Simulation would do:\n\n<ol>\n    <li>Check Rule #1\'s condition against the packet. Assume the packet is for MAC1, so the condition does not match the packet. Move on to the next rule.</li>\n    <li>Check Rule #2\'s condition against the packet. The packet is for IP1, so the condition does not match. Move on to the next rule.</li>\n    <li>Check Rule #3\'s condition. This requires doing a lookup in the local flow-state cache using a key constructed by flipping the packet\'s source and destination IP addresses and L4 ports. Assume this is a new SSH connection, no flow-state is found. Move on to the next rule.</li>\n    <li>Rule #4 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\n    <li>Evaluate SG1\'s Rule #1\'s condition. That matches ICMP, this packet\'s SSH, so move on to the next rule.</li>\n    <li>Evaluate SG2\'s Rule #2\'s condition. That matches HTTP, this packet\'s SSH, so move on to the next rule.</li>\n    <li>There\'s no next rule, so return from SG1 to the calling JUMP rule. Move on to the next rule.</li>\n    <li>Rule #5 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\n    <li>Evaluate SG1\'s Rule #1\'s condition. That matches SSH and therefore applies to this packet. The Action is ACCEPT. Break out of Chain evaluation with result ACCEPT.</li>\n    <li>The Simulation moves on to next stage in the device\'s packet processing. For a pre-routing chain, the next stage would be routing. For a bridge port\'s Inbound filter, the next stage would be entering the bridge itself. In this case, an Outbound filter, the next stage is to actually emit the packet from the port (it wasn\'t dropped).</li>\n</ol>\n\nIt\'s not part of the Rule-Chain evaluation, but remember from <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> that since this is an <em>exterior</em> port, the Simulation terminates with the result that the original packet should have its headers modified to what they looked like during Port1\'s simulation (e.g. if they traversed a router, the L2 addresses would be modified) and then be emitted from Port1. The virtual/physical translation stage maps Port1 to the compute host where VM1 resides, e.g. Host5. If VM1 is remote, a datapath flow rule is installed to match all similar packets and emit them (<em>forward</em> them in OVS kmod\'s terminology) from a tunnel port with outer IP address set to Host5\'s IP. The tunnel key will be set to encode Port1\'s ID so that the MN Agent at Host5 can emit the packet/flow from the corresponding datapath port without doing a Simulation. The ingress host of a flow does the simulation and the egress host (the recipient of tunnel traffic) trusts the result.\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 4 - Security Groups)','','inherit','open','open','','94-revision-v1','','','2014-12-06 20:20:22','2014-12-06 20:20:22','In this article we discuss how OpenStack Security Groups are implemented in MidoNet. This post is the fourth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversing of the overlay topology.</li>\r\n</ul>\r\n<h2>Neutron Security Groups</h2>\r\nIn OpenStack the term \"Security Group\" refers to two concepts:\r\n<ul>\r\n	<li>A <em>white-list</em> of rules each of which identifies some kind of <strong>allowed</strong> traffic. Traffic may be identified by any of the following: source IP prefix, destination IP prefix, IP protocol, L4 source port, L4 destination port, ICMP type, ICMP code, direction (traffic from the VM is checked against <em>egress</em> rules, traffic to the VM is checked against <em>ingress</em> rules).</li>\r\n	<li>All the ports that incorporate a Security Group\'s rules in their port-level firewalls. Therefore, traffic may also be identified by the Security Group of the source port or destination port.</li>\r\n</ul>\r\nNeutron network ports (those bound to VM instances) can be assigned one or more Security Groups. Any <em>forward</em> traffic (regardless of direction: from/to the VM) that isn\'t explicitly allowed by some Security Group rule is dropped. <em>Return</em> traffic is allowed without matching any Security Group rule.\r\n\r\nEach Tenant (the OpenStack term is <em>Project</em> but I prefer <em>Tenant</em>) has a default Security Group (initially empty) and may create any number of new Security Groups (Security Groups are not shared across Tenants). Any VM-facing network port that isn\'t explicitly assigned specific Security Groups will be assigned the Tenant\'s default group. Security Groups may be edited (creating or deleting rules) at any time in Horizon or via the Neutron CLI. A change to a Security Group is reflected almost immediately at all the ports assigned to that Security Group.\r\n\r\nNote that VM-facing network ports also have anti-spoofing rules automatically applied to them. Remember that at port-creation time, Neutron selects the port\'s MAC and IP addresses. Each port automatically gets anti-spoofing rules. These rules are different from Security Group rules in that 1) they can match on L2 fields, 2) they\'re specific to the port\'s addresses and therefore cannot be shared across ports (in contrast, think of Security Groups as rule sets that can be shared). A recent version of Neutron (Icehouse?) enabled toggling the anti-spoofing rules on a per-port basis in order to allow a VM instance to act as an appliance/router, receiving and forwarding traffic whose L3 source and destination are different than the VM\'s.\r\n\r\nTo sum up, here\'s how the VM\'s port-level firewall treats a packet emitted from the VM:\r\n<ol>\r\n	<li>First check for spoofing. Drop the packet if its source MAC and IP don\'t match the addresses reserved for the VM when the VM\'s network port was created.</li>\r\n	<li>Then, allow the packet if it can be identified as a reply/response to traffic that was allowed towards the VM.</li>\r\n	<li>Then, allow the packet if it matches any <em>egress</em> rule in the Security Groups assigned to the VM\'s network port.</li>\r\n	<li>Finally, drop the packet if it wasn\'t allowed in step 2 or 3.</li>\r\n</ol>\r\nTreatment for traffic to the VM is similar (spoofing will check destination MAC and IP; <em>ingress</em> Security Group rules are checked in step 3).\r\n<h2>MidoNet\'s Rule-Chains</h2>\r\nMN Agents do not understand Security Groups. Instead they understand the corresponding <em>low-level</em> models: Rules and Rule-Chains. Rule-Chains are a powerful mechanism that allows MN Agents to implement many different high-level network capabilities.\r\n\r\nMN Rules are distinct from Security Group rules. MN Rules have richer matching/filtering and richer actions. MN Rules have condition statements that can express boolean logic over clauses that match most L2-4 header fields (practically the same set matched by OpenFlow). MN Rules also allow:\r\n<ul>\r\n	<li>wildcards, bit-masks, or ranges on most L2-4 header fields;</li>\r\n	<li>clauses matching the packet\'s ingress port on a specific virtual device</li>\r\n	<li>clauses matching the packet\'s egress port of a specific virtual device (the egress port is known, for example, if the rule is evaluated in a virtual router\'s post-routing stage);</li>\r\n	<li>Tenant/owner of the port where the packet entered the overlay/virtual network.</li>\r\n</ul>\r\nA Rule specifies an action to apply to traffic that matches the rule\'s condition statement (think of it as a filter). Possible actions include: accept, drop, static source NAT, dynamic source network address and port translation (NAPT), static DNAT, dynamic DNAT. Some actions that are currently in development are: trace (marks the packet for tracing through the overlay and underlay network), and meter (counts flows, packets, or bytes).\r\n\r\nRule-Chains are ordered lists of Rules. Rule-Chains are referenced by virtual devices to implement some of their packet-processing logic. For example, MidoNet virtual routers have pre-routing and post-routing rule-chains; virtual bridges have pre-forwarding and post-forwarding rule-chains; router and bridge ports have inbound and outbound rule-chains.\r\n\r\nRule-Chains themselves are stateless and can be shared by multiple virtual devices. Think of them as a program specifying how packets should be treated. Therefore, some rule actions exist purely for flow-control: continue, jump and return.\r\n\r\nThe best way to understand Rule-Chains is to jump into an example: MidoNet\'s implementation of Neutron Security Groups and anti-spoofing rules.\r\n<h2>Security Groups in MidoNet</h2>\r\nConsider the following scenario. For simplicity we\'ll consider only ingress traffic to VMs (and therefore only <em>ingress</em> Security Group rules):\r\n<ul>\r\n	<li>Tenant A has two Security Groups:\r\n<ul>\r\n	<li>SG1 allows ICMP and HTTP traffic.</li>\r\n	<li>SG2 allows SSH traffic.</li>\r\n</ul>\r\n</li>\r\n	<li>Tenant A has two VMs:\r\n<ul>\r\n	<li>VM1 (with MAC1 and IP1) is bound to network Port1, which belongs to both Security Groups.</li>\r\n	<li>VM2 (with MAC2 and IP2) is bound to network Port2, which belongs only to SG1 (SSH to this VM is not allowed).</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe following diagram shows how the <em>ingress</em> Security Group and anti-spoofing rules are translated and organized into MidoNet Rule-Chains.\r\n\r\n[caption id=\"attachment_101\" align=\"alignnone\" width=\"720\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\"><img class=\"wp-image-101 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MN_PortFilterAndSecGroups.jpg\" alt=\"MN_PortFilterAndSecGroups\" width=\"720\" height=\"540\" /></a> Two port-level Firewalls implemented as MidoNet Rule Chains. Port1 (e.g. to VM1) has two Security Groups (SG1 and SG2) while Port2 (e.g. to VM2) only has one (SG1). In this example, SSH access to VM2 is disallowed on Port2.[/caption]\r\n\r\nWhen a Security Group is created, e.g. SG1, MidoNet\'s low-level topology gets two Rule-Chains: SG1-Ingress will hold translations of the Group\'s <em>ingress</em> rules; SG2-Egress will hold translations of the Group\'s <em>egress</em> rules. When a <em>ingress</em> Security Group Rule is added, a corresponding low-level MN Rule is added to SG1-Ingress. Similarly, a new <em>egress</em> Security Group Rule results in a new MN Rule in SG1-Egress. A Security Group rule is translated to a MN Rule with a condition matching the same traffic and Action set to \"ACCEPT\".\r\n\r\nWhen Neutron Port1 is created, MidoNet\'s low-level model gets a new port on the MN virtual bridge corresponding to Port1\'s network. The MN port has two Rule-Chains: Port1-Ingress filters traffic to the VM; Port1-Egress filters traffic from the VM. These Chains are immediately populated with anti-spoofing rules (because the port\'s MAC and IP are already known).\r\n\r\nWhen a Security Group is assigned to a Neutron port, for example SG1 is assigned to Port1, the Port1-Ingress gets an unconditional rule whose JUMP action causes the flow of control (during Simulation) to pass to SG1-Ingress. Similarly, Port1-Egress gets an unconditional rule whose action is <em>Jump to SG1-Egress.</em>\r\n\r\nPutting this all together, since Port1 is assigned to both SG1 and SG2, the Rule-Chain named Port1-Ingress ends up with these rules:\r\n<ol>\r\n	<li>DROP traffic whose destination MAC is not MAC1</li>\r\n	<li>DROP traffic whose destination IP is not IP1</li>\r\n	<li>ACCEPT reply traffic</li>\r\n	<li>JUMP to SG1-Ingress</li>\r\n	<li>JUMP to SG2-Ingress</li>\r\n	<li>DROP (unconditional, drops everything that gets here)</li>\r\n</ol>\r\nNotice that Port1-Ingress and Port1-Egress are Rule-Chains that are only used by Port1. In contrast, SG1-Ingress and SG1-Egress are both potentially shared Rule-Chains. They will be <em>jumped-to</em> by rules in every port that belongs to Security Group SG1.\r\n<h2>Rule-Chain Simulation</h2>\r\n<a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> described the MN Agent\'s packet-processing logic, and particularly the Simulation stage. So here we\'ll just jump right into how Rule-Chains behave during Simulation. Assume an SSH packet (to IP1 and TCP port 22 and of course having missed in the kernel and user-space flow tables) is in the Simulation stage. Assume the Simulation has determined that the packet would arrive at the virtual bridge (corresponding to a Neutron network) and would be emitted from Port1. The Simulation would proceed to check how Port1\'s <em>outbound</em> (outbound, from the perspective of the bridge) filter would behave. This means evaluating the packet against the Rule-Chain named <em>Port1-Ingress</em> (the chain name reflect Neutron\'s VM-based perspective). Here\'s what the Simulation would do:\r\n<ol>\r\n	<li>Check Rule #1\'s condition against the packet. Assume the packet is for MAC1, so the condition does not match the packet. Move on to the next rule.</li>\r\n	<li>Check Rule #2\'s condition against the packet. The packet is for IP1, so the condition does not match. Move on to the next rule.</li>\r\n	<li>Check Rule #3\'s condition. This requires doing a lookup in the local flow-state cache using a key constructed by flipping the packet\'s source and destination IP addresses and L4 ports. Assume this is a new SSH connection, no flow-state is found. Move on to the next rule.</li>\r\n	<li>Rule #4 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\r\n	<li>Evaluate SG1\'s Rule #1\'s condition. That matches ICMP, this packet\'s SSH, so move on to the next rule.</li>\r\n	<li>Evaluate SG2\'s Rule #2\'s condition. That matches HTTP, this packet\'s SSH, so move on to the next rule.</li>\r\n	<li>There\'s no next rule, so return from SG1 to the calling JUMP rule. Move on to the next rule.</li>\r\n	<li>Rule #5 is unconditional. JUMP to Chain SG1. The Simulation loads the representation of SG1.</li>\r\n	<li>Evaluate SG1\'s Rule #1\'s condition. That matches SSH and therefore applies to this packet. The Action is ACCEPT. Break out of Chain evaluation with result ACCEPT.</li>\r\n	<li>The Simulation moves on to next stage in the device\'s packet processing. For a pre-routing chain, the next stage would be routing. For a bridge port\'s Inbound filter, the next stage would be entering the bridge itself. In this case, an Outbound filter, the next stage is to actually emit the packet from the port (it wasn\'t dropped).</li>\r\n</ol>\r\nIt\'s not part of the Rule-Chain evaluation, but remember from <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> that since this is an <em>exterior</em> port, the Simulation terminates with the result that the original packet should have its headers modified to what they looked like during Port1\'s simulation (e.g. if they traversed a router, the L2 addresses would be modified) and then be emitted from Port1. The virtual/physical translation stage maps Port1 to the compute host where VM1 resides, e.g. Host5. If VM1 is remote, a datapath flow rule is installed to match all similar packets and emit them (<em>forward</em> them in OVS kmod\'s terminology) from a tunnel port with outer IP address set to Host5\'s IP. The tunnel key will be set to encode Port1\'s ID so that the MN Agent at Host5 can emit the packet/flow from the corresponding datapath port without doing a Simulation. The ingress host of a flow does the simulation and the egress host (the recipient of tunnel traffic) trusts the result.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n&nbsp;',94,'http://blog.midonet.org/94-revision-v1/',0,'revision','',0),(184,5,'2014-12-06 20:27:50','2014-12-06 20:27:50','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n</ul>\n\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\n\n<h2>Neutron Floating IPs</h2>\n\n&nbsp;\n\n&nbsp;\n\n<h2>Floating IPs in MidoNet</h2>\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-06 20:27:50','2014-12-06 20:27:50','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n</ul>\r\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\r\n<h2>Neutron Floating IPs</h2>\r\n&nbsp;\r\n\r\n&nbsp;\r\n<h2>Floating IPs in MidoNet</h2>\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(185,5,'2014-12-06 20:30:46','2014-12-06 20:30:46','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n</ul>\n\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\n\n<h2>Neutron Floating IPs</h2>\n\n&nbsp;\n\n&nbsp;\n\n<h2>Floating IPs in MidoNet</h2>\n\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\n\n&nbsp;\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 5 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-06 20:30:46','2014-12-06 20:30:46','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n</ul>\r\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\r\n<h2>Neutron Floating IPs</h2>\r\n&nbsp;\r\n\r\n&nbsp;\r\n<h2>Floating IPs in MidoNet</h2>\r\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\r\n\r\n&nbsp;\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(186,5,'2014-12-06 21:12:23','2014-12-06 21:12:23','','MNFlowStateNew3','','inherit','open','open','','mnflowstatenew3','','','2014-12-06 21:12:23','2014-12-06 21:12:23','',155,'http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png',0,'attachment','image/png',0),(187,5,'2014-12-06 21:12:24','2014-12-06 21:12:24','','MNFlowStateNew2','','inherit','open','open','','mnflowstatenew2','','','2014-12-06 21:12:24','2014-12-06 21:12:24','',155,'http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png',0,'attachment','image/png',0),(188,5,'2014-12-06 21:12:25','2014-12-06 21:12:25','','MNFlowStateNew1','','inherit','open','open','','mnflowstatenew1','','','2014-12-06 21:12:25','2014-12-06 21:12:25','',155,'http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png',0,'attachment','image/png',0),(189,5,'2014-12-06 21:12:26','2014-12-06 21:12:26','','MNFlowStateOld3','','inherit','open','open','','mnflowstateold3','','','2014-12-06 21:12:26','2014-12-06 21:12:26','',155,'http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png',0,'attachment','image/png',0),(190,5,'2014-12-06 21:12:27','2014-12-06 21:12:27','','MNFlowStateOld2','','inherit','open','open','','mnflowstateold2','','','2014-12-06 21:12:27','2014-12-06 21:12:27','',155,'http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png',0,'attachment','image/png',0),(191,5,'2014-12-06 21:12:27','2014-12-06 21:12:27','','MNFlowStateOld1','','inherit','open','open','','mnflowstateold1','','','2014-12-06 21:12:27','2014-12-06 21:12:27','',155,'http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png',0,'attachment','image/png',0),(192,5,'2014-12-06 21:20:56','2014-12-06 21:20:56','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n</ul>\n\n<h2>Introduction</h2>\n\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\n\nTraditionally, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\n\nThe single location was a SPOF, so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster of Load-balancers.\n\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\nit allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.\nwhen flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.\n\nThe Old Approach\n\nRecap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.\n\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\nis this a forward or return flow?\nin what state is this TCP connection?\nwhere should load-balancer LB1 send this flow?\nwhat Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?\nFYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case.\n\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.\n\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\n\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\n\nDiagram 1:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\n\nDiagram 2:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\n\nDiagram 3:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\nThe New Approach\n\nSummary: the ingress node pushes the flow-state directly to the Interested Set of nodes.\n\nWe reasoned that although each flow-s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\n\nTwo improvements were therefore feasible:\nreduce latency because flow computation only requires locally available state\nremove a dependency on a database\n\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\n\nDiagram 4:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\n\nDiagram 5:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\n\nDiagram 6:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\nAlgorithm/protocol for pushing flow-state between Agents\n\nTunnel packet (currently using GRE/UDP - unreliable, but later will use DSCP or PFC) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\nInterested Set Hinting\n\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\nDepending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.\nA tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.\nA VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.\nThe VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\nThe physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.\n\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.\nPort Migration\n\nThe typical use-case for port migration is VM migration. In the old approach to flow-state management, the MN Agent at the virtual port’s New Node didn’t have to do anything special. For every flow computation that required state, there would be a lookup in Cassandra.\n\nIn the new approach, flow computation at the New Node works as described above. However, for a few seconds after the port has migrated, flows will not find local state and flow computations will therefore be performed incorrectly. However, in the background the MN Agent knows that this port was recently owned/hosted by a different, Old Node. The New Node therefore pulls all the required flow state from the Old Node.\n\nConsider that when port1 migrates from Old Node to New Node, Old Node has all the flow-state required for (forward and return) flows that potentially ingress port1. Consider also that:\nif port1 is not in an Ingress Set, then no other node has all of port1’s ingress flows’ state. If Old Node is unavailable, the flow state may not be recoverable - this is acceptable because VM migration itself requires Old Node to be available for memory transfer.\nif port1 is in an Ingress Set, then nodes other than Old Node may be able to help recover the ingress flow state for the port.\n\nFlow state is indexed (and laid out contiguously in memory) by ingress port, and can be transferred from Old to New Node within a matter of seconds (this needs to be clarified in terms of how much data per flow, how many flows, and in exactly what time delay).\nAgent restart\n\nIn the old approach, the MN Agent would flush all its state (including flow rules) on every restart. When recomputing flows, the Agent would simply do lookups in Cassandra.\n\nIn the new approach, the MN Agent either flushes its per-flow state to disk periodically, or, in the case of ports belonging to Ingress Sets, may avoid disk-writes by recovering the flow-state from another Node that hosts a port in the same Ingress Set. Recovering from peers avoids disk writes on heavily loaded nodes like L3 Gateways.\n\nMore Advanced Cases: real connection tracking, DPI information.\n\nIn the new Distributed Flow State management, many nodes may potentially update flow state after it’s created...\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 5 – Flow State)','','inherit','open','open','','155-revision-v1','','','2014-12-06 21:20:56','2014-12-06 21:20:56','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\n<h2>Introduction</h2>\r\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\r\n\r\nTraditionally, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\r\n\r\nThe single location was a SPOF, so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster of Load-balancers.\r\n\r\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\r\nit allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.\r\nwhen flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.\r\n\r\nThe Old Approach\r\n\r\nRecap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.\r\n\r\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\r\nis this a forward or return flow?\r\nin what state is this TCP connection?\r\nwhere should load-balancer LB1 send this flow?\r\nwhat Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?\r\nFYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case.\r\n\r\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.\r\n\r\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\r\n\r\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\r\n\r\nDiagram 1:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\r\n\r\nDiagram 2:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\r\n\r\nDiagram 3:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\r\nThe New Approach\r\n\r\nSummary: the ingress node pushes the flow-state directly to the Interested Set of nodes.\r\n\r\nWe reasoned that although each flow-s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\r\n\r\nTwo improvements were therefore feasible:\r\nreduce latency because flow computation only requires locally available state\r\nremove a dependency on a database\r\n\r\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\r\n\r\nDiagram 4:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\r\n\r\nDiagram 5:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\r\n\r\nDiagram 6:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\r\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\r\nAlgorithm/protocol for pushing flow-state between Agents\r\n\r\nTunnel packet (currently using GRE/UDP - unreliable, but later will use DSCP or PFC) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\r\nInterested Set Hinting\r\n\r\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\r\nDepending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.\r\nA tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.\r\nA VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.\r\nThe VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\r\nThe physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.\r\n\r\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.\r\nPort Migration\r\n\r\nThe typical use-case for port migration is VM migration. In the old approach to flow-state management, the MN Agent at the virtual port’s New Node didn’t have to do anything special. For every flow computation that required state, there would be a lookup in Cassandra.\r\n\r\nIn the new approach, flow computation at the New Node works as described above. However, for a few seconds after the port has migrated, flows will not find local state and flow computations will therefore be performed incorrectly. However, in the background the MN Agent knows that this port was recently owned/hosted by a different, Old Node. The New Node therefore pulls all the required flow state from the Old Node.\r\n\r\nConsider that when port1 migrates from Old Node to New Node, Old Node has all the flow-state required for (forward and return) flows that potentially ingress port1. Consider also that:\r\nif port1 is not in an Ingress Set, then no other node has all of port1’s ingress flows’ state. If Old Node is unavailable, the flow state may not be recoverable - this is acceptable because VM migration itself requires Old Node to be available for memory transfer.\r\nif port1 is in an Ingress Set, then nodes other than Old Node may be able to help recover the ingress flow state for the port.\r\n\r\nFlow state is indexed (and laid out contiguously in memory) by ingress port, and can be transferred from Old to New Node within a matter of seconds (this needs to be clarified in terms of how much data per flow, how many flows, and in exactly what time delay).\r\nAgent restart\r\n\r\nIn the old approach, the MN Agent would flush all its state (including flow rules) on every restart. When recomputing flows, the Agent would simply do lookups in Cassandra.\r\n\r\nIn the new approach, the MN Agent either flushes its per-flow state to disk periodically, or, in the case of ports belonging to Ingress Sets, may avoid disk-writes by recovering the flow-state from another Node that hosts a port in the same Ingress Set. Recovering from peers avoids disk writes on heavily loaded nodes like L3 Gateways.\r\n\r\nMore Advanced Cases: real connection tracking, DPI information.\r\n\r\nIn the new Distributed Flow State management, many nodes may potentially update flow state after it’s created...\r\n\r\n&nbsp;\r\n\r\n&nbsp;',155,'http://blog.midonet.org/155-revision-v1/',0,'revision','',0),(193,5,'2014-12-06 21:40:36','2014-12-06 21:40:36','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n</ul>\n\n<h2>Introduction</h2>\n\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\n\nTraditionally, in physical networking, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\n\nThe single location was a SPOF (single point of failure), so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster (e.g. of Load-balancers).\n\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\n\n<ul>\n    <li>it allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.</li>\n    <li>when flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.</li>\n</ul>\n\n<h2>The Old Approach</h2>\n\n<em>Recap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.</em>\n\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\n\n<ul>\n    <li>is this a forward or return flow?</li>\n    <li>in what state is this TCP connection?</li>\n    <li>where should load-balancer LB1 send this flow?</li>\n    <li>what Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?</li>\n    <li>FYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case because once a single route is chosen it continues to be used during the lifetime of the flow installed by the Simulation that ran the ECMP algorithm.</li>\n</ul>\n\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. <em>Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.</em>\n\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\n\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\n\nDiagram 1:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\n\nDiagram 2:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\n\nDiagram 3:\n\n<h2><a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\nThe New Approach</h2>\n\n<em>Summary: the ingress node pushes the flow-state directly to the Interested Set of nodes.</em>\n\nWe reasoned that although each flow-s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\n\nTwo improvements were therefore feasible:\n\n<ul>\n    <li>reduce latency because flow computation only requires locally available state</li>\n    <li>remove a dependency on a database</li>\n</ul>\n\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\n\nDiagram 4:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\n\nDiagram 5:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\n\nDiagram 6:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\n\n<h3>Algorithm/protocol for pushing flow-state between Agents</h3>\n\nTunnel packet (currently using GRE/UDP) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\n\n<em>The upcoming release of MidoNet will have the option of setting DSCP values in the outer header of the state packets. Operators can map this DSCP value to a higher-priority traffic class in the underlay switches. This is important since the MN Agent cannot always detect when the state packet needs to be retransmitted. Failure to receive a state packet can result in a failed connection, for example, because the MN Agent at the host where the return flow ingresses does not have the forward flow state.</em>\n\n<h3>Interested Set Hinting</h3>\n\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\n\n<ul>\n    <li>Depending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.</li>\n    <li>A tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.</li>\n    <li>A VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.</li>\n    <li>The VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\n<ul>\n    <li>The physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.</li>\n</ul>\n</li>\n</ul>\n\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.','Introduction to MN (Part 5 – Flow State)','','inherit','open','open','','155-revision-v1','','','2014-12-06 21:40:36','2014-12-06 21:40:36','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\n<h2>Introduction</h2>\r\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\r\n\r\nTraditionally, in physical networking, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\r\n\r\nThe single location was a SPOF (single point of failure), so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster (e.g. of Load-balancers).\r\n\r\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\r\n<ul>\r\n	<li>it allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.</li>\r\n	<li>when flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.</li>\r\n</ul>\r\n<h2>The Old Approach</h2>\r\n<em>Recap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.</em>\r\n\r\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\r\n<ul>\r\n	<li>is this a forward or return flow?</li>\r\n	<li>in what state is this TCP connection?</li>\r\n	<li>where should load-balancer LB1 send this flow?</li>\r\n	<li>what Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?</li>\r\n	<li>FYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case because once a single route is chosen it continues to be used during the lifetime of the flow installed by the Simulation that ran the ECMP algorithm.</li>\r\n</ul>\r\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. <em>Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.</em>\r\n\r\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\r\n\r\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\r\n\r\nDiagram 1:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\r\n\r\nDiagram 2:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\r\n\r\nDiagram 3:\r\n<h2><a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\r\nThe New Approach</h2>\r\n<em>Summary: the ingress node pushes the flow-state directly to the Interested Set of nodes.</em>\r\n\r\nWe reasoned that although each flow-s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\r\n\r\nTwo improvements were therefore feasible:\r\n<ul>\r\n	<li>reduce latency because flow computation only requires locally available state</li>\r\n	<li>remove a dependency on a database</li>\r\n</ul>\r\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\r\n\r\nDiagram 4:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\r\n\r\nDiagram 5:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\r\n\r\nDiagram 6:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\r\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\r\n<h3>Algorithm/protocol for pushing flow-state between Agents</h3>\r\nTunnel packet (currently using GRE/UDP) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\r\n\r\n<em>The upcoming release of MidoNet will have the option of setting DSCP values in the outer header of the state packets. Operators can map this DSCP value to a higher-priority traffic class in the underlay switches. This is important since the MN Agent cannot always detect when the state packet needs to be retransmitted. Failure to receive a state packet can result in a failed connection, for example, because the MN Agent at the host where the return flow ingresses does not have the forward flow state.</em>\r\n<h3>Interested Set Hinting</h3>\r\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\r\n<ul>\r\n	<li>Depending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.</li>\r\n	<li>A tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.</li>\r\n	<li>A VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.</li>\r\n	<li>The VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\r\n<ul>\r\n	<li>The physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.',155,'http://blog.midonet.org/155-revision-v1/',0,'revision','',0),(194,5,'2014-12-06 21:42:55','2014-12-06 21:42:55','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\n</ul>\n\n<h2>Introduction</h2>\n\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\n\nTraditionally, in physical networking, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\n\nThe single location was a SPOF (single point of failure), so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster (e.g. of Load-balancers).\n\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\n\n<ul>\n    <li>it allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.</li>\n    <li>when flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.</li>\n</ul>\n\n<h2>The Old Approach</h2>\n\n<em>Recap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.</em>\n\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\n\n<ul>\n    <li>is this a forward or return flow?</li>\n    <li>in what state is this TCP connection?</li>\n    <li>where should load-balancer LB1 send this flow?</li>\n    <li>what Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?</li>\n    <li>FYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case because once a single route is chosen it continues to be used during the lifetime of the flow installed by the Simulation that ran the ECMP algorithm.</li>\n</ul>\n\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. <em>Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.</em>\n\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\n\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\n\nDiagram 1:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\n\nDiagram 2:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\n\nDiagram 3:\n\n<h2><a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\nThe New Approach</h2>\n\n<em>Summary: the ingress node pushes the flow-state directly to the Interested Set of nodes.</em>\n\nWe reasoned that although each flow\'s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\n\nTwo improvements were therefore feasible:\n\n<ul>\n    <li>reduce latency because flow computation only requires locally available state</li>\n    <li>remove a dependency on a database</li>\n</ul>\n\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\n\nDiagram 4:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\n\nDiagram 5:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\n\nDiagram 6:\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\n\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\n\n<h3>Algorithm/protocol for pushing flow-state between Agents</h3>\n\nTunnel packet (currently using GRE/UDP) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\n\n<em>The upcoming release of MidoNet will have the option of setting DSCP values in the outer header of the state packets. Operators can map this DSCP value to a higher-priority traffic class in the underlay switches. This is important since the MN Agent cannot always detect when the state packet needs to be retransmitted. Failure to receive a state packet can result in a failed connection, for example, because the MN Agent at the host where the return flow ingresses does not have the forward flow state.</em>\n\n<h3>Interested Set Hinting</h3>\n\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\n\n<ul>\n    <li>Depending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.</li>\n    <li>A tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.</li>\n    <li>A VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.</li>\n    <li>The VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\n<ul>\n    <li>The physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.</li>\n</ul>\n</li>\n</ul>\n\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.','Introduction to MN (Part 5 – Flow State)','','inherit','open','open','','155-revision-v1','','','2014-12-06 21:42:55','2014-12-06 21:42:55','In this article we discuss how  MidoNet manages flow state in order to implement distributed/fault-tolerant versions of advanced network services like connection tracking, load-balancing, port-masquerading. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups.</li>\r\n</ul>\r\n<h2>Introduction</h2>\r\nMany advanced network services make per-flow decisions that need to be preserved for the duration of the flow. For example, IPv4 port-masquerading (NAPT) chooses a free public IP:port pair to replace the private IP:port pair in TCP/UDP packet sent from a client in the cloud to a server on the internet.\r\n\r\nTraditionally, in physical networking, this per-flow state could live in a single location, e.g. a network device. Both forward and return flows traversed the same device allowing reverse translation in the case of port-masquerading, or connection-status-based filtering in the case of a stateful firewall.\r\n\r\nThe single location was a SPOF (single point of failure), so network devices and appliances have long had solutions for this - e.g. sharing session state between a fault-tolerant pair or among a cluster (e.g. of Load-balancers).\r\n\r\nMidoNet requires sharing per-flow state among (MN Agents at) multiple nodes because:\r\n<ul>\r\n	<li>it allows us to avoid forwarding an overlay flow to an intermediate hypervisor or network device to reach a stateful network service (whether SPOF or a fault-tolerant cluster) like load-balancing and firewalling.</li>\r\n	<li>when flows traverse L2 and L3 Gateways, sharing per-flow state supports asymmetric return paths and gateway node fault-tolerance.</li>\r\n</ul>\r\n<h2>The Old Approach</h2>\r\n<em>Recap: The (MN Agent at the) ingress node that receives a flow - whether because it was generated by a local VM or because it arrived on a non-tunnel L2 or L3 Gateway NIC that connects physical and virtual workloads - computes/simulates how the flow traverses the overlay network, whether it’s dropped or the virtual device and port where it egresses the overlay. It then maps the egress virtual port onto a physical node and sets up fast-path matching, modifying, encapsulating, and tunneling the flow to the computed egress node. MidoNet has progressively added more distributed network services to the overlay in a way that allows more flow computation to happen at the ingress host, eliminating intermediate hops and management of middle boxes/appliances.</em>\r\n\r\nPreviously, a distributed database (Cassandra) was used to store any flow state created or required during computation of the flow’s traversal of the overlay network. This state could answer these questions:\r\n<ul>\r\n	<li>is this a forward or return flow?</li>\r\n	<li>in what state is this TCP connection?</li>\r\n	<li>where should load-balancer LB1 send this flow?</li>\r\n	<li>what Public source IPv4 address and TCP port would this port-masquerading instance select? what private source IPv4 and TCP port is being masked by this public IPv4 address and TCP port?</li>\r\n	<li>FYI - we could not answer “which route did the ECMP algorithm choose?” We didn’t and still don’t consider this a use-case because once a single route is chosen it continues to be used during the lifetime of the flow installed by the Simulation that ran the ECMP algorithm.</li>\r\n</ul>\r\nDiagrams 1, 2 and 3 below illustrate how flow-state was managed in previous MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nFor a new flow (diagram 1), the ingress node, Node 1, made 2 round trips to the Flow-state Database (Cassandra). One to look for existing state (i.e. detect whether it was a new flow) and another to store the newly created flow-state. <em>Many of the flow-state lookups had to be performed synchronously by necessity, thus introducing higher first-packet latency, flow computation pausing/re-starting and thread-management complexity in the MidoNet agent.</em>\r\n\r\nDiagram 2 shows how the ingress node for the return flow (either Node 2 from Diagram 1 or some Node 3 if the return path is asymmetric) finds the flow state in the Flow-state DB, computes the return flow and tunnels it to Node 1 (or perhaps some Node 4 if the return path terminates with some ECMP routes).\r\n\r\nDiagram 3 shows that if the forward flow ingresses some Node 4 (because of upstream ECMP routing) or an amnesiac Node 1, the ingress node is able to find the flow-state in the DB and correctly compute the flow - in this case tunneling it to Node 2 as in Diagram 1.\r\n\r\nDiagram 1:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1.png\"><img class=\"alignnone wp-image-191 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld1-e1417900640805.png\" alt=\"MNFlowStateOld1\" width=\"549\" height=\"343\" /></a>\r\n\r\nDiagram 2:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2.png\"><img class=\"alignnone wp-image-190 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld2-e1417900671817.png\" alt=\"MNFlowStateOld2\" width=\"554\" height=\"336\" /></a>\r\n\r\nDiagram 3:\r\n<h2><a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3.png\"><img class=\"alignnone wp-image-189 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateOld3-e1417900702777.png\" alt=\"MNFlowStateOld3\" width=\"579\" height=\"386\" /></a>\r\nThe New Approach</h2>\r\n<em>Summary: the ingress node pushes the flow-state directly to the Interested Set of nodes.</em>\r\n\r\nWe reasoned that although each flow\'s state was required by 2 (and often more) nodes, for performance reasons the state should be local to those nodes. Then we realized that a new flow’s ingress node can easily determine (with a little hinting, explained below) the set of nodes - the Interested Set - that might possibly need that flow’s state and push it to them. Remember that each of the forward and return flows have potentially several ingress nodes and several egress nodes.\r\n\r\nTwo improvements were therefore feasible:\r\n<ul>\r\n	<li>reduce latency because flow computation only requires locally available state</li>\r\n	<li>remove a dependency on a database</li>\r\n</ul>\r\nThe new approach has the same race conditions as the old approach. In the old approach MidoNet’s implementation had to choose: how many replicas of the state should be written (usually 3), how many replicas must be confirmed before tunneling the flow to the egress node (determines the likelihood that the return flow’s computation will find the flow-state), how many replicas to read to find the flow-state when it’s needed for the return flow or to re-compute the forward flow. In the new approach the ingress node must push the state directly to all the nodes in the Interested Set (possibly more than 3 but just 1 in the vast majority of cases), the choices are slightly different, but lead to similar race condiions: fire-and-forget or confirm? if retry, how often? Delay tunneling the forward flow until the flow-state has propagated or accept the risk of the return flow being computed without the flow-state?\r\n\r\nDiagram 4:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1.png\"><img class=\"alignnone wp-image-188 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew1-e1417900737179.png\" alt=\"MNFlowStateNew1\" width=\"556\" height=\"255\" /></a>\r\n\r\nDiagram 5:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2.png\"><img class=\"alignnone wp-image-187 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew2-e1417900763568.png\" alt=\"MNFlowStateNew2\" width=\"554\" height=\"253\" /></a>\r\n\r\nDiagram 6:\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3.png\"><img class=\"alignnone wp-image-186 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/MNFlowStateNew3-e1417900795166.png\" alt=\"MNFlowStateNew3\" width=\"565\" height=\"270\" /></a>\r\nDiagrams 4, 5 and 6 above illustrate how flow-state is managed in newer MidoNet versions. In all the diagrams, flow-computation is not called out. It’s implied that flow-state is leveraged by the flow computation and encap/tunnel selection.\r\n\r\nIn diagram 4, a new flow ingresses Node 1. Node 1 queries its local state and determines this is a new flow. It performs the flow computation during which flow state is created and stored locally. Finally, “before” forwarding the flow to the egress, Node 2, Node 1 determines the Interested Set {Node 1, Node 2, Node 3, and Node 4} and pushes the flow state. In this case the Interested Set includes Nodes 1 and 4 because they’re potential ingresses for the forward flow and Nodes 2 and 3 because they’re potential ingresses for the return flow.\r\n<h3>Algorithm/protocol for pushing flow-state between Agents</h3>\r\nTunnel packet (currently using GRE/UDP) with special-value tunnel key indicating “flow state”. The state packet is sent unreliably, fire-and-forget, from ingress to egress node before the related data packets. However, the data packets are not delayed because they do not wait for the egress to reply (even if the egress is configured to reply/confirm).\r\n\r\n<em>The upcoming release of MidoNet will have the option of setting DSCP values in the outer header of the state packets. Operators can map this DSCP value to a higher-priority traffic class in the underlay switches. This is important since the MN Agent cannot always detect when the state packet needs to be retransmitted. Failure to receive a state packet can result in a failed connection, for example, because the MN Agent at the host where the return flow ingresses does not have the forward flow state.</em>\r\n<h3>Interested Set Hinting</h3>\r\nEarlier we mentioned that the ingress host can easily determine the Interested Set with a little hinting. Specifically, the Northbound API integration programmer or the cloud Admin is aware of “Ingress Sets” i.e. sets of ports that are equivalent ingress points for a single flow. For example:\r\n<ul>\r\n	<li>Depending on upstream routes, any of the static or dynamic uplinks of the Provider Router may receive a North-to-South flow. The Provider Router’s uplinks should therefore all be added to a single “Ingress Set”.</li>\r\n	<li>A tenant may have a redundant L3 VPN from their office network to their Tenant Router. Depending on the implementation, this tenant’s VPN traffic may ingress MidoNet at more than one node (on-ramp). The situation is very similar to Provider Router.</li>\r\n	<li>A VLAN L2 Gateway allows 2 physical links into an 802.1Q virtual bridge (which in turn has untagged ports virtually linked to ports on VLAN-agnostic bridges - one for each Neutron network that needs access to a physical VLAN). Depending on STP, traffic from the physical workloads can ingress MidoNet at either of the 802.1Q bridge’s “uplink” ports.</li>\r\n	<li>The VXLAN L2 Gateway allows any MidoNet node to tunnel traffic directly to a physical VTEP; the traffic is forwarded to the set of port/vlan pairs associated with the VNI. The physical vtep forwards on-virtual-switch traffic directly to MidoNet host that is local to (the VM interface that owns) the destination MAC. For on-virtual-switch traffic, MN need only consider forward ingress sets for traffic headed to the VTEP, and return ingress sets for traffic coming from the VTEP.\r\n<ul>\r\n	<li>The physical vtep is instructed to forward off-virtual-switch traffic (i.e. from a physical server, through a virtual router in MidoNet) to specific MidoNet hosts that act as VXLAN proxy nodes. For this traffic, MN need also consider the proxies (which may be per-virtual-bridge) to be an ingress set for the flow.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nMidoNet provides a new optional API and CLI commands to allow MidoNet users to group ports into Ingress Sets.',155,'http://blog.midonet.org/155-revision-v1/',0,'revision','',0),(195,5,'2014-12-06 22:28:26','2014-12-06 22:28:26','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n</ul>\n\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\n\n<h2>Neutron Floating IPs</h2>\n\n&nbsp;\n\n&nbsp;\n\n<h2>Floating IPs in MidoNet</h2>\n\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\n\n&nbsp;\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 6 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-06 22:28:26','2014-12-06 22:28:26','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the fifth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n</ul>\r\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\r\n<h2>Neutron Floating IPs</h2>\r\n&nbsp;\r\n\r\n&nbsp;\r\n<h2>Floating IPs in MidoNet</h2>\r\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\r\n\r\n&nbsp;\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(196,5,'2014-12-06 22:32:09','2014-12-06 22:32:09','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models.\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n</ul>\n\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\n\n<h2>Neutron Floating IPs</h2>\n\n&nbsp;\n\n&nbsp;\n\n<h2>Floating IPs in MidoNet</h2>\n\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\n\n&nbsp;\n\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\n\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\n\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\n\n<blockquote># neutron subnet-create ext-net \\\n\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\n\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\n\nEXTERNAL_INTERFACE_CIDR</blockquote>\n\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\n\nFloating IP prefixes/ranges are assigned...\n\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\n\n<blockquote>$ neutron floatingip-create EXT_NET_ID\n\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\n\nThis is the single-step equivalent:\n\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\n\n&nbsp;','Introduction to MN (Part 6 - Floating IPs)','','inherit','open','open','','96-revision-v1','','','2014-12-06 22:32:09','2014-12-06 22:32:09','In this article we discuss how Floating IPs are implemented in MidoNet. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models.\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n</ul>\r\nMidoNet\'s implementation of Floating IPs is based on the low-level Rule-Chain model which was covered in <a href=\"http://wp.me/p5ls6g-1w\">Part 4</a>, so be sure to read at least the Rule-Chain section of that article before proceeding.\r\n<h2>Neutron Floating IPs</h2>\r\n&nbsp;\r\n\r\n&nbsp;\r\n<h2>Floating IPs in MidoNet</h2>\r\nWhen the Floating IP prefix is created in Neutron, MidoNet\'s Provider Router gets a corresponding DROP route. When a Floating IP is assigned to a VM,...\r\n\r\n&nbsp;\r\n\r\nFrom OpenStack’s install guide, <a href=\"http://docs.openstack.org/havana/install-guide/install/apt/content/install-neutron.configure-networks.html\">Create the base Neutron networks</a>:\r\n<blockquote># neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</blockquote>\r\n<em>“This network represents a slice of the outside world. VMs are not directly linked to this network; instead, they connect to internal networks. Outgoing traffic is routed by Neutron to the external network. Additionally, floating IP addresses from the subnet for ext-net might be assigned to VMs so that the external network can contact them. Neutron routes the traffic appropriately.”</em>\r\n<blockquote># neutron subnet-create ext-net \\\r\n\r\n--allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END \\\r\n\r\n--gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False \\\r\n\r\nEXTERNAL_INTERFACE_CIDR</blockquote>\r\nIn Neutron, a Floating IP is an address reserved from a typically public IPv4 address range for exclusive use by a single VM instance. This model copied AWS’s Elastic IP model. The VM instance itself is not aware of the FIP and you won’t find this address on any of the instance’s network interfaces.\r\n\r\nFloating IP prefixes/ranges are assigned...\r\n\r\nOpenStack’s Basic L3 Operations explains how to reserve a floating IP and associate it with a specific VM NIC:\r\n<blockquote>$ neutron floatingip-create EXT_NET_ID\r\n\r\n$ neutron floatingip-associate FLOATING_IP_ID INTERNAL_VM_PORT_ID</blockquote>\r\nThis is the single-step equivalent:\r\n<blockquote>$ neutron floatingip-create --port_id INTERNAL_VM_PORT_ID EXT_NET_ID</blockquote>\r\n&nbsp;',96,'http://blog.midonet.org/96-revision-v1/',0,'revision','',0),(197,5,'2014-12-06 22:32:25','2014-12-06 22:32:25','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\n</ul>\n\n&nbsp;','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-06 22:32:25','2014-12-06 22:32:25','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\r\n</ul>\r\n&nbsp;',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(198,5,'2014-12-07 16:21:53','2014-12-07 16:21:53','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\n</ul>\n\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\n\n&nbsp;\n\n&nbsp;','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-07 16:21:53','2014-12-07 16:21:53','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\r\n</ul>\r\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\r\n\r\n&nbsp;\r\n\r\n&nbsp;',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(200,5,'2014-12-07 17:51:55','2014-12-07 17:51:55','','Blog L2 Gateways 1','','inherit','open','open','','blog-l2-gateways-1','','','2014-12-07 17:51:55','2014-12-07 17:51:55','',114,'http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png',0,'attachment','image/png',0),(201,5,'2014-12-07 17:51:56','2014-12-07 17:51:56','','Blog L2 Gateways 2','','inherit','open','open','','blog-l2-gateways-2','','','2014-12-07 17:51:56','2014-12-07 17:51:56','',114,'http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png',0,'attachment','image/png',0),(202,5,'2014-12-07 18:25:20','2014-12-07 18:25:20','','Blog L2 Gateways 3','','inherit','open','open','','blog-l2-gateways-3','','','2014-12-07 18:25:20','2014-12-07 18:25:20','',114,'http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png',0,'attachment','image/png',0),(203,5,'2014-12-07 18:25:22','2014-12-07 18:25:22','','Blog L2 Gateways 4','','inherit','open','open','','blog-l2-gateways-4','','','2014-12-07 18:25:22','2014-12-07 18:25:22','',114,'http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png',0,'attachment','image/png',0),(204,5,'2014-12-07 18:29:19','2014-12-07 18:29:19','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\n</ul>\n\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\n\nThere are two main problems with this:\n\n<ul>\n    <li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\n    <li>one physical switch port is being used for each Network that needs to be connected to physical servers</li>\n</ul>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-07 18:29:19','2014-12-07 18:29:19','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\r\n</ul>\r\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\r\n\r\nThere are two main problems with this:\r\n<ul>\r\n	<li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\r\n	<li>one physical switch port is being used for each Network that needs to be connected to physical servers</li>\r\n</ul>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(205,5,'2014-12-07 21:21:19','2014-12-07 21:21:19','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\n</ul>\n\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\n\nFor example, imagine you have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1, therefore each has a network port. You\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\n\nThere are two main problems with this:\n\n<ul>\n    <li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\n    <li>one physical switch port is being used for each Network that needs to be connected to physical servers</li>\n</ul>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-07 21:21:19','2014-12-07 21:21:19','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\r\n</ul>\r\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\r\n\r\nFor example, imagine you have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1, therefore each has a network port. You\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\r\n\r\nThere are two main problems with this:\r\n<ul>\r\n	<li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\r\n	<li>one physical switch port is being used for each Network that needs to be connected to physical servers</li>\r\n</ul>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(206,5,'2014-12-07 22:17:17','2014-12-07 22:17:17','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\n</ul>\n\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\n\nFor example, imagine you have an already installed/running cloud and have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1 and you\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\n\nHow you configure this in MidoNet and in your physical network depends on what your physical network design and the physical switch capabilities. We\'ll start by trying to use concepts we\'ve already learned in previous posts and imagining that the servers\' physical L2 segment is VLAN 10 on an 802.1q compatible switch. The diagram below illustrates one possible physical setup as well as the mapping to the overlay topology.\n\n<ul>\n    <li>Let\'s assume that the physical switch has a free port and is within distance to run an Ethernet cable from that port to a commodity x86 server where MidoNet is installed. The MidoNet host need not be a compute host, but in this example let\'s assume it\'s Compute Host 1.</li>\n    <li>Let\'s also imagine that Host 1 has a free NIC, eth1. Host 1 (and Host 2 as well) has a NIC, eth0, that it uses for to send/receive tunneled overlay traffic to/from other compute hosts.<em> If Host 1 were single-homed we could set up some software bridging to accomplish the setup. For simplicity, we\'ll assume Host 1 has an extra NIC.</em></li>\n    <li>You run a cable from the physical switch\'s free port to Host1\'s eth1 and you configure the switch\'s port to carry untagged VLAN 10 traffic.</li>\n    <li>You use MidoNet\'s API (Neutron does not yet have a L2 Gateway API) to create a free port, P4, on Virtual Bridge 1, and to <em>bind</em> P4 to Host1/eth1. MidoNet Agent on Host 1 learns about that port-interface binding from MidoNet\'s state cluster (powered by Apache ZooKeeper) and plugs eth1 directly into the local datapath.</li>\n    <li>For the sake of this example, let\'s also imagine that VM1 is running on Host 1, while VM2 and VM3 are running on Host 2. As usual for MidoNet hosts, Host 1 and Host 2 both have access to an IP fabric and can tunnel (GRE/VXLAN) overlay traffic to each other.</li>\n</ul>\n\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\n\n<ol>\n    <li>The switch will flood the broadcast packet to all ports in VLAN 10. The packet will therefore be emitted (untagged) from the port towards Host 1\'s eth1.</li>\n    <li>When the packet arrives at Host 1\'s eth1, MidoNet interprets it as arriving at Bridge 1\'s P4. Bridge 1 learns that Server 1\'s MAC is on P4 (i.e. Bridge 1\'s MAC-table gets an entry mapping Server 1\'s MAC to P4).</li>\n    <li>Normally, the Bridge\'s ARP table has entries for the IPs and MACs of VM-facing ports (this was explained in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> and <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a>). However, for the sake of this example, let\'s assume that you\'ve disabled the virtual bridge\'s ARP table. The Simulation on Host 1 therefore determines that Bridge 1 would flood the packet from all its port (except P4, the ingress port). The MN Agent on Host 1 therefore has the datapath emit one copy of the packet locally (from the datapath port corresponding to P1 and therefore also VM1\'s software interface/tap) , and tunnel one copy to Host 2 to be emitted from the datapath ports corresponding to Bridge1\'s ports P2 and P3.</li>\n    <li>VM2 will answer with an ARP reply directed at Server 1\'s MAC. The MN Agent at Host 2 interprets the packet as arriving at port P2 of Bridge 1. The MAC-table entry for Server 1\'s MAC should have propagated from the MN Agent on Host 1 to MN\'s State Database and from there to MN Agents on other hosts, and particularly Host 2. The Simulation on Host 2 will therefore determine that ARP reply (a unicast) should be emitted from Bridge 1\'s port P4. As a result the packet is tunneled from Host 2 to Host 1, with a tunnel key that encodes P4\'s UUID.</li>\n    <li>Upon receiving the tunneled packet, Host 5 will decapsulate it and decode the tunnel key to P4, map P4 to the datapath port for eth1, and consequently emit the ARP reply from eth1.</li>\n    <li>The physical switch will receive the ARP reply, learn VM2\'s MAC, and the forward the ARP reply to Server 1.</li>\n</ol>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\n\nSo this setup accomplishes L2 connectivity between the physical servers and the VM instances. But there are several problems with this approach:\n\n<ul>\n    <li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\n    <li>one physical switch port will be used for each Network that needs to be connected to physical servers</li>\n    <li>you have to make manual changes to the physical network each time you have another request to bridge virtual and physical workloads.</li>\n</ul>\n\nMidoNet\'s VLAN L2 Gateway feature is meant to address these problems while working with the 802.1q physical switch fabric.\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>','Introduction to MN (Part 7 – L2 Gateways)','','inherit','open','open','','114-revision-v1','','','2014-12-07 22:17:17','2014-12-07 22:17:17','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the seventh in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1y\">Part 6</a> covered Floating IPs.</li>\r\n</ul>\r\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\r\n\r\nFor example, imagine you have an already installed/running cloud and have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1 and you\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\r\n\r\nHow you configure this in MidoNet and in your physical network depends on what your physical network design and the physical switch capabilities. We\'ll start by trying to use concepts we\'ve already learned in previous posts and imagining that the servers\' physical L2 segment is VLAN 10 on an 802.1q compatible switch. The diagram below illustrates one possible physical setup as well as the mapping to the overlay topology.\r\n<ul>\r\n	<li>Let\'s assume that the physical switch has a free port and is within distance to run an Ethernet cable from that port to a commodity x86 server where MidoNet is installed. The MidoNet host need not be a compute host, but in this example let\'s assume it\'s Compute Host 1.</li>\r\n	<li>Let\'s also imagine that Host 1 has a free NIC, eth1. Host 1 (and Host 2 as well) has a NIC, eth0, that it uses for to send/receive tunneled overlay traffic to/from other compute hosts.<em> If Host 1 were single-homed we could set up some software bridging to accomplish the setup. For simplicity, we\'ll assume Host 1 has an extra NIC.</em></li>\r\n	<li>You run a cable from the physical switch\'s free port to Host1\'s eth1 and you configure the switch\'s port to carry untagged VLAN 10 traffic.</li>\r\n	<li>You use MidoNet\'s API (Neutron does not yet have a L2 Gateway API) to create a free port, P4, on Virtual Bridge 1, and to <em>bind</em> P4 to Host1/eth1. MidoNet Agent on Host 1 learns about that port-interface binding from MidoNet\'s state cluster (powered by Apache ZooKeeper) and plugs eth1 directly into the local datapath.</li>\r\n	<li>For the sake of this example, let\'s also imagine that VM1 is running on Host 1, while VM2 and VM3 are running on Host 2. As usual for MidoNet hosts, Host 1 and Host 2 both have access to an IP fabric and can tunnel (GRE/VXLAN) overlay traffic to each other.</li>\r\n</ul>\r\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\r\n<ol>\r\n	<li>The switch will flood the broadcast packet to all ports in VLAN 10. The packet will therefore be emitted (untagged) from the port towards Host 1\'s eth1.</li>\r\n	<li>When the packet arrives at Host 1\'s eth1, MidoNet interprets it as arriving at Bridge 1\'s P4. Bridge 1 learns that Server 1\'s MAC is on P4 (i.e. Bridge 1\'s MAC-table gets an entry mapping Server 1\'s MAC to P4).</li>\r\n	<li>Normally, the Bridge\'s ARP table has entries for the IPs and MACs of VM-facing ports (this was explained in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> and <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a>). However, for the sake of this example, let\'s assume that you\'ve disabled the virtual bridge\'s ARP table. The Simulation on Host 1 therefore determines that Bridge 1 would flood the packet from all its port (except P4, the ingress port). The MN Agent on Host 1 therefore has the datapath emit one copy of the packet locally (from the datapath port corresponding to P1 and therefore also VM1\'s software interface/tap) , and tunnel one copy to Host 2 to be emitted from the datapath ports corresponding to Bridge1\'s ports P2 and P3.</li>\r\n	<li>VM2 will answer with an ARP reply directed at Server 1\'s MAC. The MN Agent at Host 2 interprets the packet as arriving at port P2 of Bridge 1. The MAC-table entry for Server 1\'s MAC should have propagated from the MN Agent on Host 1 to MN\'s State Database and from there to MN Agents on other hosts, and particularly Host 2. The Simulation on Host 2 will therefore determine that ARP reply (a unicast) should be emitted from Bridge 1\'s port P4. As a result the packet is tunneled from Host 2 to Host 1, with a tunnel key that encodes P4\'s UUID.</li>\r\n	<li>Upon receiving the tunneled packet, Host 5 will decapsulate it and decode the tunnel key to P4, map P4 to the datapath port for eth1, and consequently emit the ARP reply from eth1.</li>\r\n	<li>The physical switch will receive the ARP reply, learn VM2\'s MAC, and the forward the ARP reply to Server 1.</li>\r\n</ol>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\r\n\r\nSo this setup accomplishes L2 connectivity between the physical servers and the VM instances. But there are several problems with this approach:\r\n<ul>\r\n	<li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\r\n	<li>one physical switch port will be used for each Network that needs to be connected to physical servers</li>\r\n	<li>you have to make manual changes to the physical network each time you have another request to bridge virtual and physical workloads.</li>\r\n</ul>\r\nMidoNet\'s VLAN L2 Gateway feature is meant to address these problems while working with the 802.1q physical switch fabric.\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(207,5,'2014-12-09 12:04:54','2014-12-09 12:04:54','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models:\n\n<ul>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\n    <li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\n    <li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\n    <li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\n</ul>\n\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\n\nFor example, imagine you have an already installed/running cloud and have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1 and you\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\n\nHow you configure this in MidoNet and in your physical network depends on what your physical network design and the physical switch capabilities. We\'ll start by trying to use concepts we\'ve already learned in previous posts and imagining that the servers\' physical L2 segment is VLAN 10 on an 802.1q compatible switch. The diagram below illustrates one possible physical setup as well as the mapping to the overlay topology.\n\n<ul>\n    <li>Let\'s assume that the physical switch has a free port and is within distance to run an Ethernet cable from that port to a commodity x86 server where MidoNet is installed. The MidoNet host need not be a compute host, but in this example let\'s assume it\'s Compute Host 1.</li>\n    <li>Let\'s also imagine that Host 1 has a free NIC, eth1. Host 1 (and Host 2 as well) has a NIC, eth0, that it uses for to send/receive tunneled overlay traffic to/from other compute hosts.<em> If Host 1 were single-homed we could set up some software bridging to accomplish the setup. For simplicity, we\'ll assume Host 1 has an extra NIC.</em></li>\n    <li>You run a cable from the physical switch\'s free port to Host1\'s eth1 and you configure the switch\'s port to carry untagged VLAN 10 traffic.</li>\n    <li>You use MidoNet\'s API (Neutron does not yet have a L2 Gateway API) to create a free port, P4, on Virtual Bridge 1, and to <em>bind</em> P4 to Host1/eth1. MidoNet Agent on Host 1 learns about that port-interface binding from MidoNet\'s state cluster (powered by Apache ZooKeeper) and plugs eth1 directly into the local datapath.</li>\n    <li>For the sake of this example, let\'s also imagine that VM1 is running on Host 1, while VM2 and VM3 are running on Host 2. As usual for MidoNet hosts, Host 1 and Host 2 both have access to an IP fabric and can tunnel (GRE/VXLAN) overlay traffic to each other.</li>\n</ul>\n\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\n\n<ol>\n    <li>The switch will flood the broadcast packet to all ports in VLAN 10. The packet will therefore be emitted (untagged) from the port towards Host 1\'s eth1.</li>\n    <li>When the packet arrives at Host 1\'s eth1, MidoNet interprets it as arriving at Bridge 1\'s P4. Bridge 1 learns that Server 1\'s MAC is on P4 (i.e. Bridge 1\'s MAC-table gets an entry mapping Server 1\'s MAC to P4).</li>\n    <li>Normally, the Bridge\'s ARP table has entries for the IPs and MACs of VM-facing ports (this was explained in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> and <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a>). However, for the sake of this example, let\'s assume that you\'ve disabled the virtual bridge\'s ARP table. The Simulation on Host 1 therefore determines that Bridge 1 would flood the packet from all its port (except P4, the ingress port). The MN Agent on Host 1 therefore has the datapath emit one copy of the packet locally (from the datapath port corresponding to P1 and therefore also VM1\'s software interface/tap) , and tunnel one copy to Host 2 to be emitted from the datapath ports corresponding to Bridge1\'s ports P2 and P3.</li>\n    <li>VM2 will answer with an ARP reply directed at Server 1\'s MAC. The MN Agent at Host 2 interprets the packet as arriving at port P2 of Bridge 1. The MAC-table entry for Server 1\'s MAC should have propagated from the MN Agent on Host 1 to MN\'s State Database and from there to MN Agents on other hosts, and particularly Host 2. The Simulation on Host 2 will therefore determine that ARP reply (a unicast) should be emitted from Bridge 1\'s port P4. As a result the packet is tunneled from Host 2 to Host 1, with a tunnel key that encodes P4\'s UUID.</li>\n    <li>Upon receiving the tunneled packet, Host 5 will decapsulate it and decode the tunnel key to P4, map P4 to the datapath port for eth1, and consequently emit the ARP reply from eth1.</li>\n    <li>The physical switch will receive the ARP reply, learn VM2\'s MAC, and the forward the ARP reply to Server 1.</li>\n</ol>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\n\nSo this setup accomplishes L2 connectivity between the physical servers and the VM instances. But there are several problems with this approach:\n\n<ul>\n    <li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\n    <li>one physical switch port will be used for each Network that needs to be connected to physical servers</li>\n    <li>you have to make manual changes to the physical network each time you have another request to bridge virtual and physical workloads.</li>\n</ul>\n\n<strong>MidoNet\'s VLAN L2 Gateway</strong> is meant to address these problems while continuing to work with the 802.1q physical switch fabric. <em>In a future post we\'ll describe the VXLAN L2 Gateway which solves these problems and more but requires the network operator to have a VTEP (VXLAN Tunnel Endpoint) connected to the physical workloads.</em>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\n\nIn the diagram above, you can see that a new overlay/virtual device has been introduced: a VLAN-aware (802.1Q compatible) Bridge2. Bridge1\'s P4 is now an <em>interior port</em> (meaning it connects to another overlay device and emits traffic that remains in the overlay topology) and links to Bridge2 at an untagged VLAN 10 port. Bridge2 has a trunk port which is <em>bound</em> to Host1/eth1 (where P4 was bound in the earlier/naive setup. Also, and importantly,<strong> the physical switch port facing Host1/eth1 must be put in TRUNK mode</strong> (earlier it was untagged VLAN 10).\n\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\n\n<ol>\n    <li>The physical switch floods the packet, it\'s tagged with VLAN10 and emitted to Host1/eth1.</li>\n    <li>MN interprets the packet as arriving on Bridge2\'s trunk port; Bridge2 learns Server1\'s MAC. The packet must be untagged and flooded from all VLAN 10 untagged ports (MN will only ever allow one, in order to prevent bridging loops). The packet is therefore emitted by Bridge2 on the port facing Bridge1.</li>\n    <li>Bridge1 receives the packet on P4 and learns Server1\'s MAC, floods it to all its port (except the ingress). All VMs on Bridge1 receive the packet. Only VM2 constructs an ARP reply.</li>\n    <li>VM2\'s ARP reply packet ingresses Bridge1 on port P2. The unicast destination MAC matches the just-learned MAC-table entry, so Bridge1 emits the packet from P4.</li>\n    <li>The packet ingresses Bridge2 at the VLAN 10 untagged port. The unicast destination MAC matches the MAC-table entry from step #2, so Bridge2 tags the packet with VLAN 10 and emits it from the trunk port.</li>\n    <li>Since the trunk port is bound to Host1/eth1, the tagged packet is emitted from that interface and arrives at the physical switch\'s trunk port. The physical switch learns VM2\'s MAC on VLAN10, untags the packet and forwards it to Server1.</li>\n</ol>\n\nThe introduction of Bridge2 now allows the following:\n\n<ul>\n    <li>Connect other vlan-agnostic virtual bridges (Neutron networks) to Bridge2. Each one will require a new untagged port on Bridge2 with a unique VLAN id. These are overlay objects and are therefore instantly provisioned.</li>\n    <li>Add a second trunk port to Bridge 2, bound to Host3/eth1. Host3/eth1 is connected via physical cable to another trunk port on the physical switch. This removes the SPOFs. The setup is manual but is only performed once. Thereafter, new vlan gateways require only new ports on Bridge2.</li>\n</ul>\n\n&nbsp;\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>\n\n&nbsp;\n\nFinally, what about STP (Spanning Tree Protocol) and its variants? MidoNet\'s VLAN-aware bridge does not implement STP. However, it recognizes STP packets and:\n\n<ul>\n    <li>drops any arriving on a non-trunk port (i.e. on the untagged VLAN ports)</li>\n    <li>if an STP packet arrives on Trunk1, it\'s forwarded from Trunk2, and vice-versa.</li>\n</ul>\n\nThis <em>STP pass-through</em> allows connecting the VLAN L2 Gateway Nodes/interfaces (Host1/eth1 and Host3/eth1 in our example) to different physical switches that are part of the same L2 fabric. That eliminates the physical switch as a SPOF. STP in the physical fabric, combined with mac-learning on the vlan-aware overlay bridge\'s trunk ports allows outgoing overlay traffic to choose the correct trunk port.','Introduction to MN (Part 6 – VLAN L2 Gateway)','','inherit','open','open','','114-revision-v1','','','2014-12-09 12:04:54','2014-12-09 12:04:54','In this article we discuss how to connect MidoNet virtual workloads to physical workloads. This post is the sixth in a series intended to familiarize users with MidoNet\'s overlay networking models:\r\n<ul>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 1 – Provider Router)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-1-provider-router/\">Part 1</a> covered MN\'s Provider Router.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> covered Tenant Routers and Networks.</li>\r\n	<li><a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a> covered how MN Agent simulates packet traversal of the virtual topology in order to compute a flow rule.</li>\r\n	<li><a href=\"http://wp.me/p5ls6g-1w\">Part 4</a> covered Security Groups and MidoNet\'s low-level Rule-Chain model.</li>\r\n	<li><a title=\"Introduction to MN (Part 5 – Flow State)\" href=\"http://blog.midonet.org/introduction-to-mn-part-5-flow-state/\">Part 5</a> explained how MidoNet distributes flow state to support of stateful network services.</li>\r\n</ul>\r\nA Layer 2 Gateway allows connecting an overlay L2 switches, i.e. a MN bridge that renders a Neutron network, to a physical L2 segment. This is typically used to allow L2 connectivity between VM instances in an OpenStack environment and physical servers (or VM instances) on physical L2 segments external to OpenStack.\r\n\r\nFor example, imagine you have an already installed/running cloud and have Neutron network \"Network 1\" (rendered by Virtual Bridge 1 in MidoNet\'s low-level model). There are three VM instances on Network 1 and you\'d like these VM instances to have L2 access to three physical servers on a physical L2 segment somewhere in your network. The servers might be a database cluster. The following diagram illustrates how you\'d like the overlay network to look: you\'d like the physical segment to be accessible via a port of your Network (equivalently, via a port of the virtual bridge). In the diagram, any traffic from the VM instances to the physical servers, and from the physical servers to the VM instances, will transit port P4 of Virtual Bridge 1.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\"><img class=\"alignnone wp-image-200 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-1.png\" alt=\"Blog L2 Gateways 1\" width=\"342\" height=\"247\" /></a>\r\n\r\nHow you configure this in MidoNet and in your physical network depends on what your physical network design and the physical switch capabilities. We\'ll start by trying to use concepts we\'ve already learned in previous posts and imagining that the servers\' physical L2 segment is VLAN 10 on an 802.1q compatible switch. The diagram below illustrates one possible physical setup as well as the mapping to the overlay topology.\r\n<ul>\r\n	<li>Let\'s assume that the physical switch has a free port and is within distance to run an Ethernet cable from that port to a commodity x86 server where MidoNet is installed. The MidoNet host need not be a compute host, but in this example let\'s assume it\'s Compute Host 1.</li>\r\n	<li>Let\'s also imagine that Host 1 has a free NIC, eth1. Host 1 (and Host 2 as well) has a NIC, eth0, that it uses for to send/receive tunneled overlay traffic to/from other compute hosts.<em> If Host 1 were single-homed we could set up some software bridging to accomplish the setup. For simplicity, we\'ll assume Host 1 has an extra NIC.</em></li>\r\n	<li>You run a cable from the physical switch\'s free port to Host1\'s eth1 and you configure the switch\'s port to carry untagged VLAN 10 traffic.</li>\r\n	<li>You use MidoNet\'s API (Neutron does not yet have a L2 Gateway API) to create a free port, P4, on Virtual Bridge 1, and to <em>bind</em> P4 to Host1/eth1. MidoNet Agent on Host 1 learns about that port-interface binding from MidoNet\'s state cluster (powered by Apache ZooKeeper) and plugs eth1 directly into the local datapath.</li>\r\n	<li>For the sake of this example, let\'s also imagine that VM1 is running on Host 1, while VM2 and VM3 are running on Host 2. As usual for MidoNet hosts, Host 1 and Host 2 both have access to an IP fabric and can tunnel (GRE/VXLAN) overlay traffic to each other.</li>\r\n</ul>\r\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\r\n<ol>\r\n	<li>The switch will flood the broadcast packet to all ports in VLAN 10. The packet will therefore be emitted (untagged) from the port towards Host 1\'s eth1.</li>\r\n	<li>When the packet arrives at Host 1\'s eth1, MidoNet interprets it as arriving at Bridge 1\'s P4. Bridge 1 learns that Server 1\'s MAC is on P4 (i.e. Bridge 1\'s MAC-table gets an entry mapping Server 1\'s MAC to P4).</li>\r\n	<li>Normally, the Bridge\'s ARP table has entries for the IPs and MACs of VM-facing ports (this was explained in <a title=\"Introduction to MN’s Overlay Network Models (Part 2 – Tenant Routers and Bridges)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-2-tenant-routers-bridges/\">Part 2</a> and <a title=\"Introduction to MN’s Overlay Network Models (Part 3 – Simulations)\" href=\"http://blog.midonet.org/introduction-mns-overlay-network-models-part-3-simulations/\">Part 3</a>). However, for the sake of this example, let\'s assume that you\'ve disabled the virtual bridge\'s ARP table. The Simulation on Host 1 therefore determines that Bridge 1 would flood the packet from all its port (except P4, the ingress port). The MN Agent on Host 1 therefore has the datapath emit one copy of the packet locally (from the datapath port corresponding to P1 and therefore also VM1\'s software interface/tap) , and tunnel one copy to Host 2 to be emitted from the datapath ports corresponding to Bridge1\'s ports P2 and P3.</li>\r\n	<li>VM2 will answer with an ARP reply directed at Server 1\'s MAC. The MN Agent at Host 2 interprets the packet as arriving at port P2 of Bridge 1. The MAC-table entry for Server 1\'s MAC should have propagated from the MN Agent on Host 1 to MN\'s State Database and from there to MN Agents on other hosts, and particularly Host 2. The Simulation on Host 2 will therefore determine that ARP reply (a unicast) should be emitted from Bridge 1\'s port P4. As a result the packet is tunneled from Host 2 to Host 1, with a tunnel key that encodes P4\'s UUID.</li>\r\n	<li>Upon receiving the tunneled packet, Host 5 will decapsulate it and decode the tunnel key to P4, map P4 to the datapath port for eth1, and consequently emit the ARP reply from eth1.</li>\r\n	<li>The physical switch will receive the ARP reply, learn VM2\'s MAC, and the forward the ARP reply to Server 1.</li>\r\n</ol>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\"><img class=\"alignnone wp-image-201 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-2.png\" alt=\"Blog L2 Gateways 2\" width=\"504\" height=\"367\" /></a>\r\n\r\nSo this setup accomplishes L2 connectivity between the physical servers and the VM instances. But there are several problems with this approach:\r\n<ul>\r\n	<li>eth1 and Compute Host 1 are SPOFs (Single points of Failure) for connecting Network1 to the physical servers</li>\r\n	<li>one physical switch port will be used for each Network that needs to be connected to physical servers</li>\r\n	<li>you have to make manual changes to the physical network each time you have another request to bridge virtual and physical workloads.</li>\r\n</ul>\r\n<strong>MidoNet\'s VLAN L2 Gateway</strong> is meant to address these problems while continuing to work with the 802.1q physical switch fabric. <em>In a future post we\'ll describe the VXLAN L2 Gateway which solves these problems and more but requires the network operator to have a VTEP (VXLAN Tunnel Endpoint) connected to the physical workloads.</em>\r\n\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\"><img class=\"alignnone wp-image-202 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-3.png\" alt=\"Blog L2 Gateways 3\" width=\"504\" height=\"367\" /></a>\r\n\r\nIn the diagram above, you can see that a new overlay/virtual device has been introduced: a VLAN-aware (802.1Q compatible) Bridge2. Bridge1\'s P4 is now an <em>interior port</em> (meaning it connects to another overlay device and emits traffic that remains in the overlay topology) and links to Bridge2 at an untagged VLAN 10 port. Bridge2 has a trunk port which is <em>bound</em> to Host1/eth1 (where P4 was bound in the earlier/naive setup. Also, and importantly,<strong> the physical switch port facing Host1/eth1 must be put in TRUNK mode</strong> (earlier it was untagged VLAN 10).\r\n\r\nWhat happens when Server1 sends an L2 broadcast packet, for example an ARP request to VM2\'s MAC?\r\n<ol>\r\n	<li>The physical switch floods the packet, it\'s tagged with VLAN10 and emitted to Host1/eth1.</li>\r\n	<li>MN interprets the packet as arriving on Bridge2\'s trunk port; Bridge2 learns Server1\'s MAC. The packet must be untagged and flooded from all VLAN 10 untagged ports (MN will only ever allow one, in order to prevent bridging loops). The packet is therefore emitted by Bridge2 on the port facing Bridge1.</li>\r\n	<li>Bridge1 receives the packet on P4 and learns Server1\'s MAC, floods it to all its port (except the ingress). All VMs on Bridge1 receive the packet. Only VM2 constructs an ARP reply.</li>\r\n	<li>VM2\'s ARP reply packet ingresses Bridge1 on port P2. The unicast destination MAC matches the just-learned MAC-table entry, so Bridge1 emits the packet from P4.</li>\r\n	<li>The packet ingresses Bridge2 at the VLAN 10 untagged port. The unicast destination MAC matches the MAC-table entry from step #2, so Bridge2 tags the packet with VLAN 10 and emits it from the trunk port.</li>\r\n	<li>Since the trunk port is bound to Host1/eth1, the tagged packet is emitted from that interface and arrives at the physical switch\'s trunk port. The physical switch learns VM2\'s MAC on VLAN10, untags the packet and forwards it to Server1.</li>\r\n</ol>\r\nThe introduction of Bridge2 now allows the following:\r\n<ul>\r\n	<li>Connect other vlan-agnostic virtual bridges (Neutron networks) to Bridge2. Each one will require a new untagged port on Bridge2 with a unique VLAN id. These are overlay objects and are therefore instantly provisioned.</li>\r\n	<li>Add a second trunk port to Bridge 2, bound to Host3/eth1. Host3/eth1 is connected via physical cable to another trunk port on the physical switch. This removes the SPOFs. The setup is manual but is only performed once. Thereafter, new vlan gateways require only new ports on Bridge2.</li>\r\n</ul>\r\n&nbsp;\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\"><img class=\"alignnone wp-image-203 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2014/12/Blog-L2-Gateways-4.png\" alt=\"Blog L2 Gateways 4\" width=\"610\" height=\"495\" /></a>\r\n\r\n&nbsp;\r\n\r\nFinally, what about STP (Spanning Tree Protocol) and its variants? MidoNet\'s VLAN-aware bridge does not implement STP. However, it recognizes STP packets and:\r\n<ul>\r\n	<li>drops any arriving on a non-trunk port (i.e. on the untagged VLAN ports)</li>\r\n	<li>if an STP packet arrives on Trunk1, it\'s forwarded from Trunk2, and vice-versa.</li>\r\n</ul>\r\nThis <em>STP pass-through</em> allows connecting the VLAN L2 Gateway Nodes/interfaces (Host1/eth1 and Host3/eth1 in our example) to different physical switches that are part of the same L2 fabric. That eliminates the physical switch as a SPOF. STP in the physical fabric, combined with mac-learning on the vlan-aware overlay bridge\'s trunk ports allows outgoing overlay traffic to choose the correct trunk port.',114,'http://blog.midonet.org/114-revision-v1/',0,'revision','',0),(209,8,'2014-12-17 21:40:02','2014-12-17 21:40:02','<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\n\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n\n&nbsp;\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','publish','open','open','','midonet-2014-11-release','','','2015-01-03 22:05:04','2015-01-03 22:05:04','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\r\n\r\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\r\n\r\n&nbsp;\r\n\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',0,'http://blog.midonet.org/?p=209',0,'post','',0),(210,8,'2014-12-17 16:24:55','2014-12-17 16:24:55','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.</div>\n</div>\n<div>The packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\n\n</div>\n<div>The existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.</div>\n<div>In the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.</div>\n<div></div>\n</div>\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, great job!</div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.</div>','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:24:55','2014-12-17 16:24:55','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.</div>\r\n</div>\r\n<div>The packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\r\n\r\n</div>\r\n<div>The existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.</div>\r\n<div>In the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.</div>\r\n<div></div>\r\n</div>\r\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, great job!</div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.</div>',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(211,8,'2014-12-17 16:26:10','2014-12-17 16:26:10','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of MidoNet 2014.11.</div>\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\n</div>\n</div>\n<div>The packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.</div>\n<div>The existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.</div>\n<div>In the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.</div>\n<div></div>\n</div>\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, great job!</div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.</div>','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:26:10','2014-12-17 16:26:10','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of MidoNet 2014.11.</div>\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\n</div>\r\n</div>\r\n<div>The packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.</div>\r\n<div>The existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.</div>\r\n<div>In the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.</div>\r\n<div></div>\r\n</div>\r\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, great job!</div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.</div>',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(212,8,'2014-12-17 16:27:21','2014-12-17 16:27:21','<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\n\n&nbsp;\n\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\n\nIn the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n\n&nbsp;\n\n</div>\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:27:21','2014-12-17 16:27:21','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\r\n\r\nIn the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\r\n\r\n&nbsp;\r\n\r\n</div>\r\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(213,8,'2014-12-17 16:27:24','2014-12-17 16:27:24','<div>\n<div>\n<div>\n<div>\n<div>\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\n&nbsp;\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\nIn the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n&nbsp;\n</div>\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:27:24','2014-12-17 16:27:24','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<p>The MidoNet project is pleased to announce the release of MidoNet 2014.11.</p>\r\n</div>\r\n<p>The release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.</p>\r\n<p>The packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.</p>\r\n<p>&nbsp;</p>\r\n</div>\r\n</div>\r\n<p>The existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.</p>\r\n<p>In the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.</p>\r\n<p>&nbsp;</p>\r\n</div>\r\n<p>Let me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.</p>\r\n</div>\r\n<p>We would like to thank everyone who contributed to this release, great job!</p>\r\n<p>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.</p>\r\n<p>You can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.</p>\r\n',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(214,8,'2014-12-17 16:27:28','2014-12-17 16:27:28','<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\n\n&nbsp;\n\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\n\nIn the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n\n&nbsp;\n\n</div>\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:27:28','2014-12-17 16:27:28','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\r\n\r\nIn the meantime, we welcome everyone to participate to this update; please feel free to use the existing documentation as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\r\n\r\n&nbsp;\r\n\r\n</div>\r\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(215,8,'2014-12-17 16:29:28','2014-12-17 16:29:28','<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\n\n&nbsp;\n\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\n\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n\n&nbsp;\n\n</div>\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:29:28','2014-12-17 16:29:28','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\r\n\r\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\r\n\r\n&nbsp;\r\n\r\n</div>\r\nLet me or the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt; know if you have any question. You can also reach out via IRC at #midonet.\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(216,8,'2014-12-17 16:32:01','2014-12-17 16:32:01','<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\n\n&nbsp;\n\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\n\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n\n&nbsp;\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:32:01','2014-12-17 16:32:01','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the <a href=\"http://repo.midonet.org/midonet/v2014.11/\" target=\"_blank\">repository</a>.\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\r\n\r\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\r\n\r\n&nbsp;\r\n\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(217,8,'2014-12-17 16:48:44','2014-12-17 16:48:44','<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\n\n</div>\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\n\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\n\n&nbsp;\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2014.11 release','','inherit','open','open','','209-revision-v1','','','2014-12-17 16:48:44','2014-12-17 16:48:44','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2014.11.\r\n\r\n</div>\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2014.11\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nThe existing <a href=\"http://docs.midonet.org\" target=\"_blank\">documentation</a> is based on Openstack Icehouse; further communication will be sent when the documentation is updated to Openstack Juno which is the recommended version.\r\n\r\nIn the meantime, we welcome everyone to participate to the documentation update; please feel free to use the existing quick start guide as a reference and suggest updates for Openstack Juno via the documentation comments or the IRC channel.\r\n\r\n&nbsp;\r\n\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',209,'http://blog.midonet.org/209-revision-v1/',0,'revision','',0),(221,12,'2015-01-05 01:33:38','2015-01-05 01:33:38','Hello,\n\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install MidoNet together with OpenStack inside containers on virtual or physical servers.\n\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\n\nThe difference to <a href=\"https://github.com/midonet/midostack\">MidoStack</a> is that this installer will use the Ubuntu packages from the MidoNet mirrors instead of building MidoNet from source.\n\nIf you are interested in trying out bleeding edge features of MidoNet or if you are developing at the tip of our source code you are encouraged to use the <a href=\"http://midonet.org/#quickstart\">MidoStack developer QuickStart guide</a>.\n\nFor building an OpenStack system using the MidoNet packages from the mirror you can download and use our new package-based MidoNet OpenStack installer from the following url:\n\n<a href=\"https://github.com/midonet/orizuru\">https://github.com/midonet/orizuru</a>\n\n&nbsp;\n\nThe software itself is based on <a href=\"http://www.fabfile.org/\">Fabric</a> (a python based remote execution framework) and <a href=\"http://cuisine.readthedocs.org/en/latest/\">Cuisine</a>.\n\nLocal flow control is implemented in a Makefile.\n\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\n\nThe containers itself are wired using a mesh VPN (based on <a href=\"http://www.tinc-vpn.org/\">tinc</a>) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\n\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digital Ocean, Zetta or Amazon to bootstrap a test environment for looking at MidoNet fast and without investing a lot of hours.\n\nAll you need now to create a demo Open Stack installation is SSH connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, IP addresses and assign the necessary roles to each server in the yaml file.\n\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\n\n&nbsp;\n\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these IPs are only used for providing a secure underlay network model to the containers.\n\nAlso you can decide if you want to look at <a href=\"http://www.midokura.com/midonet-enterprise/\">MEM</a>, the enterprise version of MidoNet, or set up the freely available open source version of MidoNet.\n\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\n\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\n\nAlso you can destroy the containers without modification to your hosts.  This is very good.\n\nYou can use the hosts for installing MidoNet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the life-cycle of an OpenStack installation.\n\n&nbsp;\n\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical MidoNet edge node.\n\nOne of the future features we are currently also working on is to provide an automatic way for setting up BGP connectivity between Quagga running on physical servers and the MidoNet gateways inside the containerized MidoNet installation.\n\nFor this we define the Quagga gateways and AS numbers also in the yaml and the installer will set up everything.\n\nThank you for trying out MidoNet and enjoy the new year!\n\n&nbsp;\n\nAlexander\n\n&nbsp;','An Easier Way to Test Drive MidoNet, with Docker','','publish','open','open','','test-drive-midonet','','','2015-01-05 01:33:38','2015-01-05 01:33:38','Hello,\r\n\r\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install MidoNet together with OpenStack inside containers on virtual or physical servers.\r\n\r\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\r\n\r\nThe difference to <a href=\"https://github.com/midonet/midostack\">MidoStack</a> is that this installer will use the Ubuntu packages from the MidoNet mirrors instead of building MidoNet from source.\r\n\r\nIf you are interested in trying out bleeding edge features of MidoNet or if you are developing at the tip of our source code you are encouraged to use the <a href=\"http://midonet.org/#quickstart\">MidoStack developer QuickStart guide</a>.\r\n\r\nFor building an OpenStack system using the MidoNet packages from the mirror you can download and use our new package-based MidoNet OpenStack installer from the following url:\r\n\r\n<a href=\"https://github.com/midonet/orizuru\">https://github.com/midonet/orizuru</a>\r\n\r\n&nbsp;\r\n\r\nThe software itself is based on <a href=\"http://www.fabfile.org/\">Fabric</a> (a python based remote execution framework) and <a href=\"http://cuisine.readthedocs.org/en/latest/\">Cuisine</a>.\r\n\r\nLocal flow control is implemented in a Makefile.\r\n\r\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\r\n\r\nThe containers itself are wired using a mesh VPN (based on <a href=\"http://www.tinc-vpn.org/\">tinc</a>) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\r\n\r\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digital Ocean, Zetta or Amazon to bootstrap a test environment for looking at MidoNet fast and without investing a lot of hours.\r\n\r\nAll you need now to create a demo Open Stack installation is SSH connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, IP addresses and assign the necessary roles to each server in the yaml file.\r\n\r\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\r\n\r\n&nbsp;\r\n\r\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these IPs are only used for providing a secure underlay network model to the containers.\r\n\r\nAlso you can decide if you want to look at <a href=\"http://www.midokura.com/midonet-enterprise/\">MEM</a>, the enterprise version of MidoNet, or set up the freely available open source version of MidoNet.\r\n\r\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\r\n\r\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\r\n\r\nAlso you can destroy the containers without modification to your hosts.  This is very good.\r\n\r\nYou can use the hosts for installing MidoNet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the life-cycle of an OpenStack installation.\r\n\r\n&nbsp;\r\n\r\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical MidoNet edge node.\r\n\r\nOne of the future features we are currently also working on is to provide an automatic way for setting up BGP connectivity between Quagga running on physical servers and the MidoNet gateways inside the containerized MidoNet installation.\r\n\r\nFor this we define the Quagga gateways and AS numbers also in the yaml and the installer will set up everything.\r\n\r\nThank you for trying out MidoNet and enjoy the new year!\r\n\r\n&nbsp;\r\n\r\nAlexander\r\n\r\n&nbsp;',0,'http://blog.midonet.org/?p=221',0,'post','',0),(222,12,'2015-01-05 00:05:48','2015-01-05 00:05:48','','orizuru','','inherit','open','open','','orizuru','','','2015-01-05 00:05:48','2015-01-05 00:05:48','',221,'http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png',0,'attachment','image/png',0),(223,12,'2015-01-05 00:47:18','2015-01-05 00:47:18','','Cranes_made_by_Origami_paper','','inherit','open','open','','cranes_made_by_origami_paper','','','2015-01-05 00:47:18','2015-01-05 00:47:18','',221,'http://blog.midonet.org/wp-content/uploads/2015/01/Cranes_made_by_Origami_paper.jpg',0,'attachment','image/jpeg',0),(224,12,'2015-01-05 00:48:02','2015-01-05 00:48:02','Hello,\n\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install Midonet together with OpenStack inside containers on virtual or physical servers.\n\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\n\nThe difference to Midostack is that this installer will use the Ubuntu packages from the Midonet mirrors.\n\nIf you are interested in trying out bleeding edge features of Midonet or if you are developing at the tip of our source code you are encouraged to use the Midostack developer quickstart guide.\n\nFor building an OpenStack system using the Midonet packages from the mirror you can download and use our new package-based Midonet Openstack installer from the following url:\n\nhttps://github.com/midonet/orizuru\n\n&nbsp;\n\nThe software itself is based on fabric (a python based remote execution framework) and cuisine.\n\nLocal flow control is implemented in a Makefile.\n\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\n\nThe containers itself are wired using a mesh vpn (based on tinc) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\n\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digitalocean, Zetta or Amazon to bootstrap a test environment for looking at Midonet fast and without investing a lot of hours.\n\nAll you need now to create a demo Openstack installation is ssh connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, ip addresses and assign the necessary roles to each server in the yaml file.\n\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\n\n&nbsp;\n\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these ips are only used for providing a secure underlay network model to the containers.\n\nAlso you can decide if you want to look at MEM, the enterprise version of Midonet, or set up the freely available open source version of Midonet.\n\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\n\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\n\nAlso you can destroy the containers without modification to your hosts.  This is very good.\n\nYou can use the hosts for installing Midonet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the lifecycle of an openstack installation.\n\n&nbsp;\n\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical Midonet edge node.\n\nOne of the future features we are currently also working on is to provide an automatic way for setting up bgp connectivity between quagga running on physical servers and the midonet gateways inside the containerized Midonet installation.\n\nFor this we define the quagga gateways and AS numbers also in the yaml and the installer will set up everything.\n\nThank you for trying out Midonet and enjoy the new year!\n\n&nbsp;\n\nAlexander\n\n&nbsp;','test drive Midonet','','inherit','open','open','','221-revision-v1','','','2015-01-05 00:48:02','2015-01-05 00:48:02','Hello,\r\n\r\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install Midonet together with OpenStack inside containers on virtual or physical servers.\r\n\r\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\r\n\r\nThe difference to Midostack is that this installer will use the Ubuntu packages from the Midonet mirrors.\r\n\r\nIf you are interested in trying out bleeding edge features of Midonet or if you are developing at the tip of our source code you are encouraged to use the Midostack developer quickstart guide.\r\n\r\nFor building an OpenStack system using the Midonet packages from the mirror you can download and use our new package-based Midonet Openstack installer from the following url:\r\n\r\nhttps://github.com/midonet/orizuru\r\n\r\n&nbsp;\r\n\r\nThe software itself is based on fabric (a python based remote execution framework) and cuisine.\r\n\r\nLocal flow control is implemented in a Makefile.\r\n\r\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\r\n\r\nThe containers itself are wired using a mesh vpn (based on tinc) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\r\n\r\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digitalocean, Zetta or Amazon to bootstrap a test environment for looking at Midonet fast and without investing a lot of hours.\r\n\r\nAll you need now to create a demo Openstack installation is ssh connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, ip addresses and assign the necessary roles to each server in the yaml file.\r\n\r\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\r\n\r\n&nbsp;\r\n\r\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these ips are only used for providing a secure underlay network model to the containers.\r\n\r\nAlso you can decide if you want to look at MEM, the enterprise version of Midonet, or set up the freely available open source version of Midonet.\r\n\r\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\r\n\r\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\r\n\r\nAlso you can destroy the containers without modification to your hosts.  This is very good.\r\n\r\nYou can use the hosts for installing Midonet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the lifecycle of an openstack installation.\r\n\r\n&nbsp;\r\n\r\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical Midonet edge node.\r\n\r\nOne of the future features we are currently also working on is to provide an automatic way for setting up bgp connectivity between quagga running on physical servers and the midonet gateways inside the containerized Midonet installation.\r\n\r\nFor this we define the quagga gateways and AS numbers also in the yaml and the installer will set up everything.\r\n\r\nThank you for trying out Midonet and enjoy the new year!\r\n\r\n&nbsp;\r\n\r\nAlexander\r\n\r\n&nbsp;',221,'http://blog.midonet.org/221-revision-v1/',0,'revision','',0),(225,12,'2015-01-05 01:00:35','2015-01-05 01:00:35','Hello,\n\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install Midonet together with OpenStack inside containers on virtual or physical servers.\n\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\n\nThe difference to Midostack is that this installer will use the Ubuntu packages from the Midonet mirrors.\n\nIf you are interested in trying out bleeding edge features of Midonet or if you are developing at the tip of our source code you are encouraged to use the Midostack developer quickstart guide.\n\nFor building an OpenStack system using the Midonet packages from the mirror you can download and use our new package-based Midonet Openstack installer from the following url:\n\nhttps://github.com/midonet/orizuru\n\n&nbsp;\n\nThe software itself is based on fabric (a python based remote execution framework) and cuisine.\n\nLocal flow control is implemented in a Makefile.\n\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\n\nThe containers itself are wired using a mesh vpn (based on tinc) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\n\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digitalocean, Zetta or Amazon to bootstrap a test environment for looking at Midonet fast and without investing a lot of hours.\n\nAll you need now to create a demo Openstack installation is ssh connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, ip addresses and assign the necessary roles to each server in the yaml file.\n\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\n\n&nbsp;\n\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these ips are only used for providing a secure underlay network model to the containers.\n\nAlso you can decide if you want to look at MEM, the enterprise version of Midonet, or set up the freely available open source version of Midonet.\n\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\n\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\n\nAlso you can destroy the containers without modification to your hosts.  This is very good.\n\nYou can use the hosts for installing Midonet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the lifecycle of an openstack installation.\n\n&nbsp;\n\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical Midonet edge node.\n\nOne of the future features we are currently also working on is to provide an automatic way for setting up bgp connectivity between quagga running on physical servers and the midonet gateways inside the containerized Midonet installation.\n\nFor this we define the quagga gateways and AS numbers also in the yaml and the installer will set up everything.\n\nThank you for trying out Midonet and enjoy the new year!\n\n&nbsp;\n\nAlexander\n\n&nbsp;','test drive Midonet','','inherit','open','open','','221-autosave-v1','','','2015-01-05 01:00:35','2015-01-05 01:00:35','',221,'http://blog.midonet.org/221-autosave-v1/',0,'revision','',0),(226,2,'2015-01-05 01:32:40','2015-01-05 01:32:40','Hello,\n\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install MidoNet together with OpenStack inside containers on virtual or physical servers.\n\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\n\nThe difference to <a href=\"https://github.com/midonet/midostack\">MidoStack</a> is that this installer will use the Ubuntu packages from the MidoNet mirrors instead of building MidoNet from source.\n\nIf you are interested in trying out bleeding edge features of MidoNet or if you are developing at the tip of our source code you are encouraged to use the <a href=\"http://midonet.org/#quickstart\">MidoStack developer QuickStart guide</a>.\n\nFor building an OpenStack system using the MidoNet packages from the mirror you can download and use our new package-based MidoNet OpenStack installer from the following url:\n\n<a href=\"https://github.com/midonet/orizuru\">https://github.com/midonet/orizuru</a>\n\n&nbsp;\n\nThe software itself is based on <a href=\"http://www.fabfile.org/\">Fabric</a> (a python based remote execution framework) and <a href=\"http://cuisine.readthedocs.org/en/latest/\">cuisine</a>.\n\nLocal flow control is implemented in a Makefile.\n\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\n\nThe containers itself are wired using a mesh VPN (based on <a href=\"http://www.tinc-vpn.org/\">tinc</a>) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\n\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digital Ocean, Zetta or Amazon to bootstrap a test environment for looking at Midonet fast and without investing a lot of hours.\n\nAll you need now to create a demo Open Stack installation is SSH connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, IP addresses and assign the necessary roles to each server in the yaml file.\n\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\n\n&nbsp;\n\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these IPs are only used for providing a secure underlay network model to the containers.\n\nAlso you can decide if you want to look at <a href=\"http://www.midokura.com/midonet-enterprise/\">MEM</a>, the enterprise version of MidoNet, or set up the freely available open source version of MidoNet.\n\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\n\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\n\nAlso you can destroy the containers without modification to your hosts.  This is very good.\n\nYou can use the hosts for installing MidoNet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the life-cycle of an OpenStack installation.\n\n&nbsp;\n\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical MidoNet edge node.\n\nOne of the future features we are currently also working on is to provide an automatic way for setting up BGP connectivity between Quagga running on physical servers and the MidoNet gateways inside the containerized MidoNet installation.\n\nFor this we define the Quagga gateways and AS numbers also in the yaml and the installer will set up everything.\n\nThank you for trying out MidoNet and enjoy the new year!\n\n&nbsp;\n\nAlexander\n\n&nbsp;','A New Way to Test Drive MidoNet','','inherit','open','open','','221-autosave-v1','','','2015-01-05 01:32:40','2015-01-05 01:32:40','',221,'http://blog.midonet.org/221-autosave-v1/',0,'revision','',0),(227,2,'2015-01-05 01:33:38','2015-01-05 01:33:38','Hello,\n\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install MidoNet together with OpenStack inside containers on virtual or physical servers.\n\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\n\nThe difference to <a href=\"https://github.com/midonet/midostack\">MidoStack</a> is that this installer will use the Ubuntu packages from the MidoNet mirrors instead of building MidoNet from source.\n\nIf you are interested in trying out bleeding edge features of MidoNet or if you are developing at the tip of our source code you are encouraged to use the <a href=\"http://midonet.org/#quickstart\">MidoStack developer QuickStart guide</a>.\n\nFor building an OpenStack system using the MidoNet packages from the mirror you can download and use our new package-based MidoNet OpenStack installer from the following url:\n\n<a href=\"https://github.com/midonet/orizuru\">https://github.com/midonet/orizuru</a>\n\n&nbsp;\n\nThe software itself is based on <a href=\"http://www.fabfile.org/\">Fabric</a> (a python based remote execution framework) and <a href=\"http://cuisine.readthedocs.org/en/latest/\">Cuisine</a>.\n\nLocal flow control is implemented in a Makefile.\n\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\n\nThe containers itself are wired using a mesh VPN (based on <a href=\"http://www.tinc-vpn.org/\">tinc</a>) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\n\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digital Ocean, Zetta or Amazon to bootstrap a test environment for looking at MidoNet fast and without investing a lot of hours.\n\nAll you need now to create a demo Open Stack installation is SSH connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, IP addresses and assign the necessary roles to each server in the yaml file.\n\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\n\n&nbsp;\n\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these IPs are only used for providing a secure underlay network model to the containers.\n\nAlso you can decide if you want to look at <a href=\"http://www.midokura.com/midonet-enterprise/\">MEM</a>, the enterprise version of MidoNet, or set up the freely available open source version of MidoNet.\n\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\n\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\n\nAlso you can destroy the containers without modification to your hosts.  This is very good.\n\nYou can use the hosts for installing MidoNet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the life-cycle of an OpenStack installation.\n\n&nbsp;\n\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical MidoNet edge node.\n\nOne of the future features we are currently also working on is to provide an automatic way for setting up BGP connectivity between Quagga running on physical servers and the MidoNet gateways inside the containerized MidoNet installation.\n\nFor this we define the Quagga gateways and AS numbers also in the yaml and the installer will set up everything.\n\nThank you for trying out MidoNet and enjoy the new year!\n\n&nbsp;\n\nAlexander\n\n&nbsp;','An Easier Way to Test Drive MidoNet, with Docker','','inherit','open','open','','221-revision-v1','','','2015-01-05 01:33:38','2015-01-05 01:33:38','Hello,\r\n\r\nas a result of our brain storms during the engineering meeting in Barcelona at the end of November we have created a new python-based installer that allows you to install MidoNet together with OpenStack inside containers on virtual or physical servers.\r\n\r\nThe purpose of this installer is to provide a way for a practical and fast implementation of our reference architecture for demo purposes as well as serve as an executable specification of our deployment documentation.\r\n\r\nThe difference to <a href=\"https://github.com/midonet/midostack\">MidoStack</a> is that this installer will use the Ubuntu packages from the MidoNet mirrors instead of building MidoNet from source.\r\n\r\nIf you are interested in trying out bleeding edge features of MidoNet or if you are developing at the tip of our source code you are encouraged to use the <a href=\"http://midonet.org/#quickstart\">MidoStack developer QuickStart guide</a>.\r\n\r\nFor building an OpenStack system using the MidoNet packages from the mirror you can download and use our new package-based MidoNet OpenStack installer from the following url:\r\n\r\n<a href=\"https://github.com/midonet/orizuru\">https://github.com/midonet/orizuru</a>\r\n\r\n&nbsp;\r\n\r\nThe software itself is based on <a href=\"http://www.fabfile.org/\">Fabric</a> (a python based remote execution framework) and <a href=\"http://cuisine.readthedocs.org/en/latest/\">Cuisine</a>.\r\n\r\nLocal flow control is implemented in a Makefile.\r\n\r\nIt is very easy to use, all you have to set up is a small and concise configuration file in yaml format where you define the servers and the local network used for the containers in the servers.\r\n\r\nThe containers itself are wired using a mesh VPN (based on <a href=\"http://www.tinc-vpn.org/\">tinc</a>) which means that communication between the services in the containers is kept very secure and not visible to the outside world.\r\n\r\nThis way you can even use public servers from providers like OVH or Hetzner or cloud instances from Digital Ocean, Zetta or Amazon to bootstrap a test environment for looking at MidoNet fast and without investing a lot of hours.\r\n\r\nAll you need now to create a demo Open Stack installation is SSH connectivity to the servers running Ubuntu Linux and an existing local Ubuntu to download and activate the installer after setting your servers, IP addresses and assign the necessary roles to each server in the yaml file.\r\n\r\nThe Makefile will use sudo to prepare your machine with all the necessary prerequisites for running the installation.\r\n\r\n&nbsp;\r\n\r\nThe following picture shows a part of my configuration file, the servers and the role section, and what containers will be created during an installation.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru.png\"><img class=\"alignnone wp-image-222 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/01/orizuru-1024x620.png\" alt=\"orizuru\" width=\"1024\" height=\"620\" /></a>You can choose your own numbers for the local Docker networks, these IPs are only used for providing a secure underlay network model to the containers.\r\n\r\nAlso you can decide if you want to look at <a href=\"http://www.midokura.com/midonet-enterprise/\">MEM</a>, the enterprise version of MidoNet, or set up the freely available open source version of MidoNet.\r\n\r\nAll the services are started in the containers, using the upstart jobs if possible, otherwise they are started by running the necessary commands in screen sessions.\r\n\r\nIf you reboot the servers or shut down the containers you can run the installer Makefile again to restore the test installation.\r\n\r\nAlso you can destroy the containers without modification to your hosts.  This is very good.\r\n\r\nYou can use the hosts for installing MidoNet multiple times and test different versions, for example you can run upgrade tests and dry-run similar important operations tasks that come up in the life-cycle of an OpenStack installation.\r\n\r\n&nbsp;\r\n\r\nWhen the installation is done, the gateways will use static routing and a veth pair to simulate the behaviour of a typical MidoNet edge node.\r\n\r\nOne of the future features we are currently also working on is to provide an automatic way for setting up BGP connectivity between Quagga running on physical servers and the MidoNet gateways inside the containerized MidoNet installation.\r\n\r\nFor this we define the Quagga gateways and AS numbers also in the yaml and the installer will set up everything.\r\n\r\nThank you for trying out MidoNet and enjoy the new year!\r\n\r\n&nbsp;\r\n\r\nAlexander\r\n\r\n&nbsp;',221,'http://blog.midonet.org/221-revision-v1/',0,'revision','',0),(228,12,'2015-01-13 15:17:16','2015-01-13 15:17:16','What is network virtualization?\n\nWhat is a network virtualization overlay?\n\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\n\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\n\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\n\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer. I am looking at a networking solution through the eyes of a user.\n\nAnd I like what I am seeing here.\n\nThe essence of an overlay is that the underlay does not have to understand the traffic any more. It becomes a carrier for data. This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project life-cycles.\n\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\n\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\n\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\n\nIn the future of the internet of things everything will speak internet protocol.\n\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation. Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\n\nHowever, IP is the best underlay technology we have right now.\n\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a VXLAN header and use it as the overlay protocol.\n\nThis year 2015 will be about installers and putting our work upstream.\n\nMidoNet will become visible to you and your customers as an installable option in Mirantis OpenStack, Redhat Enterprise Linux OpenStack, and Ubuntu OpenStack.\n\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\n\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\n\nHave some ideas on your own how to make it better?\n\nTalk to us, we always welcome to your feedback, and we are hiring!','Back to the Future','','publish','open','open','','back-future','','','2015-01-13 15:17:16','2015-01-13 15:17:16','What is network virtualization?\r\n\r\nWhat is a network virtualization overlay?\r\n\r\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\r\n\r\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\r\n\r\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\r\n\r\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer. I am looking at a networking solution through the eyes of a user.\r\n\r\nAnd I like what I am seeing here.\r\n\r\nThe essence of an overlay is that the underlay does not have to understand the traffic any more. It becomes a carrier for data. This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project life-cycles.\r\n\r\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\r\n\r\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\r\n\r\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\r\n\r\nIn the future of the internet of things everything will speak internet protocol.\r\n\r\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation. Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\r\n\r\nHowever, IP is the best underlay technology we have right now.\r\n\r\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a VXLAN header and use it as the overlay protocol.\r\n\r\nThis year 2015 will be about installers and putting our work upstream.\r\n\r\nMidoNet will become visible to you and your customers as an installable option in Mirantis OpenStack, Redhat Enterprise Linux OpenStack, and Ubuntu OpenStack.\r\n\r\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\r\n\r\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\r\n\r\nHave some ideas on your own how to make it better?\r\n\r\nTalk to us, we always welcome to your feedback, and we are hiring!',0,'http://blog.midonet.org/?p=228',0,'post','',0),(229,12,'2015-01-13 13:59:30','2015-01-13 13:59:30','What is network virtualization?\n\nWhat is a network virtualization overlay?\n\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\n\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\n\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\n\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer.  I am looking at a networking solution through the eyes of a user.\n\nAnd i like what i am seeing here.\n\nThe essence of an overlay is that the underlay does not have to understand the traffic any more.  It becomes a carrier for data.  This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project lifecycles.\n\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\n\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\n\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\n\nIn the future of the internet of things everything will speak internet protocol.\n\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation.  Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\n\nHowever, IP is the best underlay technology we have right now.\n\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a Vxlan header and use it as the overlay protocol also.\n\nThis year 2015 will be about installers and putting our work upstream.\n\nMidonet will become visible to you and your customers as an installable option in Mirantis, RHEL and Ubuntu OpenStack.\n\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\n\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\n\nHave some ideas on your own how to make it better?\n\nTalk to us, we always welcome to your feedback and we are hiring!','back to the future','','inherit','open','open','','228-revision-v1','','','2015-01-13 13:59:30','2015-01-13 13:59:30','What is network virtualization?\r\n\r\nWhat is a network virtualization overlay?\r\n\r\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\r\n\r\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\r\n\r\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\r\n\r\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer.  I am looking at a networking solution through the eyes of a user.\r\n\r\nAnd i like what i am seeing here.\r\n\r\nThe essence of an overlay is that the underlay does not have to understand the traffic any more.  It becomes a carrier for data.  This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project lifecycles.\r\n\r\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\r\n\r\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\r\n\r\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\r\n\r\nIn the future of the internet of things everything will speak internet protocol.\r\n\r\n\r\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation.  Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\r\n\r\nHowever, IP is the best underlay technology we have right now.\r\n\r\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a Vxlan header and use it as the overlay protocol also.\r\n\r\nThis year 2015 will be about installers and putting our work upstream.\r\n\r\nMidonet will become visible to you and your customers as an installable option in Mirantis, RHEL and Ubuntu OpenStack.\r\n\r\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\r\n\r\n\r\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\r\n\r\nHave some ideas on your own how to make it better?\r\n\r\nTalk to us, we always welcome to your feedback and we are hiring!',228,'http://blog.midonet.org/228-revision-v1/',0,'revision','',0),(231,2,'2015-01-13 15:16:58','2015-01-13 15:16:58','What is network virtualization?\n\nWhat is a network virtualization overlay?\n\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\n\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\n\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\n\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer. I am looking at a networking solution through the eyes of a user.\n\nAnd I like what I am seeing here.\n\nThe essence of an overlay is that the underlay does not have to understand the traffic any more. It becomes a carrier for data. This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project life-cycles.\n\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\n\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\n\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\n\nIn the future of the internet of things everything will speak internet protocol.\n\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation. Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\n\nHowever, IP is the best underlay technology we have right now.\n\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a VXLAN header and use it as the overlay protocol.\n\nThis year 2015 will be about installers and putting our work upstream.\n\nMidoNet will become visible to you and your customers as an installable option in Mirantis OpenStack, Redhat Enterprise Linux OpenStack, and Ubuntu OpenStack.\n\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\n\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\n\nHave some ideas on your own how to make it better?\n\nTalk to us, we always welcome to your feedback, and we are hiring!','Back to the Future','','inherit','open','open','','228-autosave-v1','','','2015-01-13 15:16:58','2015-01-13 15:16:58','',228,'http://blog.midonet.org/228-autosave-v1/',0,'revision','',0),(232,2,'2015-01-13 15:16:45','2015-01-13 15:16:45','','Giant_Cogwheel_Sculpture_Wynyard_Quarter','','inherit','open','open','','giant_cogwheel_sculpture_wynyard_quarter','','','2015-01-13 15:16:45','2015-01-13 15:16:45','',228,'http://blog.midonet.org/wp-content/uploads/2015/01/Giant_Cogwheel_Sculpture_Wynyard_Quarter.jpg',0,'attachment','image/jpeg',0),(233,2,'2015-01-13 15:17:16','2015-01-13 15:17:16','What is network virtualization?\n\nWhat is a network virtualization overlay?\n\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\n\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\n\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\n\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer. I am looking at a networking solution through the eyes of a user.\n\nAnd I like what I am seeing here.\n\nThe essence of an overlay is that the underlay does not have to understand the traffic any more. It becomes a carrier for data. This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project life-cycles.\n\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\n\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\n\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\n\nIn the future of the internet of things everything will speak internet protocol.\n\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation. Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\n\nHowever, IP is the best underlay technology we have right now.\n\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a VXLAN header and use it as the overlay protocol.\n\nThis year 2015 will be about installers and putting our work upstream.\n\nMidoNet will become visible to you and your customers as an installable option in Mirantis OpenStack, Redhat Enterprise Linux OpenStack, and Ubuntu OpenStack.\n\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\n\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\n\nHave some ideas on your own how to make it better?\n\nTalk to us, we always welcome to your feedback, and we are hiring!','Back to the Future','','inherit','open','open','','228-revision-v1','','','2015-01-13 15:17:16','2015-01-13 15:17:16','What is network virtualization?\r\n\r\nWhat is a network virtualization overlay?\r\n\r\nWhen you think about and look at the cogwheels that keep our solution together you will find out that things like NAT and tunneling are here since the 90s.\r\n\r\nWhen i worked with Linux and NAT for the first time the tool was still known as ipmasqadm.\r\n\r\nBasically every VPN you ever built (and every PPP connection you ever did - and every phone call you ever had) was, technically, a network \'virtualization\' overlay.\r\n\r\nI have seen and debugged routers, firewalls, even double NAT firewalls (company merger, both had the same 10.x.x.x networks) from the point of view of a data center systems engineer. I am looking at a networking solution through the eyes of a user.\r\n\r\nAnd I like what I am seeing here.\r\n\r\nThe essence of an overlay is that the underlay does not have to understand the traffic any more. It becomes a carrier for data. This is the logical consequence of transforming your IT department to a service organization, giving your users the freedom to build arbitrary, safe, high performance, value driven network topologies for their applications and their software development project life-cycles.\r\n\r\nSDN is nothing but a nice industry term for \'TCP/IP network change management as a service\'.\r\n\r\nSome people say that a disruptive, L3 centric, IP-only overlay is the wrong way to enter the network virtualization market (Here\'s looking at you, MPLS).\r\n\r\nWe believe that IP, and only IP, will be the technology that will be the last survivor of the architecture wars (which, according to Big V are supposed to be already over).\r\n\r\nIn the future of the internet of things everything will speak internet protocol.\r\n\r\nPeople who have been working with IP and routing for a long time know that it has its own problems,- provide a working implementation of roaming for example is a dirty little secret and has taken countless sleepless nights from network engineers before seeing a successful implementation. Or put an IP address into every car of a country and try to make them talk to a single load balancer IP for modern telemetry applications. Just not possible with one IP and one piece of hardware.\r\n\r\nHowever, IP is the best underlay technology we have right now.\r\n\r\nSo it is only fair to wrap it up decoratively in the fancy gift paper of a VXLAN header and use it as the overlay protocol.\r\n\r\nThis year 2015 will be about installers and putting our work upstream.\r\n\r\nMidoNet will become visible to you and your customers as an installable option in Mirantis OpenStack, Redhat Enterprise Linux OpenStack, and Ubuntu OpenStack.\r\n\r\nBecause we are rooted in Open Source and deeply converged with the Linux networking model (ovs-datapath, netlink, tap devices, network namespaces) this is only consequential and logical.\r\n\r\nWe understand that most people out there are still afraid of network virtualization. After all its a new concept of doing things. Like every new technology in this wonderful market of information technology we will prove that our product, distributed, IP- based, API- driven satisfies a business need and makes the work of the systems engineers easier who have to deal, live and work with this architecture on a daily basis.\r\n\r\nHave some ideas on your own how to make it better?\r\n\r\nTalk to us, we always welcome to your feedback, and we are hiring!',228,'http://blog.midonet.org/228-revision-v1/',0,'revision','',0),(235,1,'2015-01-20 19:41:29','2015-01-20 19:41:29','Watch the video to hear from Distinguished Technologist Wenjing Chu about the Open NFV Project and the trends for open network virtualization.\nhttps://www.youtube.com/watch?v=4oPL1Th05C4','VIDEO: Midokura aligns with Dell on Open Platform for NFV','','publish','open','open','','midokura-aligns-dell-open-platform-nfv','','','2015-01-20 19:46:17','2015-01-20 19:46:17','Watch the video to hear from Distinguished Technologist Wenjing Chu about the Open NFV Project and the trends for open network virtualization.\r\nhttps://www.youtube.com/watch?v=4oPL1Th05C4',0,'http://blog.midonet.org/?p=235',0,'post','',0),(236,1,'2015-01-20 19:41:29','2015-01-20 19:41:29','https://www.youtube.com/watch?v=4oPL1Th05C4','Midokura aligns with Dell on Open Platform for NFV','','inherit','open','open','','235-revision-v1','','','2015-01-20 19:41:29','2015-01-20 19:41:29','https://www.youtube.com/watch?v=4oPL1Th05C4',235,'http://blog.midonet.org/235-revision-v1/',0,'revision','',0),(237,1,'2015-01-20 19:43:20','2015-01-20 19:43:20','','Screen Shot 2015-01-20 at 11.42.59 AM','','inherit','open','open','','screen-shot-2015-01-20-at-11-42-59-am','','','2015-01-20 19:43:20','2015-01-20 19:43:20','',235,'http://blog.midonet.org/wp-content/uploads/2015/01/Screen-Shot-2015-01-20-at-11.42.59-AM.png',0,'attachment','image/png',0),(238,1,'2015-01-20 19:44:38','2015-01-20 19:44:38','https://www.youtube.com/watch?v=4oPL1Th05C4','VIDEO: Midokura aligns with Dell on Open Platform for NFV','','inherit','open','open','','235-revision-v1','','','2015-01-20 19:44:38','2015-01-20 19:44:38','https://www.youtube.com/watch?v=4oPL1Th05C4',235,'http://blog.midonet.org/235-revision-v1/',0,'revision','',0),(239,1,'2015-01-20 19:46:17','2015-01-20 19:46:17','Watch the video to hear from Distinguished Technologist Wenjing Chu about the Open NFV Project and the trends for open network virtualization.\nhttps://www.youtube.com/watch?v=4oPL1Th05C4','VIDEO: Midokura aligns with Dell on Open Platform for NFV','','inherit','open','open','','235-revision-v1','','','2015-01-20 19:46:17','2015-01-20 19:46:17','Watch the video to hear from Distinguished Technologist Wenjing Chu about the Open NFV Project and the trends for open network virtualization.\r\nhttps://www.youtube.com/watch?v=4oPL1Th05C4',235,'http://blog.midonet.org/235-revision-v1/',0,'revision','',0),(241,8,'2015-01-26 14:06:58','0000-00-00 00:00:00','This post details the implementation of Load Balancer as a Service (LBaaS) inside MidoNet and how it differs from the Openstack Neutron LBaaS implementation.\n\nNeutron implemented LBaaS with the assumption there will be an appliance hooked up in the network to perform the load balancing. Typically this appliance is HA Proxy.\n\nBecause the appliance will be assigned the floating IP (FIP) ; you can\'t use the FIP on the router itself.\n\n&nbsp;\nbecause you have to hook up the appliance, you can\'t use the FIP anymore\n\nin MN LB is a public IP and the nodes are private IP\nMN LB is not L7 so we can\'t modify the headers HTTP\n\ntempest and horizon are designed to work with upstream and default HA proxy implementation\nwe need to attach the pool to a router, that isn\'t done in tempest\nthat\'s why all tempest tests are failing for LB\n\nLBaaS API already allows extra fields in the GUI such as the router ID.\n\nA similar issue will arise with FWaaS.\n\nThis issue also happens with NSX implementation','MidoNet\'s LBaaS implementation','','draft','open','open','','','','','2015-01-26 14:06:58','2015-01-26 14:06:58','This post details the implementation of Load Balancer as a Service (LBaaS) inside MidoNet and how it differs from the Openstack Neutron LBaaS implementation.\r\n\r\nNeutron implemented LBaaS with the assumption there will be an appliance hooked up in the network to perform the load balancing. Typically this appliance is HA Proxy.\r\n\r\nBecause the appliance will be assigned the floating IP (FIP) ; you can\'t use the FIP on the router itself.\r\n\r\n&nbsp;\r\nbecause you have to hook up the appliance, you can\'t use the FIP anymore\r\n\r\nin MN LB is a public IP and the nodes are private IP\r\nMN LB is not L7 so we can\'t modify the headers HTTP\r\n\r\ntempest and horizon are designed to work with upstream and default HA proxy implementation\r\nwe need to attach the pool to a router, that isn\'t done in tempest\r\nthat\'s why all tempest tests are failing for LB\r\n\r\nLBaaS API already allows extra fields in the GUI such as the router ID.\r\n\r\nA similar issue will arise with FWaaS.\r\n\r\nThis issue also happens with NSX implementation',0,'http://blog.midonet.org/?p=241',0,'post','',0),(242,8,'2015-01-26 14:06:58','2015-01-26 14:06:58','This post details the implementation of Load Balancer as a Service (LBaaS) inside MidoNet and how it differs from the Openstack Neutron LBaaS implementation.\n\nNeutron implemented LBaaS with the assumption there will be an appliance hooked up in the network to perform the load balancing. Typically this appliance is HA Proxy.\n\nBecause the appliance will be assigned the floating IP (FIP) ; you can\'t use the FIP on the router itself.\n\n&nbsp;\nbecause you have to hook up the appliance, you can\'t use the FIP anymore\n\nin MN LB is a public IP and the nodes are private IP\nMN LB is not L7 so we can\'t modify the headers HTTP\n\ntempest and horizon are designed to work with upstream and default HA proxy implementation\nwe need to attach the pool to a router, that isn\'t done in tempest\nthat\'s why all tempest tests are failing for LB\n\nLBaaS API already allows extra fields in the GUI such as the router ID.\n\nA similar issue will arise with FWaaS.\n\nThis issue also happens with NSX implementation','MidoNet\'s LBaaS implementation','','inherit','open','open','','241-revision-v1','','','2015-01-26 14:06:58','2015-01-26 14:06:58','This post details the implementation of Load Balancer as a Service (LBaaS) inside MidoNet and how it differs from the Openstack Neutron LBaaS implementation.\r\n\r\nNeutron implemented LBaaS with the assumption there will be an appliance hooked up in the network to perform the load balancing. Typically this appliance is HA Proxy.\r\n\r\nBecause the appliance will be assigned the floating IP (FIP) ; you can\'t use the FIP on the router itself.\r\n\r\n&nbsp;\r\nbecause you have to hook up the appliance, you can\'t use the FIP anymore\r\n\r\nin MN LB is a public IP and the nodes are private IP\r\nMN LB is not L7 so we can\'t modify the headers HTTP\r\n\r\ntempest and horizon are designed to work with upstream and default HA proxy implementation\r\nwe need to attach the pool to a router, that isn\'t done in tempest\r\nthat\'s why all tempest tests are failing for LB\r\n\r\nLBaaS API already allows extra fields in the GUI such as the router ID.\r\n\r\nA similar issue will arise with FWaaS.\r\n\r\nThis issue also happens with NSX implementation',241,'http://blog.midonet.org/241-revision-v1/',0,'revision','',0),(244,8,'2015-02-05 16:34:06','2015-02-05 16:34:06','<div>\n<div>\n<div>\n<div>\n<div>\n\n<a href=\"https://www.flickr.com/photos/salman2000/6796263679/in/photostream/\"><img class=\"alignnone\" src=\"https://farm8.staticflickr.com/7164/6796263679_347394850b_z_d.jpg\" alt=\"\" width=\"640\" height=\"424\" /></a>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\n\n</div>\n&nbsp;\n\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.01 release','','publish','open','open','','midonet-2015-01-release','','','2015-02-05 16:34:27','2015-02-05 16:34:27','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\n<a href=\"https://www.flickr.com/photos/salman2000/6796263679/in/photostream/\"><img class=\"alignnone\" src=\"https://farm8.staticflickr.com/7164/6796263679_347394850b_z_d.jpg\" alt=\"\" width=\"640\" height=\"424\" /></a>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\r\n\r\n</div>\r\n&nbsp;\r\n\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',0,'http://blog.midonet.org/?p=244',0,'post','',0),(245,8,'2015-02-05 16:29:50','2015-02-05 16:29:50','<div>\n<div>\n<div>\n<div>\n<div>\n\n[embed]https://flic.kr/p/bmyCpZ[/embed]\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\n\n</div>\n&nbsp;\n\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n&nbsp;\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.01 release','','inherit','open','open','','244-revision-v1','','','2015-02-05 16:29:50','2015-02-05 16:29:50','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\n[embed]https://flic.kr/p/bmyCpZ[/embed]\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\r\n\r\n</div>\r\n&nbsp;\r\n\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n&nbsp;\r\n\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',244,'http://blog.midonet.org/244-revision-v1/',0,'revision','',0),(246,8,'2015-02-05 16:29:58','2015-02-05 16:29:58','<div>\n<div>\n<div>\n<div>\n<div>\n\n[embed]https://flic.kr/p/bmyCpZ[/embed]\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\n\n</div>\n&nbsp;\n\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.01 release','','inherit','open','open','','244-revision-v1','','','2015-02-05 16:29:58','2015-02-05 16:29:58','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\n[embed]https://flic.kr/p/bmyCpZ[/embed]\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\r\n\r\n</div>\r\n&nbsp;\r\n\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',244,'http://blog.midonet.org/244-revision-v1/',0,'revision','',0),(247,8,'2015-02-05 16:33:10','2015-02-05 16:33:10','<div>\n<div>\n<div>\n<div>\n<div>\n\n<a href=\"https://www.flickr.com/photos/salman2000/6796263679/in/photostream/\"><img class=\"alignnone\" src=\"https://farm8.staticflickr.com/7164/6796263679_347394850b_z_d.jpg\" alt=\"\" width=\"640\" height=\"424\" /></a>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\n\n</div>\n&nbsp;\n\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.01 release','','inherit','open','open','','244-revision-v1','','','2015-02-05 16:33:10','2015-02-05 16:33:10','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\n<a href=\"https://www.flickr.com/photos/salman2000/6796263679/in/photostream/\"><img class=\"alignnone\" src=\"https://farm8.staticflickr.com/7164/6796263679_347394850b_z_d.jpg\" alt=\"\" width=\"640\" height=\"424\" /></a>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\r\n\r\n</div>\r\n&nbsp;\r\n\r\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',244,'http://blog.midonet.org/244-revision-v1/',0,'revision','',0),(248,8,'2015-02-05 16:34:51','2015-02-05 16:34:51','<div>\n<div>\n<div>\n<div>\n<div>\n\n<a href=\"https://www.flickr.com/photos/salman2000/6796263679/in/photostream/\"><img class=\"alignnone\" src=\"https://farm8.staticflickr.com/7164/6796263679_347394850b_z_d.jpg\" alt=\"\" width=\"640\" height=\"424\" /></a>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.01.\n\n</div>\n&nbsp;\n\nThe release notes are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.01\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via IRC at #midonet or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.01 release','','inherit','open','open','','244-autosave-v1','','','2015-02-05 16:34:51','2015-02-05 16:34:51','',244,'http://blog.midonet.org/244-autosave-v1/',0,'revision','',0),(250,1,'2015-02-17 08:49:23','2015-02-17 08:49:23','Hey Midos,\n\nWe recently made a decision to move our public chat channels away from freenode IRC, over to <a href=\"http://slack.com\">Slack</a>.  You can immediately sign up for our public Slack team at <a href=\"https://slack.midonet.org\">https://slack.midonet.org</a>. <a href=\"http://midokura.com\">Midokura</a> started out using IRC long ago, and moved to more modern chat based systems awhile ago and never looked back.  Slack has been our tool of choice to communicate internally for many reason.  It has <a href=\"https://slack.com/is/team-communication\">many more features</a>, and is generally easier to use than IRC for non-techie people.\n\nWe originally went with IRC for the open MidoNet community because it was the typical tool used to communicate for open source projects.  Unfortunately, many of the tools which open source projects use are becoming antiquated, and in need of something better.  We\'ve seen tons of open source projects moving their code repositories over to Github for modern tools and communication, but chat has remained stagnant.  We think that it\'s time for a change.\n\nWe recently saw some other communities moving over to Slack, and decided to make the jump ourselves.  We\'ve used an open source project called <a href=\"https://github.com/rauchg/slackin\">SlackIn</a> to create the automated signup page found at https://slack.midonet.org.  Huge props to <a href=\"https://github.com/rauchg/\">rauchg</a> for creating this great project which enables open source communities to take advantage of all that Slack has to offer.\n\nBe sure to <a href=\"https://slack.midonet.org\">sign up for free</a> and say hello to the rest of the Midos!','IRC chat has moved to Slack','','publish','open','open','','irc-chat-moved-slack','','','2015-02-17 08:50:17','2015-02-17 08:50:17','Hey Midos,\r\n\r\nWe recently made a decision to move our public chat channels away from freenode IRC, over to <a href=\"http://slack.com\">Slack</a>.  You can immediately sign up for our public Slack team at <a href=\"https://slack.midonet.org\">https://slack.midonet.org</a>. <a href=\"http://midokura.com\">Midokura</a> started out using IRC long ago, and moved to more modern chat based systems awhile ago and never looked back.  Slack has been our tool of choice to communicate internally for many reason.  It has <a href=\"https://slack.com/is/team-communication\">many more features</a>, and is generally easier to use than IRC for non-techie people.  \r\n\r\nWe originally went with IRC for the open MidoNet community because it was the typical tool used to communicate for open source projects.  Unfortunately, many of the tools which open source projects use are becoming antiquated, and in need of something better.  We\'ve seen tons of open source projects moving their code repositories over to Github for modern tools and communication, but chat has remained stagnant.  We think that it\'s time for a change.\r\n\r\nWe recently saw some other communities moving over to Slack, and decided to make the jump ourselves.  We\'ve used an open source project called <a href=\"https://github.com/rauchg/slackin\">SlackIn</a> to create the automated signup page found at https://slack.midonet.org.  Huge props to <a href=\"https://github.com/rauchg/\">rauchg</a> for creating this great project which enables open source communities to take advantage of all that Slack has to offer.\r\n\r\nBe sure to <a href=\"https://slack.midonet.org\">sign up for free</a> and say hello to the rest of the Midos!',0,'http://blog.midonet.org/?p=250',0,'post','',1),(251,1,'2015-02-17 08:49:10','2015-02-17 08:49:10','','slack-logo','','inherit','open','open','','slack-logo','','','2015-02-17 08:49:10','2015-02-17 08:49:10','',250,'http://blog.midonet.org/wp-content/uploads/2015/02/slack-logo.png',0,'attachment','image/png',0),(252,1,'2015-02-17 08:49:23','2015-02-17 08:49:23','Hey Midos,\n\nWe recently made a decision to move our public chat channels away from freenode IRC, over to <a href=\"http://slack.com\">Slack</a>.  You can immediately sign up for our public Slack team at https://slack.midonet.org. <a href=\"http://midokura.com\">Midokura</a> started out using IRC long ago, and moved to more modern chat based systems awhile ago and never looked back.  Slack has been our tool of choice to communicate internally for many reason.  It has <a href=\"https://slack.com/is/team-communication\">many more features</a>, and is generally easier to use than IRC for non-techie people.\n\nWe originally went with IRC for the open MidoNet community because it was the typical tool used to communicate for open source projects.  Unfortunately, many of the tools which open source projects use are becoming antiquated, and in need of something better.  We\'ve seen tons of open source projects moving their code repositories over to Github for modern tools and communication, but chat has remained stagnant.  We think that it\'s time for a change.\n\nWe recently saw some other communities moving over to Slack, and decided to make the jump ourselves.  We\'ve used an open source project called <a href=\"https://github.com/rauchg/slackin\">SlackIn</a> to create the automated signup page found at https://slack.midonet.org.  Huge props to <a href=\"https://github.com/rauchg/\">rauchg</a> for creating this great project which enables open source communities to take advantage of all that Slack has to offer.\n\nBe sure to <a href=\"https://slack.midonet.org\">sign up for free</a> and say hello to the rest of the Midos!','IRC chat has moved to Slack','','inherit','open','open','','250-revision-v1','','','2015-02-17 08:49:23','2015-02-17 08:49:23','Hey Midos,\r\n\r\nWe recently made a decision to move our public chat channels away from freenode IRC, over to <a href=\"http://slack.com\">Slack</a>.  You can immediately sign up for our public Slack team at https://slack.midonet.org. <a href=\"http://midokura.com\">Midokura</a> started out using IRC long ago, and moved to more modern chat based systems awhile ago and never looked back.  Slack has been our tool of choice to communicate internally for many reason.  It has <a href=\"https://slack.com/is/team-communication\">many more features</a>, and is generally easier to use than IRC for non-techie people.  \r\n\r\nWe originally went with IRC for the open MidoNet community because it was the typical tool used to communicate for open source projects.  Unfortunately, many of the tools which open source projects use are becoming antiquated, and in need of something better.  We\'ve seen tons of open source projects moving their code repositories over to Github for modern tools and communication, but chat has remained stagnant.  We think that it\'s time for a change.\r\n\r\nWe recently saw some other communities moving over to Slack, and decided to make the jump ourselves.  We\'ve used an open source project called <a href=\"https://github.com/rauchg/slackin\">SlackIn</a> to create the automated signup page found at https://slack.midonet.org.  Huge props to <a href=\"https://github.com/rauchg/\">rauchg</a> for creating this great project which enables open source communities to take advantage of all that Slack has to offer.\r\n\r\nBe sure to <a href=\"https://slack.midonet.org\">sign up for free</a> and say hello to the rest of the Midos!',250,'http://blog.midonet.org/250-revision-v1/',0,'revision','',0),(253,1,'2015-02-17 08:50:17','2015-02-17 08:50:17','Hey Midos,\n\nWe recently made a decision to move our public chat channels away from freenode IRC, over to <a href=\"http://slack.com\">Slack</a>.  You can immediately sign up for our public Slack team at <a href=\"https://slack.midonet.org\">https://slack.midonet.org</a>. <a href=\"http://midokura.com\">Midokura</a> started out using IRC long ago, and moved to more modern chat based systems awhile ago and never looked back.  Slack has been our tool of choice to communicate internally for many reason.  It has <a href=\"https://slack.com/is/team-communication\">many more features</a>, and is generally easier to use than IRC for non-techie people.\n\nWe originally went with IRC for the open MidoNet community because it was the typical tool used to communicate for open source projects.  Unfortunately, many of the tools which open source projects use are becoming antiquated, and in need of something better.  We\'ve seen tons of open source projects moving their code repositories over to Github for modern tools and communication, but chat has remained stagnant.  We think that it\'s time for a change.\n\nWe recently saw some other communities moving over to Slack, and decided to make the jump ourselves.  We\'ve used an open source project called <a href=\"https://github.com/rauchg/slackin\">SlackIn</a> to create the automated signup page found at https://slack.midonet.org.  Huge props to <a href=\"https://github.com/rauchg/\">rauchg</a> for creating this great project which enables open source communities to take advantage of all that Slack has to offer.\n\nBe sure to <a href=\"https://slack.midonet.org\">sign up for free</a> and say hello to the rest of the Midos!','IRC chat has moved to Slack','','inherit','open','open','','250-revision-v1','','','2015-02-17 08:50:17','2015-02-17 08:50:17','Hey Midos,\r\n\r\nWe recently made a decision to move our public chat channels away from freenode IRC, over to <a href=\"http://slack.com\">Slack</a>.  You can immediately sign up for our public Slack team at <a href=\"https://slack.midonet.org\">https://slack.midonet.org</a>. <a href=\"http://midokura.com\">Midokura</a> started out using IRC long ago, and moved to more modern chat based systems awhile ago and never looked back.  Slack has been our tool of choice to communicate internally for many reason.  It has <a href=\"https://slack.com/is/team-communication\">many more features</a>, and is generally easier to use than IRC for non-techie people.  \r\n\r\nWe originally went with IRC for the open MidoNet community because it was the typical tool used to communicate for open source projects.  Unfortunately, many of the tools which open source projects use are becoming antiquated, and in need of something better.  We\'ve seen tons of open source projects moving their code repositories over to Github for modern tools and communication, but chat has remained stagnant.  We think that it\'s time for a change.\r\n\r\nWe recently saw some other communities moving over to Slack, and decided to make the jump ourselves.  We\'ve used an open source project called <a href=\"https://github.com/rauchg/slackin\">SlackIn</a> to create the automated signup page found at https://slack.midonet.org.  Huge props to <a href=\"https://github.com/rauchg/\">rauchg</a> for creating this great project which enables open source communities to take advantage of all that Slack has to offer.\r\n\r\nBe sure to <a href=\"https://slack.midonet.org\">sign up for free</a> and say hello to the rest of the Midos!',250,'http://blog.midonet.org/250-revision-v1/',0,'revision','',0),(257,13,'2015-03-05 19:24:30','2015-03-05 19:24:30','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\n\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\n\n[caption id=\"attachment_264\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"wp-image-264 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-1024x768.jpg\" alt=\"FullSizeRender\" width=\"1024\" height=\"768\" /></a> Kato-san speaking at the MidoNet event in Tokyo[/caption]\n\n&nbsp;\n\n&nbsp;\n\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\n\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\n\n[caption id=\"attachment_265\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"wp-image-265 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-1024x768.jpg\" alt=\"FullSizeRender-2\" width=\"1024\" height=\"768\" /></a> User panel at the MidoNet Tokyo event[/caption]\n\n&nbsp;\n\n&nbsp;\n\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone wp-image-266 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-1024x768.jpg\" alt=\"FullSizeRender-4\" width=\"1024\" height=\"768\" /></a>\n\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\n\n&nbsp;\n\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>Drawing out the comments what specifically attendee enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture</li>\n    <li>Future roadmap</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history</li>\n    <li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\n    <li>LB demo</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Hands-on</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases, large scale user story</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n</ul>\n</li>\n</ul>\n\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','publish','open','open','','more-than-100-people-joined-midonet-community-event-in-tokyo','','','2015-03-05 19:24:30','2015-03-05 19:24:30','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\r\n\r\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\r\n\r\n[caption id=\"attachment_264\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"wp-image-264 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-1024x768.jpg\" alt=\"FullSizeRender\" width=\"1024\" height=\"768\" /></a> Kato-san speaking at the MidoNet event in Tokyo[/caption]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\r\n\r\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\r\n\r\n[caption id=\"attachment_265\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"wp-image-265 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-1024x768.jpg\" alt=\"FullSizeRender-2\" width=\"1024\" height=\"768\" /></a> User panel at the MidoNet Tokyo event[/caption]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone wp-image-266 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-1024x768.jpg\" alt=\"FullSizeRender-4\" width=\"1024\" height=\"768\" /></a>\r\n\r\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\r\n\r\n&nbsp;\r\n\r\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\r\n<ul>\r\n	<li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\r\n	<li>Drawing out the comments what specifically attendee enjoyed:\r\n<ul>\r\n	<li>Detailed explanation about MidoNet distributed architecture</li>\r\n	<li>Future roadmap</li>\r\n	<li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\r\n	<li>Why open-sourced MidoNet</li>\r\n	<li>MidoNet history</li>\r\n	<li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\r\n	<li>LB demo</li>\r\n</ul>\r\n</li>\r\n	<li>Request for the next event\r\n<ul>\r\n	<li>Hands-on</li>\r\n	<li>Roadmap</li>\r\n	<li>Updates about MidoNet ecosystem, features</li>\r\n	<li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\r\n	<li>Continuously updating the MidoNet use cases, large scale user story</li>\r\n	<li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',0,'http://blog.midonet.org/?p=257',0,'post','',0),(258,13,'2015-03-04 14:20:12','2015-03-04 14:20:12','After 3 months since Midokura open-sourced MidoNet, we had a MidoNet community even in Tokyo on February 16th.  The event was two-folded : (1) User event and (2) Developer event.  The event was very successful with 150 people registered and 100+ showed up, great discussion and of course, sushi &amp; beer.\n\n&nbsp;\n\nBased on the after survey we had positive feedback and constructive suggestion about the next community event.\n\n&nbsp;\n\n2nd community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-revision-v1','','','2015-03-04 14:20:12','2015-03-04 14:20:12','After 3 months since Midokura open-sourced MidoNet, we had a MidoNet community even in Tokyo on February 16th.  The event was two-folded : (1) User event and (2) Developer event.  The event was very successful with 150 people registered and 100+ showed up, great discussion and of course, sushi &amp; beer.\r\n\r\n&nbsp;\r\n\r\nBased on the after survey we had positive feedback and constructive suggestion about the next community event.\r\n\r\n&nbsp;\r\n\r\n2nd community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',257,'http://blog.midonet.org/257-revision-v1/',0,'revision','',0),(259,13,'2015-03-04 14:31:17','2015-03-04 14:31:17','After 3 months since Midokura open-sourced MidoNet, we had a MidoNet community even in Tokyo on February 16th.  The event was two-folded : (1) User event and (2) Developer event.  The event was very successful with 150 people registered and 100+ showed up, great discussion and of course, sushi &amp; beer.\n\nUser event opened with\n\n<ul>\n    <li>TK gave the opening speech and MidoNet Deepdive from Etsuji Nakai, Red Hat followed talked about MidoNet architecture in detail, MidoNet user panel discussion with Fujitsu, KVH and Samir (MidoCloud), Nobuyuki Tamaoki of VTJ presented how to install MidoNet with Orizuru.  The event was wrapped up by Dan’s speech about Future MidoNet</li>\n</ul>\n\n<ul>\n    <li>Started with Taku’s presentation about MidoNet, MidoNet deepdive.  After opening beer and Sushi again, we welcomed Lightning talk by Kobayashi of ADOC about BGP setting of MidoNet and then moved to LB demo by Ryu and Joe.</li>\n    <li>We had great audience including a few Neutron active contributors.  So, we got many high-level questions such as:</li>\n</ul>\n\nDuring the developer\'s event, there was active Q&amp;A\n\n<ul>\n<ul>\n    <li>What were the LB methods supported?</li>\n    <li>How Midolman behaves when it receives packet during live migration.  Does it have \"三角転送 (direct EN translation is triangle forwarding?)\" process?</li>\n</ul>\n</ul>\n\nBased on the after survey we had positive feedback and constructive suggestion about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>\n<ul>\n<ul>\n    <li>Drawing out the comments what specifically enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture by Nakai</li>\n    <li>Future road map by DMD</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion</li>\n    <li>Comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history by TK</li>\n    <li>Enjoyed panel.  The discussion was based on the real experience and very open.</li>\n    <li>\n<ul>\n<ul>\n<ul>\n    <li>Enjoyed the talk by real developers at MidoNet Deepdive</li>\n    <li>Periodically continue having Hands-on seminar</li>\n    <li>Enjoyed the direct conversation and Q&amp;A with MidoNet core developers.  Discussion on the future development, distributed LB, and the details on how midlman works.</li>\n    <li>Enjoyed the LB demo</li>\n    <li>Liked the presentation about how MidoNet behaves in detail</li>\n    <li>MidoNet deepdive after upgrading to 2.0</li>\n</ul>\n</ul>\n</ul>\n</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Performance test result of MidoNet</li>\n    <li>Large scale user story of MidoNet</li>\n    <li>LB, FW and MidoNet Agent on SW</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detail numbers showing the stability of running MidoNet.  Such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases</li>\n    <li>\n<ul>\n<ul>\n<ul>\n    <li>Hands-on</li>\n    <li>Learn how to use MidoNet.  Following the guideline and installed MidoNet but do not know what to do next.  (&lt;&lt; We ended up doing not doing hands-on, so this comment)</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n    <li>Would like to do hands-on with more than 2HV.</li>\n</ul>\n</ul>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</ul>\n</li>\n</ul>\n\n&nbsp;\n\n&nbsp;\n\n2nd community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-revision-v1','','','2015-03-04 14:31:17','2015-03-04 14:31:17','After 3 months since Midokura open-sourced MidoNet, we had a MidoNet community even in Tokyo on February 16th.  The event was two-folded : (1) User event and (2) Developer event.  The event was very successful with 150 people registered and 100+ showed up, great discussion and of course, sushi &amp; beer.\r\n\r\nUser event opened with\r\n<ul>\r\n	<li>TK gave the opening speech and MidoNet Deepdive from Etsuji Nakai, Red Hat followed talked about MidoNet architecture in detail, MidoNet user panel discussion with Fujitsu, KVH and Samir (MidoCloud), Nobuyuki Tamaoki of VTJ presented how to install MidoNet with Orizuru.  The event was wrapped up by Dan’s speech about Future MidoNet</li>\r\n</ul>\r\n<ul>\r\n	<li>Started with Taku’s presentation about MidoNet, MidoNet deepdive.  After opening beer and Sushi again, we welcomed Lightning talk by Kobayashi of ADOC about BGP setting of MidoNet and then moved to LB demo by Ryu and Joe.</li>\r\n	<li>We had great audience including a few Neutron active contributors.  So, we got many high-level questions such as:</li>\r\n</ul>\r\nDuring the developer\'s event, there was active Q&amp;A\r\n<ul>\r\n<ul>\r\n	<li>What were the LB methods supported?</li>\r\n	<li>How Midolman behaves when it receives packet during live migration.  Does it have \"三角転送 (direct EN translation is triangle forwarding?)\" process?</li>\r\n</ul>\r\n</ul>\r\nBased on the after survey we had positive feedback and constructive suggestion about the next community event.\r\n<ul>\r\n	<li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\r\n	<li>\r\n<ul>\r\n<ul>\r\n	<li>Drawing out the comments what specifically enjoyed:\r\n<ul>\r\n	<li>Detailed explanation about MidoNet distributed architecture by Nakai</li>\r\n	<li>Future road map by DMD</li>\r\n	<li>Many honest feedback about using MidoNet by users at panel discussion</li>\r\n	<li>Comparison with OvS, NSX</li>\r\n	<li>Why open-sourced MidoNet</li>\r\n	<li>MidoNet history by TK</li>\r\n	<li>Enjoyed panel.  The discussion was based on the real experience and very open.</li>\r\n	<li>\r\n<ul>\r\n<ul>\r\n<ul>\r\n	<li>Enjoyed the talk by real developers at MidoNet Deepdive</li>\r\n	<li>Periodically continue having Hands-on seminar</li>\r\n	<li>Enjoyed the direct conversation and Q&amp;A with MidoNet core developers.  Discussion on the future development, distributed LB, and the details on how midlman works.</li>\r\n	<li>Enjoyed the LB demo</li>\r\n	<li>Liked the presentation about how MidoNet behaves in detail</li>\r\n	<li>MidoNet deepdive after upgrading to 2.0</li>\r\n</ul>\r\n</ul>\r\n</ul>\r\n</li>\r\n</ul>\r\n</li>\r\n	<li>Request for the next event\r\n<ul>\r\n	<li>Performance test result of MidoNet</li>\r\n	<li>Large scale user story of MidoNet</li>\r\n	<li>LB, FW and MidoNet Agent on SW</li>\r\n	<li>Roadmap</li>\r\n	<li>Updates about MidoNet ecosystem, features</li>\r\n	<li>Detail numbers showing the stability of running MidoNet.  Such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\r\n	<li>Continuously updating the MidoNet use cases</li>\r\n	<li>\r\n<ul>\r\n<ul>\r\n<ul>\r\n	<li>Hands-on</li>\r\n	<li>Learn how to use MidoNet.  Following the guideline and installed MidoNet but do not know what to do next.  (&lt;&lt; We ended up doing not doing hands-on, so this comment)</li>\r\n	<li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\r\n	<li>Would like to do hands-on with more than 2HV.</li>\r\n</ul>\r\n</ul>\r\n</ul>\r\n</li>\r\n</ul>\r\n</li>\r\n</ul>\r\n</ul>\r\n</li>\r\n</ul>\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\n2nd community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',257,'http://blog.midonet.org/257-revision-v1/',0,'revision','',0),(260,13,'2015-03-04 15:11:43','2015-03-04 15:11:43','After 3 months since Midokura open-sourced MidoNet, we had a MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for develpers.  The event was very successful with 100+ people, great discussion and of course, sushi &amp; beer.\n\nUser event opened with the presentation by Tatsuya Kato of Midokura introducing MidoNet history.\n\n&nbsp;\n\n++Picture++\n\n&nbsp;\n\nEtsuji Nakai of Red Hat Japan followed and talked about MidoNet architecture in detail.  We moved on to MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud) and then Nobuyuki Tamaoki of Virtual Tech Japan presented how to install MidoNet with Orizuru, an installation tool of MidoNet.  The event was wrapped up by Dan’s speech about Future MidoNet\n\nAudience enjoyed especially the user panel where the panelist share their experience with MidoNet and also how they compared MidoNet with other products such as VMWare NSX.\n\n++Picture\n\n&nbsp;\n\nDeveloper event started with the presentation about MidoNet and then MidoNet deepdive by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\n\n++Picture++\n\nWe had great audience including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions raised such as \"How Midolman behaves when it receives packet during live migration?\"\n\n&nbsp;\n\nAccording to the after survey, we had positive feedback and constructive suggestion about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>Drawing out the comments what specifically attendee enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture</li>\n    <li>Future road map</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history</li>\n    <li>Enjoyed the talk by real developers at MidoNet Deepdive</li>\n    <li>LB demo</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Hands-on</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detail numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases, large scale user story</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n</ul>\n</li>\n</ul>\n\n2nd community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-revision-v1','','','2015-03-04 15:11:43','2015-03-04 15:11:43','After 3 months since Midokura open-sourced MidoNet, we had a MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for develpers.  The event was very successful with 100+ people, great discussion and of course, sushi &amp; beer.\r\n\r\nUser event opened with the presentation by Tatsuya Kato of Midokura introducing MidoNet history.\r\n\r\n&nbsp;\r\n\r\n++Picture++\r\n\r\n&nbsp;\r\n\r\nEtsuji Nakai of Red Hat Japan followed and talked about MidoNet architecture in detail.  We moved on to MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud) and then Nobuyuki Tamaoki of Virtual Tech Japan presented how to install MidoNet with Orizuru, an installation tool of MidoNet.  The event was wrapped up by Dan’s speech about Future MidoNet\r\n\r\nAudience enjoyed especially the user panel where the panelist share their experience with MidoNet and also how they compared MidoNet with other products such as VMWare NSX.\r\n\r\n++Picture\r\n\r\n&nbsp;\r\n\r\nDeveloper event started with the presentation about MidoNet and then MidoNet deepdive by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\r\n\r\n++Picture++\r\n\r\nWe had great audience including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions raised such as \"How Midolman behaves when it receives packet during live migration?\"\r\n\r\n&nbsp;\r\n\r\nAccording to the after survey, we had positive feedback and constructive suggestion about the next community event.\r\n<ul>\r\n	<li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\r\n	<li>Drawing out the comments what specifically attendee enjoyed:\r\n<ul>\r\n	<li>Detailed explanation about MidoNet distributed architecture</li>\r\n	<li>Future road map</li>\r\n	<li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\r\n	<li>Why open-sourced MidoNet</li>\r\n	<li>MidoNet history</li>\r\n	<li>Enjoyed the talk by real developers at MidoNet Deepdive</li>\r\n	<li>LB demo</li>\r\n</ul>\r\n</li>\r\n	<li>Request for the next event\r\n<ul>\r\n	<li>Hands-on</li>\r\n	<li>Roadmap</li>\r\n	<li>Updates about MidoNet ecosystem, features</li>\r\n	<li>Detail numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\r\n	<li>Continuously updating the MidoNet use cases, large scale user story</li>\r\n	<li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\n2nd community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',257,'http://blog.midonet.org/257-revision-v1/',0,'revision','',0),(262,1,'2015-03-05 19:23:53','2015-03-05 19:23:53','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\n\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\n\n[caption id=\"attachment_264\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"wp-image-264 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-1024x768.jpg\" alt=\"FullSizeRender\" width=\"1024\" height=\"768\" /></a> Kato-san speaking at the MidoNet event in Tokyo[/caption]\n\n&nbsp;\n\n&nbsp;\n\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\n\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\n\n[caption id=\"attachment_265\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"wp-image-265 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-1024x768.jpg\" alt=\"FullSizeRender-2\" width=\"1024\" height=\"768\" /></a> User panel at the MidoNet Tokyo event[/caption]\n\n&nbsp;\n\n&nbsp;\n\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone size-medium wp-image-266\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-300x225.jpg\" alt=\"FullSizeRender-4\" width=\"300\" height=\"225\" /></a>\n\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\n\n&nbsp;\n\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>Drawing out the comments what specifically attendee enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture</li>\n    <li>Future roadmap</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history</li>\n    <li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\n    <li>LB demo</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Hands-on</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases, large scale user story</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n</ul>\n</li>\n</ul>\n\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-autosave-v1','','','2015-03-05 19:23:53','2015-03-05 19:23:53','',257,'http://blog.midonet.org/257-autosave-v1/',0,'revision','',0),(263,1,'2015-03-05 04:56:55','2015-03-05 04:56:55','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\n\nThe user event opened with the presentation by Tatsuya Kato of Midokura, introducing MidoNet\'s history.\n\n&nbsp;\n\n++Picture++\n\n&nbsp;\n\nEtsuji Nakai of Red Hat Japan followed and talked about MidoNet\'s architecture in detail.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented how to install MidoNet with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about the future of MidoNet.\n\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\n\n++Picture\n\n&nbsp;\n\nDeveloper event started with the presentation about MidoNet and then MidoNet deepdive by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\n\n++Picture++\n\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\n\n&nbsp;\n\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>Drawing out the comments what specifically attendee enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture</li>\n    <li>Future roadmap</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history</li>\n    <li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\n    <li>LB demo</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Hands-on</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases, large scale user story</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n</ul>\n</li>\n</ul>\n\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-revision-v1','','','2015-03-05 04:56:55','2015-03-05 04:56:55','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\r\n\r\nThe user event opened with the presentation by Tatsuya Kato of Midokura, introducing MidoNet\'s history.\r\n\r\n&nbsp;\r\n\r\n++Picture++\r\n\r\n&nbsp;\r\n\r\nEtsuji Nakai of Red Hat Japan followed and talked about MidoNet\'s architecture in detail.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented how to install MidoNet with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about the future of MidoNet.\r\n\r\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\r\n\r\n++Picture\r\n\r\n&nbsp;\r\n\r\nDeveloper event started with the presentation about MidoNet and then MidoNet deepdive by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\r\n\r\n++Picture++\r\n\r\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\r\n\r\n&nbsp;\r\n\r\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\r\n<ul>\r\n	<li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\r\n	<li>Drawing out the comments what specifically attendee enjoyed:\r\n<ul>\r\n	<li>Detailed explanation about MidoNet distributed architecture</li>\r\n	<li>Future roadmap</li>\r\n	<li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\r\n	<li>Why open-sourced MidoNet</li>\r\n	<li>MidoNet history</li>\r\n	<li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\r\n	<li>LB demo</li>\r\n</ul>\r\n</li>\r\n	<li>Request for the next event\r\n<ul>\r\n	<li>Hands-on</li>\r\n	<li>Roadmap</li>\r\n	<li>Updates about MidoNet ecosystem, features</li>\r\n	<li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\r\n	<li>Continuously updating the MidoNet use cases, large scale user story</li>\r\n	<li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',257,'http://blog.midonet.org/257-revision-v1/',0,'revision','',0),(264,13,'2015-03-05 06:00:51','2015-03-05 06:00:51','','FullSizeRender','','inherit','open','open','','fullsizerender','','','2015-03-05 06:00:51','2015-03-05 06:00:51','',257,'http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg',0,'attachment','image/jpeg',0),(265,13,'2015-03-05 06:01:22','2015-03-05 06:01:22','','FullSizeRender-2','','inherit','open','open','','fullsizerender-2','','','2015-03-05 06:01:22','2015-03-05 06:01:22','',257,'http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg',0,'attachment','image/jpeg',0),(266,13,'2015-03-05 06:02:05','2015-03-05 06:02:05','','FullSizeRender-4','','inherit','open','open','','fullsizerender-4','','','2015-03-05 06:02:05','2015-03-05 06:02:05','',257,'http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg',0,'attachment','image/jpeg',0),(267,13,'2015-03-05 06:20:44','2015-03-05 06:20:44','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\n\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"alignnone size-medium wp-image-264\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-300x225.jpg\" alt=\"FullSizeRender\" width=\"300\" height=\"225\" /></a>\n\n&nbsp;\n\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\n\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"alignnone size-medium wp-image-265\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-300x225.jpg\" alt=\"FullSizeRender-2\" width=\"300\" height=\"225\" /></a>\n\n&nbsp;\n\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone size-medium wp-image-266\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-300x225.jpg\" alt=\"FullSizeRender-4\" width=\"300\" height=\"225\" /></a>\n\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\n\n&nbsp;\n\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>Drawing out the comments what specifically attendee enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture</li>\n    <li>Future roadmap</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history</li>\n    <li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\n    <li>LB demo</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Hands-on</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases, large scale user story</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n</ul>\n</li>\n</ul>\n\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-revision-v1','','','2015-03-05 06:20:44','2015-03-05 06:20:44','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\r\n\r\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"alignnone size-medium wp-image-264\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-300x225.jpg\" alt=\"FullSizeRender\" width=\"300\" height=\"225\" /></a>\r\n\r\n&nbsp;\r\n\r\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\r\n\r\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"alignnone size-medium wp-image-265\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-300x225.jpg\" alt=\"FullSizeRender-2\" width=\"300\" height=\"225\" /></a>\r\n\r\n&nbsp;\r\n\r\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone size-medium wp-image-266\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-300x225.jpg\" alt=\"FullSizeRender-4\" width=\"300\" height=\"225\" /></a>\r\n\r\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\r\n\r\n&nbsp;\r\n\r\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\r\n<ul>\r\n	<li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\r\n	<li>Drawing out the comments what specifically attendee enjoyed:\r\n<ul>\r\n	<li>Detailed explanation about MidoNet distributed architecture</li>\r\n	<li>Future roadmap</li>\r\n	<li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\r\n	<li>Why open-sourced MidoNet</li>\r\n	<li>MidoNet history</li>\r\n	<li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\r\n	<li>LB demo</li>\r\n</ul>\r\n</li>\r\n	<li>Request for the next event\r\n<ul>\r\n	<li>Hands-on</li>\r\n	<li>Roadmap</li>\r\n	<li>Updates about MidoNet ecosystem, features</li>\r\n	<li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\r\n	<li>Continuously updating the MidoNet use cases, large scale user story</li>\r\n	<li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',257,'http://blog.midonet.org/257-revision-v1/',0,'revision','',0),(268,1,'2015-03-05 19:24:30','2015-03-05 19:24:30','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\n\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\n\n[caption id=\"attachment_264\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"wp-image-264 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-1024x768.jpg\" alt=\"FullSizeRender\" width=\"1024\" height=\"768\" /></a> Kato-san speaking at the MidoNet event in Tokyo[/caption]\n\n&nbsp;\n\n&nbsp;\n\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\n\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\n\n[caption id=\"attachment_265\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"wp-image-265 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-1024x768.jpg\" alt=\"FullSizeRender-2\" width=\"1024\" height=\"768\" /></a> User panel at the MidoNet Tokyo event[/caption]\n\n&nbsp;\n\n&nbsp;\n\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone wp-image-266 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-1024x768.jpg\" alt=\"FullSizeRender-4\" width=\"1024\" height=\"768\" /></a>\n\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\n\n&nbsp;\n\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\n\n<ul>\n    <li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\n    <li>Drawing out the comments what specifically attendee enjoyed:\n<ul>\n    <li>Detailed explanation about MidoNet distributed architecture</li>\n    <li>Future roadmap</li>\n    <li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\n    <li>Why open-sourced MidoNet</li>\n    <li>MidoNet history</li>\n    <li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\n    <li>LB demo</li>\n</ul>\n</li>\n    <li>Request for the next event\n<ul>\n    <li>Hands-on</li>\n    <li>Roadmap</li>\n    <li>Updates about MidoNet ecosystem, features</li>\n    <li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\n    <li>Continuously updating the MidoNet use cases, large scale user story</li>\n    <li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\n</ul>\n</li>\n</ul>\n\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\n\n&nbsp;','More than 100 people joined MidoNet Community Event in Tokyo','','inherit','open','open','','257-revision-v1','','','2015-03-05 19:24:30','2015-03-05 19:24:30','Three months after Midokura open-sourced MidoNet, we had our first MidoNet community event in Tokyo on February 16th.  The first part of the event was for MidoNet users and the second part was for developers.  The event was a huge success with 100+ people, great discussions, and of course, sushi &amp; beer.\r\n\r\nThe user event opened with <a href=\"http://www.slideshare.net/midokura/tk-public-mn-community2015\">the presentation by Tatsuya Kato</a> of Midokura, introducing MidoNet\'s history.\r\n\r\n[caption id=\"attachment_264\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender.jpg\"><img class=\"wp-image-264 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-1024x768.jpg\" alt=\"FullSizeRender\" width=\"1024\" height=\"768\" /></a> Kato-san speaking at the MidoNet event in Tokyo[/caption]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\nEtsuji Nakai of Red Hat Japan followed and talked about <a href=\"http://www.slideshare.net/enakai/midonet-technology-internalsv10?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=4\">MidoNet\'s architecture in detail</a>.  We moved on to a MidoNet user panel discussion with Fujitsu, KVH and Midokura (MidoCloud).  Next up, Nobuyuki Tamaoki of Virtual Tech Japan presented <a href=\"http://www.slideshare.net/ntamaoki/midonet-orizuru20150211-44577981?qid=28ff9881-9ef1-480f-9d3b-9a37e7c47e86&amp;v=default&amp;b=&amp;from_search=3\">how to install MidoNet</a> with <a href=\"https://github.com/midonet/orizuru\">Orizuru</a>, an installation tool of MidoNet utilizing Docker containers.  The event was wrapped up by a speech from Dan Dumitriu about <a href=\"http://www.slideshare.net/midokura/midonet-future\">the future of MidoNet</a>.\r\n\r\nThe audience especially enjoyed the user panel, where panelists shared their experiences operating MidoNet, and also how MidoNet compares with other products, such as VMWare NSX.\r\n\r\n[caption id=\"attachment_265\" align=\"aligncenter\" width=\"1024\"]<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2.jpg\"><img class=\"wp-image-265 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-2-1024x768.jpg\" alt=\"FullSizeRender-2\" width=\"1024\" height=\"768\" /></a> User panel at the MidoNet Tokyo event[/caption]\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\nDeveloper event started with <a href=\"http://www.slideshare.net/takufukushima79/introduction-to-mido-net\">the introduction to MidoNet</a> and then <a href=\"http://www.slideshare.net/takufukushima79/mido-net-deep-dive\">MidoNet deepdive</a> by Taku Fukushima, a core developer of MidoNet from Midokura.  After opening beer and having Sushi, we welcomed Lightning talk about BGP setting of MidoNet by Keiichi Kobayashi of ADOC and finale was the  L4 Load Balancer demo by Ryu Ishimoto and Joe Mills by Midokura.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4.jpg\"><img class=\"alignnone wp-image-266 size-large\" src=\"http://blog.midonet.org/wp-content/uploads/2015/03/FullSizeRender-4-1024x768.jpg\" alt=\"FullSizeRender-4\" width=\"1024\" height=\"768\" /></a>\r\n\r\nWe had a great audience, including a few Neutron active contributors: Naturally we got into active Q&amp;A and many questions were raised, such as \"How Midolman behaves when it receives packet during live migration?\"\r\n\r\n&nbsp;\r\n\r\nAccording to the after survey, we had positive feedback and constructive suggestions about the next community event.\r\n<ul>\r\n	<li>90% of attendees answered that they were satisfied and enjoyed the event.</li>\r\n	<li>Drawing out the comments what specifically attendee enjoyed:\r\n<ul>\r\n	<li>Detailed explanation about MidoNet distributed architecture</li>\r\n	<li>Future roadmap</li>\r\n	<li>Many honest feedback about using MidoNet by users at panel discussion, such as comparison with OvS, NSX</li>\r\n	<li>Why open-sourced MidoNet</li>\r\n	<li>MidoNet history</li>\r\n	<li>Enjoyed the talk by real developers at the MidoNet Deep dive</li>\r\n	<li>LB demo</li>\r\n</ul>\r\n</li>\r\n	<li>Request for the next event\r\n<ul>\r\n	<li>Hands-on</li>\r\n	<li>Roadmap</li>\r\n	<li>Updates about MidoNet ecosystem, features</li>\r\n	<li>Detailed numbers showing the stability of running MidoNet, such as how much the environment can scale, # of switches, # of flows.  Trouble examples.</li>\r\n	<li>Continuously updating the MidoNet use cases, large scale user story</li>\r\n	<li>Like the LB demo, would like to see the MidoNet feature demo by developers with the explanation how and why the feature demo is great.</li>\r\n</ul>\r\n</li>\r\n</ul>\r\nThe second community event will coming sometime late April or early May.  If you are in Tokyo, stay tuned!\r\n\r\n&nbsp;',257,'http://blog.midonet.org/257-revision-v1/',0,'revision','',0),(270,2,'2015-03-25 06:30:21','2015-03-25 06:30:21','http://www.meetup.com/San-Francisco-MidoNet-Network-Virtualization-Meetup/events/221190784/','San Francisco MidoNet Meetup on March 31','','publish','open','open','','san-francisco-midonet-meetup-on-march-31','','','2015-03-25 06:31:43','2015-03-25 06:31:43','http://www.meetup.com/San-Francisco-MidoNet-Network-Virtualization-Meetup/events/221190784/',0,'http://blog.midonet.org/?p=270',0,'post','',0),(271,2,'2015-03-25 06:30:21','2015-03-25 06:30:21','http://www.meetup.com/San-Francisco-MidoNet-Network-Virtualization-Meetup/events/221190784/','San Francisco MidoNet Meetup on March 31','','inherit','open','open','','270-revision-v1','','','2015-03-25 06:30:21','2015-03-25 06:30:21','http://www.meetup.com/San-Francisco-MidoNet-Network-Virtualization-Meetup/events/221190784/',270,'http://blog.midonet.org/270-revision-v1/',0,'revision','',0),(272,5,'2015-04-06 00:05:56','2015-04-06 00:05:56','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</pre>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<pre class=\"prettyprint\"># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</pre>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<pre class=\"prettyprint\">mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\n</pre>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<pre class=\"prettyprint\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>','MidoNet\'s traffic meters and Prometheus TSDB','','publish','open','open','','midonets-traffic-meters-and-prometheus-tsdb','','','2015-04-09 16:08:48','2015-04-09 16:08:48','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</pre>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<pre class=\"prettyprint\"># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</pre>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<pre class=\"prettyprint\">mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\n</pre>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<pre class=\"prettyprint\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>',0,'http://blog.midonet.org/?p=272',0,'post','',1),(273,5,'2015-04-04 17:51:06','2015-04-04 17:51:06','Install Docker. Here\'s how you do it on Ubuntu\n\nhttps://docs.docker.com/installation/ubuntulinux/\n\nhttps://docs.docker.com/installation/\n\nFollow the instructions at http://prometheus.io/docs/introduction/install/\n\nhttps://github.com/prometheus/jmx_exporter\n\n<p class=\"p1\"><span class=\"s1\">docker run -d —name prom -p 9090:9090 -v ~/prometheus.conf:/prometheus.conf -v ~/prometheus.rules:/prometheus.rules prom/prometheus</span></p>\n\n&nbsp;\n\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\n\n&nbsp;','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-04 17:51:06','2015-04-04 17:51:06','Install Docker. Here\'s how you do it on Ubuntu\r\n\r\nhttps://docs.docker.com/installation/ubuntulinux/\r\n\r\nhttps://docs.docker.com/installation/\r\n\r\nFollow the instructions at http://prometheus.io/docs/introduction/install/\r\n\r\nhttps://github.com/prometheus/jmx_exporter\r\n<p class=\"p1\"><span class=\"s1\">docker run -d —name prom -p 9090:9090 -v ~/prometheus.conf:/prometheus.conf -v ~/prometheus.rules:/prometheus.rules prom/prometheus</span></p>\r\n&nbsp;\r\n\r\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\r\n\r\n&nbsp;',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(274,5,'2015-04-04 17:58:03','2015-04-04 17:58:03','Install Docker. Here\'s how you do it on Ubuntu\n\nhttps://docs.docker.com/installation/ubuntulinux/\n\nhttps://docs.docker.com/installation/\n\nFollow the instructions at http://prometheus.io/docs/introduction/install/\n\nhttps://github.com/prometheus/jmx_exporter\n\n<p class=\"p1\"><span class=\"s1\">docker run -d —name prom -p 9090:9090 -v ~/prometheus.conf:/prometheus.conf -v ~/prometheus.rules:/prometheus.rules prom/prometheus</span></p>\n\n&nbsp;\n\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\n\nHere\'s a tar file of the exporter tailored for MN\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\n\n&nbsp;','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-04 17:58:03','2015-04-04 17:58:03','Install Docker. Here\'s how you do it on Ubuntu\r\n\r\nhttps://docs.docker.com/installation/ubuntulinux/\r\n\r\nhttps://docs.docker.com/installation/\r\n\r\nFollow the instructions at http://prometheus.io/docs/introduction/install/\r\n\r\nhttps://github.com/prometheus/jmx_exporter\r\n<p class=\"p1\"><span class=\"s1\">docker run -d —name prom -p 9090:9090 -v ~/prometheus.conf:/prometheus.conf -v ~/prometheus.rules:/prometheus.rules prom/prometheus</span></p>\r\n&nbsp;\r\n\r\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\r\n\r\nHere\'s a tar file of the exporter tailored for MN\r\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\r\n\r\n&nbsp;',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(275,5,'2015-04-04 18:22:47','2015-04-04 18:22:47','Install Docker. Here\'s how you do it on Ubuntu\n\nhttps://docs.docker.com/installation/ubuntulinux/\n\nhttps://docs.docker.com/installation/\n\nFollow the instructions at http://prometheus.io/docs/introduction/install/\n\nhttps://github.com/prometheus/jmx_exporter\n\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\n\nHere\'s a tar file of the exporter tailored for MN\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\n\nDownload the tar file.\ntar -zxvf prom_mn_jmx_scraper.tar.gz\ncd prom_mn_jmx_scraper\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\n    \"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\nPoint a browser to http://<your ip>:7201/metrics\nYou should see something like:\n\n<h1>HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes>&lt;>Value)</h1>\n\n<h1>TYPE mndev_bytes counter</h1>\n\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\n...\n...\n\n<h1>HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes>&lt;>Value)</h1>\n\n<h1>TYPE mnport_rx_bytes counter</h1>\n\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\n\n<h1>HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts>&lt;>Value)</h1>\n\n<h1>TYPE mndev_pkts counter</h1>\n\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0\n\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\n\ndocker run -d —name prom -p 9090:9090 -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.conf:/prometheus.conf -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.rules:/prometheus.rules prom/prometheus\n\nSome things to notice about prometheus.conf:\njob: {\n  # The job name is added as a label <code>job={job-name}</code> to any time series scraped from this job.\n  name: \"jmx_scraper\"\n  # Override the global default and scrape targets from this job every 5 seconds.\n  scrape_interval: \"15s\"\n\n# Let\'s define a group of static targets to scrape for this job. In this\n  # case, only one.\n  target_group: {\n    # These endpoints are scraped via HTTP.\n    target: \"http://172.17.42.1:7201/metrics\"\n  }\n}\n\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\n\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\n\ndocker inspect sleepy_wilson | less\n\"NetworkMode\": \"bridge\"\n\n&nbsp;','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-04 18:22:47','2015-04-04 18:22:47','Install Docker. Here\'s how you do it on Ubuntu\r\n\r\nhttps://docs.docker.com/installation/ubuntulinux/\r\n\r\nhttps://docs.docker.com/installation/\r\n\r\nFollow the instructions at http://prometheus.io/docs/introduction/install/\r\n\r\nhttps://github.com/prometheus/jmx_exporter\r\n\r\n\r\n\r\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\r\n\r\nHere\'s a tar file of the exporter tailored for MN\r\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\r\n\r\nDownload the tar file.\r\ntar -zxvf prom_mn_jmx_scraper.tar.gz\r\ncd prom_mn_jmx_scraper\r\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\r\n    \"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\r\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\r\nPoint a browser to http://<your ip>:7201/metrics\r\nYou should see something like:\r\n\r\n# HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=bytes><>Value)\r\n# TYPE mndev_bytes counter\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\r\n...\r\n...\r\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5<hostPort=119.15.127.100:7200, type=bytes><>Value)\r\n# TYPE mnport_rx_bytes counter\r\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\r\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\r\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=pkts><>Value)\r\n# TYPE mndev_pkts counter\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0\r\n\r\n\r\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\r\n\r\ndocker run -d —name prom -p 9090:9090 -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.conf:/prometheus.conf -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.rules:/prometheus.rules prom/prometheus\r\n\r\nSome things to notice about prometheus.conf:\r\njob: {\r\n  # The job name is added as a label `job={job-name}` to any time series scraped from this job.\r\n  name: \"jmx_scraper\"\r\n  # Override the global default and scrape targets from this job every 5 seconds.\r\n  scrape_interval: \"15s\"\r\n\r\n  # Let\'s define a group of static targets to scrape for this job. In this\r\n  # case, only one.\r\n  target_group: {\r\n    # These endpoints are scraped via HTTP.\r\n    target: \"http://172.17.42.1:7201/metrics\"\r\n  }\r\n}\r\n\r\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\r\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\r\n\r\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\r\n\r\ndocker inspect sleepy_wilson | less\r\n\"NetworkMode\": \"bridge\"\r\n\r\n\r\n\r\n&nbsp;',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(276,5,'2015-04-05 07:21:03','2015-04-05 07:21:03','Since v1.7 MidoNet has support for meters. There four kinds of meters:\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote>\n# mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16\n</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote>\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\n     packets        bytes\n        6390       268380\n          10          420\n          31         1126\n           3          126\n</blockquote>\n\nInstall Docker. Here\'s how you do it on Ubuntu\n\nhttps://docs.docker.com/installation/ubuntulinux/\n\nhttps://docs.docker.com/installation/\n\nFollow the instructions at http://prometheus.io/docs/introduction/install/\n\nhttps://github.com/prometheus/jmx_exporter\n\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\n\nHere\'s a tar file of the exporter tailored for MN\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\n\nDownload the tar file.\ntar -zxvf prom_mn_jmx_scraper.tar.gz\ncd prom_mn_jmx_scraper\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\n    \"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\n\n<pre><code>\"rules\": [\n  {\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n   \"name\": \"mndev_$3\",\n   \"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\n   \"type\": \"COUNTER\"},\n  {\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n   \"name\": \"mnport_$1_$4\",\n   \"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\n   \"type\": \"COUNTER\"},\n  {\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n   \"name\": \"mntun_$4\",\n   \"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\n   \"type\": \"COUNTER\"},\n]\n</code></pre>\n\nPoint a browser to http://<your ip>:7201/metrics\nYou should see something like:\n\n<h1>HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes>&lt;>Value)</h1>\n\n<h1>TYPE mndev_bytes counter</h1>\n\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\n...\n...\n\n<h1>HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes>&lt;>Value)</h1>\n\n<h1>TYPE mnport_rx_bytes counter</h1>\n\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\n\n<h1>HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts>&lt;>Value)</h1>\n\n<h1>TYPE mndev_pkts counter</h1>\n\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0\n\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\n\ndocker run -d —name prom -p 9090:9090 -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.conf:/prometheus.conf -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.rules:/prometheus.rules prom/prometheus\n\nSome things to notice about prometheus.conf:\njob: {\n  # The job name is added as a label <code>job={job-name}</code> to any time series scraped from this job.\n  name: \"jmx_scraper\"\n  # Override the global default and scrape targets from this job every 5 seconds.\n  scrape_interval: \"15s\"\n\n# Let\'s define a group of static targets to scrape for this job. In this\n  # case, only one.\n  target_group: {\n    # These endpoints are scraped via HTTP.\n    target: \"http://172.17.42.1:7201/metrics\"\n  }\n}\n\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\n\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\n\ndocker inspect sleepy_wilson | less\n\"NetworkMode\": \"bridge\"\n\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)\n\n&nbsp;','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-05 07:21:03','2015-04-05 07:21:03','Since v1.7 MidoNet has support for meters. There four kinds of meters:\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n\r\n<blockquote>\r\n# mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16\r\n</blockquote>\r\n\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n\r\n<blockquote>\r\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\n     packets        bytes\r\n        6390       268380\r\n          10          420\r\n          31         1126\r\n           3          126\r\n</blockquote>\r\n\r\n\r\n\r\n\r\n\r\nInstall Docker. Here\'s how you do it on Ubuntu\r\n\r\nhttps://docs.docker.com/installation/ubuntulinux/\r\n\r\nhttps://docs.docker.com/installation/\r\n\r\nFollow the instructions at http://prometheus.io/docs/introduction/install/\r\n\r\nhttps://github.com/prometheus/jmx_exporter\r\n\r\n\r\n\r\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\r\n\r\nHere\'s a tar file of the exporter tailored for MN\r\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\r\n\r\nDownload the tar file.\r\ntar -zxvf prom_mn_jmx_scraper.tar.gz\r\ncd prom_mn_jmx_scraper\r\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\r\n    \"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\r\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\r\n\r\n    \"rules\": [\r\n      {\"pattern\": \"^meters:device:([0-9a-f-]+)<hostPort=(.+), type=(\\w+)><>Value:\",\r\n       \"name\": \"mndev_$3\",\r\n       \"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\r\n       \"type\": \"COUNTER\"},\r\n      {\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)<hostPort=(.+), type=(\\w+)><>Value:\",\r\n       \"name\": \"mnport_$1_$4\",\r\n       \"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\r\n       \"type\": \"COUNTER\"},\r\n      {\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)<hostPort=(.+), type=(\\w+)><>Value:\",\r\n       \"name\": \"mntun_$4\",\r\n       \"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\r\n       \"type\": \"COUNTER\"},\r\n    ]\r\n\r\n\r\nPoint a browser to http://<your ip>:7201/metrics\r\nYou should see something like:\r\n\r\n# HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=bytes><>Value)\r\n# TYPE mndev_bytes counter\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\r\n...\r\n...\r\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5<hostPort=119.15.127.100:7200, type=bytes><>Value)\r\n# TYPE mnport_rx_bytes counter\r\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\r\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\r\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=pkts><>Value)\r\n# TYPE mndev_pkts counter\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0\r\n\r\n\r\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\r\n\r\ndocker run -d —name prom -p 9090:9090 -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.conf:/prometheus.conf -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.rules:/prometheus.rules prom/prometheus\r\n\r\nSome things to notice about prometheus.conf:\r\njob: {\r\n  # The job name is added as a label `job={job-name}` to any time series scraped from this job.\r\n  name: \"jmx_scraper\"\r\n  # Override the global default and scrape targets from this job every 5 seconds.\r\n  scrape_interval: \"15s\"\r\n\r\n  # Let\'s define a group of static targets to scrape for this job. In this\r\n  # case, only one.\r\n  target_group: {\r\n    # These endpoints are scraped via HTTP.\r\n    target: \"http://172.17.42.1:7201/metrics\"\r\n  }\r\n}\r\n\r\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\r\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\r\n\r\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\r\n\r\ndocker inspect sleepy_wilson | less\r\n\"NetworkMode\": \"bridge\"\r\n\r\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)\r\n\r\n\r\n\r\n&nbsp;',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(277,5,'2015-04-05 07:24:20','2015-04-05 07:24:20','Since v1.7 MidoNet has support for meters. There four kinds of meters:\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote>\n# mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16\n</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote>\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\n     packets        bytes\n        6390       268380\n          10          420\n          31         1126\n           3          126\n</blockquote>\n\nInstall Docker. Here\'s how you do it on Ubuntu\n\nhttps://docs.docker.com/installation/ubuntulinux/\n\nhttps://docs.docker.com/installation/\n\nFollow the instructions at http://prometheus.io/docs/introduction/install/\n\nhttps://github.com/prometheus/jmx_exporter\n\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\n\nHere\'s a tar file of the exporter tailored for MN\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\n\nDownload the tar file.\ntar -zxvf prom_mn_jmx_scraper.tar.gz\ncd prom_mn_jmx_scraper\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\n    \"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\n\n<pre><code>\"rules\": [\n  {\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n   \"name\": \"mndev_$3\",\n   \"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\n   \"type\": \"COUNTER\"},\n  {\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n   \"name\": \"mnport_$1_$4\",\n   \"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\n   \"type\": \"COUNTER\"},\n  {\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n   \"name\": \"mntun_$4\",\n   \"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\n   \"type\": \"COUNTER\"},\n]\n</code></pre>\n\nPoint a browser to http://<your ip>:7201/metrics\nYou should see something like:\n\n<blockquote>\n# HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=bytes><>Value)\n# TYPE mndev_bytes counter\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\n...\n...\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5<hostPort=119.15.127.100:7200, type=bytes><>Value)\n# TYPE mnport_rx_bytes counter\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=pkts><>Value)\n# TYPE mndev_pkts counter\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0\n</blockquote>\n\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\n\ndocker run -d —name prom -p 9090:9090 -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.conf:/prometheus.conf -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.rules:/prometheus.rules prom/prometheus\n\nSome things to notice about prometheus.conf:\njob: {\n  # The job name is added as a label <code>job={job-name}</code> to any time series scraped from this job.\n  name: \"jmx_scraper\"\n  # Override the global default and scrape targets from this job every 5 seconds.\n  scrape_interval: \"15s\"\n\n# Let\'s define a group of static targets to scrape for this job. In this\n  # case, only one.\n  target_group: {\n    # These endpoints are scraped via HTTP.\n    target: \"http://172.17.42.1:7201/metrics\"\n  }\n}\n\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\n\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\n\ndocker inspect sleepy_wilson | less\n\"NetworkMode\": \"bridge\"\n\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)\n\n&nbsp;','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-05 07:24:20','2015-04-05 07:24:20','Since v1.7 MidoNet has support for meters. There four kinds of meters:\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n\r\n<blockquote>\r\n# mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16\r\n</blockquote>\r\n\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n\r\n<blockquote>\r\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\n     packets        bytes\r\n        6390       268380\r\n          10          420\r\n          31         1126\r\n           3          126\r\n</blockquote>\r\n\r\n\r\n\r\n\r\n\r\nInstall Docker. Here\'s how you do it on Ubuntu\r\n\r\nhttps://docs.docker.com/installation/ubuntulinux/\r\n\r\nhttps://docs.docker.com/installation/\r\n\r\nFollow the instructions at http://prometheus.io/docs/introduction/install/\r\n\r\nhttps://github.com/prometheus/jmx_exporter\r\n\r\n\r\n\r\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\r\n\r\nHere\'s a tar file of the exporter tailored for MN\r\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\r\n\r\nDownload the tar file.\r\ntar -zxvf prom_mn_jmx_scraper.tar.gz\r\ncd prom_mn_jmx_scraper\r\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\r\n    \"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\r\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\r\n\r\n    \"rules\": [\r\n      {\"pattern\": \"^meters:device:([0-9a-f-]+)<hostPort=(.+), type=(\\w+)><>Value:\",\r\n       \"name\": \"mndev_$3\",\r\n       \"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\r\n       \"type\": \"COUNTER\"},\r\n      {\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)<hostPort=(.+), type=(\\w+)><>Value:\",\r\n       \"name\": \"mnport_$1_$4\",\r\n       \"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\r\n       \"type\": \"COUNTER\"},\r\n      {\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)<hostPort=(.+), type=(\\w+)><>Value:\",\r\n       \"name\": \"mntun_$4\",\r\n       \"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\r\n       \"type\": \"COUNTER\"},\r\n    ]\r\n\r\n\r\nPoint a browser to http://<your ip>:7201/metrics\r\nYou should see something like:\r\n\r\n\r\n<blockquote>\r\n# HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=bytes><>Value)\r\n# TYPE mndev_bytes counter\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\r\n...\r\n...\r\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5<hostPort=119.15.127.100:7200, type=bytes><>Value)\r\n# TYPE mnport_rx_bytes counter\r\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\r\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\r\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3<hostPort=119.15.127.100:7200, type=pkts><>Value)\r\n# TYPE mndev_pkts counter\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0\r\n</blockquote>\r\n\r\n\r\n\r\n\r\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\r\n\r\ndocker run -d —name prom -p 9090:9090 -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.conf:/prometheus.conf -v <absolute-path-to-prom_mn_jmx_scraper>/prometheus.rules:/prometheus.rules prom/prometheus\r\n\r\nSome things to notice about prometheus.conf:\r\njob: {\r\n  # The job name is added as a label `job={job-name}` to any time series scraped from this job.\r\n  name: \"jmx_scraper\"\r\n  # Override the global default and scrape targets from this job every 5 seconds.\r\n  scrape_interval: \"15s\"\r\n\r\n  # Let\'s define a group of static targets to scrape for this job. In this\r\n  # case, only one.\r\n  target_group: {\r\n    # These endpoints are scraped via HTTP.\r\n    target: \"http://172.17.42.1:7201/metrics\"\r\n  }\r\n}\r\n\r\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\r\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\r\n\r\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\r\n\r\ndocker inspect sleepy_wilson | less\r\n\"NetworkMode\": \"bridge\"\r\n\r\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)\r\n\r\n\r\n\r\n&nbsp;',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(279,5,'2015-04-05 09:41:15','2015-04-05 09:41:15','Since v1.7 MidoNet has support for meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h1>Enter Prometheus</h1>\n\n&nbsp;\n\nInstall Docker. Here\'s how you do it on Ubuntu\n\nhttps://docs.docker.com/installation/ubuntulinux/\n\nhttps://docs.docker.com/installation/\n\nFollow the instructions at http://prometheus.io/docs/introduction/install/\n\nhttps://github.com/prometheus/jmx_exporter\n\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\n\nHere\'s a tar file of the exporter tailored for MN\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\n\nDownload the tar file.\ntar -zxvf prom_mn_jmx_scraper.tar.gz\ncd prom_mn_jmx_scraper\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\n\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\n\n\"rules\": [\n{\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mndev_$3\",\n\"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\n\"type\": \"COUNTER\"},\n{\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mnport_$1_$4\",\n\"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\n\"type\": \"COUNTER\"},\n{\"pattern\": \"^meters:tunnel:([0-9&#46;]+):([0-9&#46;]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mntun_$4\",\n\"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\n\"type\": \"COUNTER\"},\n]\n\nPoint a browser to http://:7201/metrics\nYou should see something like:\n\n<blockquote># HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\n# TYPE mndev_bytes counter\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\n...\n...\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\n# TYPE mnport_rx_bytes counter\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts&gt;&lt;&gt;Value)\n# TYPE mndev_pkts counter\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0</blockquote>\n\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\n\ndocker run -d —name prom -p 9090:9090 -v /prometheus.conf:/prometheus.conf -v /prometheus.rules:/prometheus.rules prom/prometheus\n\nSome things to notice about prometheus.conf:\njob: {\n\n<h1>The job name is added as a label <code>job={job-name}</code> to any time series scraped from this job.</h1>\n\nname: \"jmx_scraper\"\n\n<h1>Override the global default and scrape targets from this job every 5 seconds.</h1>\n\nscrape_interval: \"15s\"\n\n<h1>Let\'s define a group of static targets to scrape for this job. In this</h1>\n\n<h1>case, only one.</h1>\n\ntarget_group: {\n\n<h1>These endpoints are scraped via HTTP.</h1>\n\ntarget: \"http://172.17.42.1:7201/metrics\"\n}\n}\n\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\n\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\n\ndocker inspect sleepy_wilson | less\n\"NetworkMode\": \"bridge\"\n\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)\n\n&nbsp;','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-05 09:41:15','2015-04-05 09:41:15','Since v1.7 MidoNet has support for meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h1>Enter Prometheus</h1>\r\n&nbsp;\r\n\r\nInstall Docker. Here\'s how you do it on Ubuntu\r\n\r\nhttps://docs.docker.com/installation/ubuntulinux/\r\n\r\nhttps://docs.docker.com/installation/\r\n\r\nFollow the instructions at http://prometheus.io/docs/introduction/install/\r\n\r\nhttps://github.com/prometheus/jmx_exporter\r\n\r\n<a title=\"JMX Exporter for Prometheus\" href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>\r\n\r\nHere\'s a tar file of the exporter tailored for MN\r\nhttps://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\r\n\r\nDownload the tar file.\r\ntar -zxvf prom_mn_jmx_scraper.tar.gz\r\ncd prom_mn_jmx_scraper\r\nedit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape.\r\n\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"\r\nThe scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.\r\n\r\n\"rules\": [\r\n{\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mndev_$3\",\r\n\"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\r\n\"type\": \"COUNTER\"},\r\n{\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mnport_$1_$4\",\r\n\"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\r\n\"type\": \"COUNTER\"},\r\n{\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mntun_$4\",\r\n\"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\r\n\"type\": \"COUNTER\"},\r\n]\r\n\r\nPoint a browser to http://:7201/metrics\r\nYou should see something like:\r\n<blockquote># HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\r\n# TYPE mndev_bytes counter\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\r\n...\r\n...\r\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\r\n# TYPE mnport_rx_bytes counter\r\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\r\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\r\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts&gt;&lt;&gt;Value)\r\n# TYPE mndev_pkts counter\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0</blockquote>\r\nIf that\'s what you\'re seeing, then you\'re ready to start Prometheus.\r\n\r\ndocker run -d —name prom -p 9090:9090 -v /prometheus.conf:/prometheus.conf -v /prometheus.rules:/prometheus.rules prom/prometheus\r\n\r\nSome things to notice about prometheus.conf:\r\njob: {\r\n# The job name is added as a label `job={job-name}` to any time series scraped from this job.\r\nname: \"jmx_scraper\"\r\n# Override the global default and scrape targets from this job every 5 seconds.\r\nscrape_interval: \"15s\"\r\n\r\n# Let\'s define a group of static targets to scrape for this job. In this\r\n# case, only one.\r\ntarget_group: {\r\n# These endpoints are scraped via HTTP.\r\ntarget: \"http://172.17.42.1:7201/metrics\"\r\n}\r\n}\r\n\r\nThe jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\r\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.\r\n\r\nYou should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).\r\n\r\ndocker inspect sleepy_wilson | less\r\n\"NetworkMode\": \"bridge\"\r\n\r\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)\r\n\r\n&nbsp;',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(280,5,'2015-04-05 10:26:52','2015-04-05 10:26:52','Since v1.7 MidoNet has support for meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\n<p style=\"line-height: 1.5;\"></p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>.</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">\"rules\": [\n{\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mndev_$3\",\n\"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\n\"type\": \"COUNTER\"},\n{\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mnport_$1_$4\",\n\"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\n\"type\": \"COUNTER\"},\n{\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mntun_$4\",\n\"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\n\"type\": \"COUNTER\"},\n]</p>\n\n<p style=\"line-height: 1.5;\">Point a browser to http://:7201/metrics\nYou should see something like:</p>\n\n<blockquote style=\"line-height: 1.5;\"># HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\n# TYPE mndev_bytes counter\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\n...\n...\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\n# TYPE mnport_rx_bytes counter\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts&gt;&lt;&gt;Value)\n# TYPE mndev_pkts counter\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0</blockquote>\n\n<p style=\"line-height: 1.5;\">If that\'s what you\'re seeing, then you\'re ready to start Prometheus.</p>\n\n<p style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /prometheus.conf:/prometheus.conf -v /prometheus.rules:/prometheus.rules prom/prometheus</p>\n\n<p style=\"line-height: 1.5;\">Some things to notice about prometheus.conf:\njob: {\n# The job name is added as a label `job={job-name}` to any time series scraped from this job.\nname: \"jmx_scraper\"\n# Override the global default and scrape targets from this job every 5 seconds.\nscrape_interval: \"15s\"</p>\n\n<p style=\"line-height: 1.5;\"># Let\'s define a group of static targets to scrape for this job. In this\n# case, only one.\ntarget_group: {\n# These endpoints are scraped via HTTP.\ntarget: \"http://172.17.42.1:7201/metrics\"\n}\n}</p>\n\n<p style=\"line-height: 1.5;\">The jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.</p>\n\n<p style=\"line-height: 1.5;\">You should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).</p>\n\n<p style=\"line-height: 1.5;\">docker inspect sleepy_wilson | less\n\"NetworkMode\": \"bridge\"</p>\n\n<p style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</p>\n\n<p style=\"line-height: 1.5;\"></p>','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-05 10:26:52','2015-04-05 10:26:52','Since v1.7 MidoNet has support for meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\n<p style=\"line-height: 1.5;\"></p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>.</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">\"rules\": [\r\n{\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mndev_$3\",\r\n\"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\r\n\"type\": \"COUNTER\"},\r\n{\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mnport_$1_$4\",\r\n\"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\r\n\"type\": \"COUNTER\"},\r\n{\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mntun_$4\",\r\n\"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\r\n\"type\": \"COUNTER\"},\r\n]</p>\r\n<p style=\"line-height: 1.5;\">Point a browser to http://:7201/metrics\r\nYou should see something like:</p>\r\n\r\n<blockquote style=\"line-height: 1.5;\"># HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\r\n# TYPE mndev_bytes counter\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\r\n...\r\n...\r\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\r\n# TYPE mnport_rx_bytes counter\r\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\r\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\r\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts&gt;&lt;&gt;Value)\r\n# TYPE mndev_pkts counter\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0</blockquote>\r\n<p style=\"line-height: 1.5;\">If that\'s what you\'re seeing, then you\'re ready to start Prometheus.</p>\r\n<p style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /prometheus.conf:/prometheus.conf -v /prometheus.rules:/prometheus.rules prom/prometheus</p>\r\n<p style=\"line-height: 1.5;\">Some things to notice about prometheus.conf:\r\njob: {\r\n# The job name is added as a label `job={job-name}` to any time series scraped from this job.\r\nname: \"jmx_scraper\"\r\n# Override the global default and scrape targets from this job every 5 seconds.\r\nscrape_interval: \"15s\"</p>\r\n<p style=\"line-height: 1.5;\"># Let\'s define a group of static targets to scrape for this job. In this\r\n# case, only one.\r\ntarget_group: {\r\n# These endpoints are scraped via HTTP.\r\ntarget: \"http://172.17.42.1:7201/metrics\"\r\n}\r\n}</p>\r\n<p style=\"line-height: 1.5;\">The jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\r\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.</p>\r\n<p style=\"line-height: 1.5;\">You should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).</p>\r\n<p style=\"line-height: 1.5;\">docker inspect sleepy_wilson | less\r\n\"NetworkMode\": \"bridge\"</p>\r\n<p style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</p>\r\n<p style=\"line-height: 1.5;\"></p>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(281,5,'2015-04-05 10:28:12','2015-04-05 10:28:12','Since v1.7 MidoNet has support for meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nTODO: point to MN\'s fork of the JMX Exporter, and to the patch that adds the changes described above.\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>.</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">\"rules\": [\n{\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mndev_$3\",\n\"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\n\"type\": \"COUNTER\"},\n{\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mnport_$1_$4\",\n\"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\n\"type\": \"COUNTER\"},\n{\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\n\"name\": \"mntun_$4\",\n\"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\n\"type\": \"COUNTER\"},\n]</p>\n\n<p style=\"line-height: 1.5;\">Point a browser to http://:7201/metrics\nYou should see something like:</p>\n\n<blockquote style=\"line-height: 1.5;\"># HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\n# TYPE mndev_bytes counter\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\n...\n...\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\n# TYPE mnport_rx_bytes counter\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts&gt;&lt;&gt;Value)\n# TYPE mndev_pkts counter\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0</blockquote>\n\n<p style=\"line-height: 1.5;\">If that\'s what you\'re seeing, then you\'re ready to start Prometheus.</p>\n\n<p style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /prometheus.conf:/prometheus.conf -v /prometheus.rules:/prometheus.rules prom/prometheus</p>\n\n<p style=\"line-height: 1.5;\">Some things to notice about prometheus.conf:\njob: {\n# The job name is added as a label `job={job-name}` to any time series scraped from this job.\nname: \"jmx_scraper\"\n# Override the global default and scrape targets from this job every 5 seconds.\nscrape_interval: \"15s\"</p>\n\n<p style=\"line-height: 1.5;\"># Let\'s define a group of static targets to scrape for this job. In this\n# case, only one.\ntarget_group: {\n# These endpoints are scraped via HTTP.\ntarget: \"http://172.17.42.1:7201/metrics\"\n}\n}</p>\n\n<p style=\"line-height: 1.5;\">The jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.</p>\n\n<p style=\"line-height: 1.5;\">You should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).</p>\n\n<p style=\"line-height: 1.5;\">docker inspect sleepy_wilson | less\n\"NetworkMode\": \"bridge\"</p>\n\n<p style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</p>','MidoNet meets Prometheus.io','','inherit','open','open','','272-revision-v1','','','2015-04-05 10:28:12','2015-04-05 10:28:12','Since v1.7 MidoNet has support for meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nTODO: point to MN\'s fork of the JMX Exporter, and to the patch that adds the changes described above.\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>.</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit run_mn_scraper.sh if you want to change the port.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">\"rules\": [\r\n{\"pattern\": \"^meters:device:([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mndev_$3\",\r\n\"labels\": {\"mn_id\":\"$1\", \"mn_agent\":\"$2\"},\r\n\"type\": \"COUNTER\"},\r\n{\"pattern\": \"^meters:port:(\\w+):([0-9a-f-]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mnport_$1_$4\",\r\n\"labels\": {\"mn_id\":\"$2\", \"mn_agent\":\"$3\"},\r\n\"type\": \"COUNTER\"},\r\n{\"pattern\": \"^meters:tunnel:([0-9\\.]+):([0-9\\.]+)&lt;hostPort=(.+), type=(\\w+)&gt;&lt;&gt;Value:\",\r\n\"name\": \"mntun_$4\",\r\n\"labels\": {\"src\":\"$1\", \"dst\":\"$2\", \"mn_agent\":\"$3\"},\r\n\"type\": \"COUNTER\"},\r\n]</p>\r\n<p style=\"line-height: 1.5;\">Point a browser to http://:7201/metrics\r\nYou should see something like:</p>\r\n\r\n<blockquote style=\"line-height: 1.5;\"># HELP mndev_bytes Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\r\n# TYPE mndev_bytes counter\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 213486.0\r\n...\r\n...\r\n# HELP mnport_rx_bytes Description (meters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5&lt;hostPort=119.15.127.100:7200, type=bytes&gt;&lt;&gt;Value)\r\n# TYPE mnport_rx_bytes counter\r\nmnport_rx_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"64594d49-cae0-4e21-8472-7d5c1fc30dc5\",} 213486.0\r\nmnport_rx_bytes{mn_agent=\"119.15.127.101:7200\",mn_id=\"c040e61d-8329-49b6-8a3a-a11b834fb2c5\",} 221134.0\r\n# HELP mndev_pkts Description (meters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3&lt;hostPort=119.15.127.100:7200, type=pkts&gt;&lt;&gt;Value)\r\n# TYPE mndev_pkts counter\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 5083.0\r\nmndev_pkts{mn_agent=\"119.15.127.100:7200\",mn_id=\"405fedd8-f03e-418d-8aff-36c843eb481f\",} 5083.0</blockquote>\r\n<p style=\"line-height: 1.5;\">If that\'s what you\'re seeing, then you\'re ready to start Prometheus.</p>\r\n<p style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /prometheus.conf:/prometheus.conf -v /prometheus.rules:/prometheus.rules prom/prometheus</p>\r\n<p style=\"line-height: 1.5;\">Some things to notice about prometheus.conf:\r\njob: {\r\n# The job name is added as a label `job={job-name}` to any time series scraped from this job.\r\nname: \"jmx_scraper\"\r\n# Override the global default and scrape targets from this job every 5 seconds.\r\nscrape_interval: \"15s\"</p>\r\n<p style=\"line-height: 1.5;\"># Let\'s define a group of static targets to scrape for this job. In this\r\n# case, only one.\r\ntarget_group: {\r\n# These endpoints are scraped via HTTP.\r\ntarget: \"http://172.17.42.1:7201/metrics\"\r\n}\r\n}</p>\r\n<p style=\"line-height: 1.5;\">The jmx_scraper job has a single target: http://172.17.42.1:7201/metrics\r\nOn my host, 172.17.42.1:7201 is the address of my docker0 bridge. Prometheus running inside the container will pull metrics from the JMX_Scraper http server running on the host. The container\'s interface (hidden if you run ifconfig on the host) is a half of a veth pair. I can see the other half veth1b6ef3e by running ifconfig, but no address is assigned to this interface because it\'s used as an L2 port on the docker0 bridge.</p>\r\n<p style=\"line-height: 1.5;\">You should change that target to match the address of your docker0 bridge (more precisely, the address assigned to docker0\'s \"local\" port).</p>\r\n<p style=\"line-height: 1.5;\">docker inspect sleepy_wilson | less\r\n\"NetworkMode\": \"bridge\"</p>\r\n<p style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</p>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(282,5,'2015-04-06 00:04:29','2015-04-06 00:04:29','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nTODO: point to MN\'s fork of the JMX Exporter, and to the patch that adds the changes described above.\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (mn_scraper_config.json) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\">Prometheus rules file with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</blockquote>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\n</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<blockquote>\nmndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-06 00:04:29','2015-04-06 00:04:29','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nTODO: point to MN\'s fork of the JMX Exporter, and to the patch that adds the changes described above.\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (mn_scraper_config.json) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\">Prometheus rules file with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</blockquote>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\r\n</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<blockquote>\r\nmndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(284,5,'2015-04-09 15:50:06','2015-04-09 15:50:06','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<pre class=\"prettyprint\">\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</pre>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<pre class=\"prettyprint\"># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</pre>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<pre class=\"prettyprint\">mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</pre>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<pre class=\"prettyprint\">\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-autosave-v1','','','2015-04-09 15:50:06','2015-04-09 15:50:06','',272,'http://blog.midonet.org/272-autosave-v1/',0,'revision','',0),(285,5,'2015-04-09 15:07:02','2015-04-09 15:07:02','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (mn_scraper_config.json) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\">Prometheus rules file with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</blockquote>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 15:07:02','2015-04-09 15:07:02','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (mn_scraper_config.json) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\">Prometheus rules file with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</blockquote>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(287,5,'2015-04-09 15:36:41','2015-04-09 15:36:41','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</blockquote>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 15:36:41','2015-04-09 15:36:41','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</blockquote>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</blockquote>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(288,5,'2015-04-09 15:42:45','2015-04-09 15:42:45','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</blockquote>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</blockquote>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 15:42:45','2015-04-09 15:42:45','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally.\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<blockquote># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</blockquote>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</blockquote>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(289,5,'2015-04-09 15:47:06','2015-04-09 15:47:06','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n<code class=\"prettyprint\">\n\n<h1>mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4</h1>\n\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</code>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</blockquote>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 15:47:06','2015-04-09 15:47:06','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<code class=\"prettyprint\">\r\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</code>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</blockquote>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(290,5,'2015-04-09 15:47:57','2015-04-09 15:47:57','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<pre class=\"prettyprint\">\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</pre>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</blockquote>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 15:47:57','2015-04-09 15:47:57','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<pre class=\"prettyprint\">\r\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</pre>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<blockquote># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</blockquote>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<blockquote>mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0</blockquote>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<blockquote style=\"line-height: 1.5;\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</blockquote>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<blockquote>mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</blockquote>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(291,5,'2015-04-09 15:50:54','2015-04-09 15:50:54','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<pre class=\"prettyprint\">\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</pre>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<pre class=\"prettyprint\">\n# HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</pre>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<pre class=\"prettyprint\">\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\n</pre>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<pre class=\"prettyprint\">\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 15:50:54','2015-04-09 15:50:54','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<pre class=\"prettyprint\">\r\n# mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</pre>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<pre class=\"prettyprint\">\r\n# HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</pre>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<pre class=\"prettyprint\">\r\nmndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\n</pre>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<pre class=\"prettyprint\">\r\nmndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(292,1,'2015-04-09 16:07:43','2015-04-09 16:07:43','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\n\n<ul>\n    <li>device, to track packets/bytes arriving at a bridge or router</li>\n    <li>port, to track packets/bytes transmitted-from/received-at a device port</li>\n    <li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\n    <li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\n</ul>\n\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\n\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\n\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:tunnel:-1062731735:-1062731745\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\nmeters:tunnel:-1062731735:-1062731737\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\nmeters:user:null\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\n\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\n\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\npackets bytes\n6390 268380\n10 420\n31 1126\n3 126</pre>\n\n<h2>Prometheus</h2>\n\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\n\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\n\n<ol>\n    <li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\n    <li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\n    <li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\n</ol>\n\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\n\n<ul>\n    <li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\n    <li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\n</ul>\n\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\n\n<h2>Running the JMX Scraper</h2>\n\n<ol>\n    <li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\n<ul>\n    <li style=\"line-height: 1.5;\">JMX Exporter jar</li>\n    <li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\n    <li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\n    <li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\n    <li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\n    <li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\n    <li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\n    <li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\n    <li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\n<ul>\n    <li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\n</ul>\n</li>\n    <li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\n<pre class=\"prettyprint\"># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\n# TYPE jmx_scrape_duration_seconds gauge\njmx_scrape_duration_seconds 3.414254924\n# HELP jmx_scrape_error Non-zero if this scrape failed.\n# TYPE jmx_scrape_error gauge\njmx_scrape_error 0.0</pre>\n</li>\n    <li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\n<pre class=\"prettyprint\">mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\n</pre>\n</li>\n</ol>\n\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\n\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\n\n<ol>\n    <li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\n    <li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\n    <li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\n<ul>\n    <li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\n    <li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\n<pre class=\"prettyprint\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\n</li>\n</ul>\n</li>\n</ol>\n\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\n\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>','MidoNet\'s traffic meters and Prometheus TSDB','','inherit','open','open','','272-revision-v1','','','2015-04-09 16:07:43','2015-04-09 16:07:43','Since v1.5 the MidoNet Java Agent exposes many counters and metrics via JMX. Initially, these were related to the Agent\'s workload and performance. However, since v1.7 MidoNet has support for overlay and underlay traffic meters. There four kinds of meters:\r\n<ul>\r\n	<li>device, to track packets/bytes arriving at a bridge or router</li>\r\n	<li>port, to track packets/bytes transmitted-from/received-at a device port</li>\r\n	<li>tunnel, to track packets/bytes tunneled from a source host to a destination host</li>\r\n	<li>user defined, track packets/bytes of flows matching conditions defined by the user</li>\r\n</ul>\r\nThe first three don\'t need to be configured, they are provided by default. MN Agent tracks them by default on all corresponding objects. The user defined meters need to be configured in rules (we\'ll leave that for a future post).\r\n\r\nEach Agent exposes its locally tracked meters via JMX. Remember that an Agent\'s local meters only count packets/bytes that ingress locally, so usually it\'s necessary to sum across all Agents in a deployment to get a meaningful count. That\'s the case for Tenant Routers, for example.\r\n\r\n<strong>mm-meter</strong> is a tool installed by the <strong>midolman</strong> package that allows you to query a single Agent\'s meters. The following is example output for its <strong>list</strong> command. Notice that this Agent only tracks meters for two ports, they are two VM-facing ports on the same overlay bridge in my test deployment. This agent only sees packets transmitted from c040e61d and received at 64594d49 because the former is bound at a remote Agent and the latter is bound locally. <em>Also, notice that the tunnel source and destination IP addresses are integers. This should probably be fixed in MidoNet itself, but for this post I fixed it in the MN Agent scraper.</em>\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 list\r\nmeters:device:4fc20423-5e56-4cea-a4f3-cdf02917e7a3\r\nmeters:port:tx:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:405fedd8-f03e-418d-8aff-36c843eb481f\r\nmeters:device:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:tunnel:-1062731735:-1062731745\r\nmeters:device:0e561fb2-6130-4443-8a69-a6270c89727e\r\nmeters:tunnel:-1062731735:-1062731737\r\nmeters:device:c040e61d-8329-49b6-8a3a-a11b834fb2c5\r\nmeters:device:c5590f1f-2ce0-4d60-a4cc-d8e27d01dff7\r\nmeters:device:daa86f7c-8701-49ee-ad1f-b52f8fd6af36\r\nmeters:device:7303fdb8-03ad-4da6-8e10-3274ead3c018\r\nmeters:port:rx:64594d49-cae0-4e21-8472-7d5c1fc30dc5\r\nmeters:user:null\r\nmeters:device:c6cd63c0-aab4-491e-8f7a-45b3637e0cdc\r\nmeters:device:a11abdd3-8d0b-478f-8911-5259531c1c16</pre>\r\nYou can query the packet and byte counters for a single meter. The following is example output for the <strong>get</strong> command. The last two argument specify 4 updates at 60 second intervals. You can see that the first stats show the total counters, afterwards only deltas are printed.\r\n<pre class=\"prettyprint\"># mm-meter -h 119.15.127.100 -p 7200 get -n meters:port:tx:c040e61d-8329-49b6-8a3a-a1834fb2c5 60 4\r\npackets bytes\r\n6390 268380\r\n10 420\r\n31 1126\r\n3 126</pre>\r\n<h2>Prometheus</h2>\r\nPrometheus is a recent Time Series Database by SoundCloud - it\'s simply fantastic. In this post I\'m going to help you set up Prometheus to collect, aggregate, and store MidoNet meters.\r\n\r\nStart by installing Prometheus. You can follow their <a href=\"http://prometheus.io/docs/introduction/install/\" target=\"_blank\">Installing</a> and <a href=\"http://prometheus.io/docs/introduction/getting_started/\" target=\"_blank\">Getting Started</a> documentation (kudos to SoundCloud for providing excellent documentation), but I provide short instructions here.\r\n<ol>\r\n	<li>First you need to install Docker. The <a href=\"https://docs.docker.com/installation/ubuntulinux/\" target=\"_blank\">Ubuntu install docs</a> suggest:  <em>wget -qO- https://get.docker.com/ | sh</em></li>\r\n	<li>Now launch the Prometheus container: <em>docker run -p 9090:9090 prom/prometheus</em></li>\r\n	<li>The Prometheus container is configured to monitor itself. Verify that Prometheus is running correctly by pointing your browser to http://&lt;your-host-ip&gt;:9090/. Scroll down to the <strong>Targets</strong> section, you should see one healthy target: http://localhost:9090/metrics.</li>\r\n</ol>\r\n<p style=\"line-height: 1.5;\">Prometheus pulls metrics over HTTP. Since MN Agents only serve metrics via JMX, we\'re going to need the <a href=\"https://github.com/prometheus/jmx_exporter\" target=\"_blank\">JMX to Prometheus bridge</a>. However, I\'ve had to hack it a bit so that:</p>\r\n\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">it can read the MN Agent\'s <a href=\"https://github.com/midonet/midonet/blob/master/midolman/src/main/java/org/midonet/midolman/management/MeteringMXBean.java\" target=\"_blank\">MeteringMXBean</a>. This bean has 2 methods: <em>String[] listMeters()</em> and <em>FlowStats getMeter(String name)</em>. In contrast, Prometheus\' JMX Exporter expects one bean per metric.</li>\r\n	<li style=\"line-height: 1.5;\">it can scrape multiple MN Agents.</li>\r\n</ul>\r\nYou can look at the <a href=\"https://github.com/midonet/jmx_exporter/commit/1ecb305dbe1d966a7b31414dc503ac12a3e9e08f\">patch</a> for these hacks in my <a href=\"https://github.com/midonet/jmx_exporter\">fork of the JMX Exporter</a>.\r\n<h2>Running the JMX Scraper</h2>\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">Download the tar-file of the modified JMX Exporter <a href=\"https://drive.google.com/file/d/0B7IYSQH-LBhvSnRQRTAtTWQwUVU/view?pli=1\" target=\"_blank\">here</a>. It contains the following:\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter jar</li>\r\n	<li style=\"line-height: 1.5;\">MidoNet jars containing the MeteringMXBean and FlowStats classes</li>\r\n	<li style=\"line-height: 1.5;\">Shell script for running the JMX Exporter (run_mn_scraper.sh)</li>\r\n	<li style=\"line-height: 1.5;\">JMX Exporter configuration file (<a href=\"https://gist.github.com/gdecandia/2aad07c810fbf5084206\">mn_scraper_config.json</a>) with MN Agent IP addresses, and rules matching only MidoNet\'s device, port and tunnel metrics. <em>Later we can add rules to expose the MN Agent\'s other metrics.</em></li>\r\n	<li style=\"line-height: 1.5;\">Prometheus configuration file that defines a job for pulling metrics from the JMX Exporter.</li>\r\n	<li style=\"line-height: 1.5;\"><a href=\"https://gist.github.com/gdecandia/6a3d5040f5309bbbacfe\">Prometheus rules file</a> with rule definitions to pre-aggregate device and port meters across all MN Agents.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">tar -zxvf prom_mn_jmx_scraper.tar.gz</li>\r\n	<li style=\"line-height: 1.5;\">cd prom_mn_jmx_scraper</li>\r\n	<li style=\"line-height: 1.5;\">edit mn_scraper_config.json to provide the comma-separated list of IP addresses of all the MN Agents you want to scrape. For example: <em>\"hostPort\": \"119.15.127.100:7200,119.15.127.101:7200\"</em></li>\r\n	<li style=\"line-height: 1.5;\">Launch run_mn_scraper.sh\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">The scraper runs an httpserver on port 7201. Edit the script if you want to change the port.</li>\r\n</ul>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">The JMX Exporter (when run as an http server as we\'re doing) scrapes its target each time its URL is queried. Verify the Exporter is working correctly by pointing your browser to http://&lt;your-host-ip&gt;:7201/metrics. You should at least see the following at the end of the response:\r\n<pre class=\"prettyprint\"># HELP jmx_scrape_duration_seconds Time this JMX scrape took, in seconds.\r\n# TYPE jmx_scrape_duration_seconds gauge\r\njmx_scrape_duration_seconds 3.414254924\r\n# HELP jmx_scrape_error Non-zero if this scrape failed.\r\n# TYPE jmx_scrape_error gauge\r\njmx_scrape_error 0.0</pre>\r\n</li>\r\n	<li style=\"line-height: 1.5;\">Now verify that your MN Agents are being scraped. The response should include lines like the following, where \"mn_agent\" shows the IP of your Agent. You should be able to find all the IPs you listed in mn_scraper_config.json:\r\n<pre class=\"prettyprint\">mndev_bytes{mn_agent=\"119.15.127.100:7200\",mn_id=\"4fc20423-5e56-4cea-a4f3-cdf02917e7a3\",} 213486.0\r\n</pre>\r\n</li>\r\n</ol>\r\n<h2 style=\"line-height: 1.5;\">Updating Prometheus</h2>\r\n<p style=\"line-height: 1.5;\">Now we\'re ready to reconfigure Prometheus to pull metrics from our JMX Exporter and to run the aggregation rules specified in our file.</p>\r\n\r\n<ol>\r\n	<li style=\"line-height: 1.5;\">If necessary, change the target address specified in prometheus.conf.  Since Prometheus is running in a container, while the JMX Exporter runs on the host, Prometheus must send queries to the host\'s address on the docker0 bridge (the container\'s \"NetworkMode\" = \"bridge\"). On my host, ifconfig shows that docker0 has address 172.17.42.1, and that\'s why the file prometheus.conf uses that address.</li>\r\n	<li style=\"line-height: 1.5;\">docker run -d —name prom -p 9090:9090 -v /full/host/path/to/prometheus.conf:/prometheus.conf -v /full/host/path/to/prometheus.rules:/prometheus.rules prom/prometheus</li>\r\n	<li><span style=\"line-height: 1.5;\">Reload Prometheus\' main page: http://&lt;your-host-ip&gt;:9090/.</span>\r\n<ul>\r\n	<li style=\"line-height: 1.5;\">Verify that the \"Targets\" includes the JMX Exporter\'s address. In my case it\'s http://172.17.42.1:7201/metrics</li>\r\n	<li><span style=\"line-height: 1.5;\">Verify that the section \"Rules\" contains the rules specified in the file prometheus.rules</span>\r\n<pre class=\"prettyprint\">mndev_bytes_1m = sum(delta(mndev_bytes[1m])) by (mn_id)\r\nmndev_pkts_1m = sum(delta(mndev_pkts[1m])) by (mn_id)\r\nmnport_tx_bytes_1m = sum(delta(mnport_tx_bytes[1m])) by (mn_id)\r\nmnport_rx_bytes_1m = sum(delta(mnport_rx_bytes[1m])) by (mn_id)\r\nmnport_tx_pkts_1m = sum(delta(mnport_tx_pkts[1m])) by (mn_id)\r\nmnport_rx_pkts_1m = sum(delta(mnport_rx_pkts[1m])) by (mn_id)\r\nmntun_bytes_1m = sum(delta(mntun_bytes[1m])) by (src)\r\nmntun_pkts_1m = sum(delta(mntun_pkts[1m])) by (src)</pre>\r\n</li>\r\n</ul>\r\n</li>\r\n</ol>\r\nNow you can go to the \"Graph\" section of Prometheus\' dashboard http://119.15.121.38:9090/graph and you can graph the pre-aggregated metrics created by the rules. For example the following identifies the metric containing the bytes arriving in a given minute at the device with UUID 7303fdb8...\r\n<pre class=\"prettyprint\">mndev_bytes_1m{mn_id=\"7303fdb8-03ad-4da6-8e10-3274ead3c018\"}</pre>',272,'http://blog.midonet.org/272-revision-v1/',0,'revision','',0),(293,14,'2015-05-05 08:42:33','2015-05-05 08:42:33','In its upcoming version, Midonet will include a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\n\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','publish','open','open','','zoom-reactive-programming-zookeeper','','','2015-05-07 16:18:04','2015-05-07 16:18:04','In its upcoming version, Midonet will include a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\r\n\r\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',0,'http://blog.midonet.org/?p=293',0,'post','',0),(294,14,'2015-04-13 15:38:56','2015-04-13 15:38:56','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\nMidonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.\n\nStreams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.\n\nReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n&nbsp;','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-13 15:38:56','2015-04-13 15:38:56','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n\r\nMidonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.\r\n\r\nStreams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.\r\n\r\nReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\n&nbsp;',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(296,14,'2015-04-13 16:04:08','2015-04-13 16:04:08','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nmessage Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n\n</code></pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nval hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</code></pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\">INSERT DRAWING</p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nval observable: Observable[MidoHost] =\n\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\n                      aliveObservable,\n                      hostObservable)\n    .filter(isHostReady)\n    .map[MidoHost](deviceUpdated)\n    .distinctUntilChanged\n</code></pre>\n\n&nbsp;\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<p class=\"prettyprint\">/**</p>\n\n<p class=\"prettyprint\">* A map function that creates the host simulation device from the current</p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">* host, tunnel-zones and alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</code></pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-13 16:04:08','2015-04-13 16:04:08','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nmessage Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n\r\n</code></pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nval hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</code></pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\">INSERT DRAWING</p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nval observable: Observable[MidoHost] =\r\n\r\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                      aliveObservable,\r\n                      hostObservable)\r\n    .filter(isHostReady)\r\n    .map[MidoHost](deviceUpdated)\r\n    .distinctUntilChanged\r\n</code></pre>\r\n&nbsp;\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n<p class=\"prettyprint\">/**</p>\r\n<p class=\"prettyprint\">* A map function that creates the host simulation device from the current</p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">* host, tunnel-zones and alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</code></pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(297,14,'2015-04-13 16:08:43','2015-04-13 16:08:43','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nmessage Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n\n</code></pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nval hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</code></pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\">INSERT DRAWING</p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nval observable: Observable[MidoHost] =\n\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\n                      aliveObservable,\n                      hostObservable)\n    .filter(isHostReady)\n    .map[MidoHost](deviceUpdated)\n    .distinctUntilChanged\n</code></pre>\n\n&nbsp;\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\n/**\n* A map function that creates the host simulation device from the current\n* host, tunnel-zones and alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</code></pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-13 16:08:43','2015-04-13 16:08:43','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nmessage Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n\r\n</code></pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nval hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</code></pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\">INSERT DRAWING</p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nval observable: Observable[MidoHost] =\r\n\r\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                      aliveObservable,\r\n                      hostObservable)\r\n    .filter(isHostReady)\r\n    .map[MidoHost](deviceUpdated)\r\n    .distinctUntilChanged\r\n</code></pre>\r\n&nbsp;\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\n/**\r\n* A map function that creates the host simulation device from the current\r\n* host, tunnel-zones and alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</code></pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(298,14,'2015-04-13 16:17:18','2015-04-13 16:17:18','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\">INSERT DRAWING</p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<pre class=\"prettyprint\"><code class=\"language-scala\">\nval observable: Observable[MidoHost] =\n\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\n                      aliveObservable,\n                      hostObservable)\n    .filter(isHostReady)\n    .map[MidoHost](deviceUpdated)\n    .distinctUntilChanged\n</code></pre>\n\n&nbsp;\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation device from the current\n* host, tunnel-zones and alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-13 16:17:18','2015-04-13 16:17:18','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\">INSERT DRAWING</p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n\r\n<pre class=\"prettyprint\"><code class=\"language-scala\">\r\nval observable: Observable[MidoHost] =\r\n\r\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                      aliveObservable,\r\n                      hostObservable)\r\n    .filter(isHostReady)\r\n    .map[MidoHost](deviceUpdated)\r\n    .distinctUntilChanged\r\n</code></pre>\r\n&nbsp;\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation device from the current\r\n* host, tunnel-zones and alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(299,14,'2015-04-13 16:18:49','2015-04-13 16:18:49','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\">INSERT DRAWING</p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\n                      aliveObservable,\n                      hostObservable)\n    .filter(isHostReady)\n    .map[MidoHost](deviceUpdated)\n    .distinctUntilChanged\n</pre>\n\n&nbsp;\n\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation device from the current\n* host, tunnel-zones and alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-13 16:18:49','2015-04-13 16:18:49','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\">INSERT DRAWING</p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n\r\nObservable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                      aliveObservable,\r\n                      hostObservable)\r\n    .filter(isHostReady)\r\n    .map[MidoHost](deviceUpdated)\r\n    .distinctUntilChanged\r\n</pre>\r\n&nbsp;\r\n<p class=\"prettyprint\"><strong><strong> </strong></strong></p>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation device from the current\r\n* host, tunnel-zones and alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(302,14,'2015-04-16 15:19:03','2015-04-16 15:19:03','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-16 15:19:03','2015-04-16 15:19:03','<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(303,14,'2015-04-16 15:25:00','2015-04-16 15:25:00','#page {\nwidth: 1200px;\n}\n\n<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-16 15:25:00','2015-04-16 15:25:00','#page {\r\nwidth: 1200px;\r\n}\r\n<p style=\"text-align: justify\">Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.</p>\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(304,14,'2015-04-16 15:28:30','2015-04-16 15:28:30','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-16 15:28:30','2015-04-16 15:28:30','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(305,1,'2015-04-16 17:11:30','2015-04-16 17:11:30','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<script src=\"https://gist.github.com/adjohn/92053e9277e500eb55d2.js\"></script>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-autosave-v1','','','2015-04-16 17:11:30','2015-04-16 17:11:30','',293,'http://blog.midonet.org/293-autosave-v1/',0,'revision','',0),(306,1,'2015-04-16 16:02:10','2015-04-16 16:02:10','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<script src=\"https://gist.github.com/adjohn/92053e9277e500eb55d2.js\"></script>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-16 16:02:10','2015-04-16 16:02:10','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<script src=\"https://gist.github.com/adjohn/92053e9277e500eb55d2.js\"></script>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(307,14,'2015-04-16 16:18:13','2015-04-16 16:18:13','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-16 16:18:13','2015-04-16 16:18:13','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(308,1,'2015-04-16 17:11:24','2015-04-16 17:11:24','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<script src=\"https://gist.github.com/adjohn/92053e9277e500eb55d2.js\"></script>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-16 17:11:24','2015-04-16 17:11:24','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<script src=\"https://gist.github.com/adjohn/92053e9277e500eb55d2.js\"></script>\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(309,14,'2015-04-17 07:22:32','2015-04-17 07:22:32','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-17 07:22:32','2015-04-17 07:22:32','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged</pre>\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(310,14,'2015-04-17 07:26:56','2015-04-17 07:26:56','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged\n</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-17 07:26:56','2015-04-17 07:26:56','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged\r\n</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(311,14,'2015-04-17 07:52:47','2015-04-17 07:52:47','wp_enqueue_script()\n\nMidonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged\n</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-17 07:52:47','2015-04-17 07:52:47','wp_enqueue_script()\r\n\r\nMidonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged\r\n</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(312,14,'2015-04-17 07:54:47','2015-04-17 07:54:47','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged\n</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-17 07:54:47','2015-04-17 07:54:47','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged\r\n</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(315,14,'2015-04-20 07:34:51','2015-04-20 07:34:51','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<pre class=\"prettyprint\">message Host {\n   optional UUID id = 1;\n   optional string name = 2;\n   repeated IPAddress addresses = 3;\n\n   message PortBinding {\n       optional UUID port_id = 1;\n       optional string interface_name = 2;\n   }\n   repeated PortBinding port_bindings = 4;\n   optional int32 flooding_proxy_weight = 5;\n\n   // Back-reference. Expected to be set from the Tunnel Zone side.\n   repeated UUID tunnel_zone_ids = 101;\n}\n\n@ZoomClass(clazz = classOf[TopologyHost])\nclass MidoHost extends ZoomObject with Device {\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\n    var id: UUID = _\n\n    @ZoomField(name = \"port_bindings\",\n               converter = classOf[PortBindingConverter])\n    var portBindings = Map.empty[UUID, String]\n\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\n    var tunnelZoneIds = Set.empty[UUID]\n\n    // The IP address of the host in each one of the tunnel zones\n    // (this is stored outside of the host proto).\n    var tunnelZones = Map.empty[UUID, IPAddr]\n    // The alive status of the host (this is stored outside of the host proto).\n    var alive: Boolean = false\n      ...\n}\n</pre>\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\n<pre class=\"prettyprint\">val hostObservable = \n    zoom.observable(classOf[TopologyHost], hostId)\n        .observeOn(vt.vtScheduler)\n        .doOnCompleted(hostDeleted())\n</pre>\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\n                          aliveObservable,\n                          hostObservable)\n        .filter(isHostReady)\n        .map[MidoHost](deviceUpdated)\n        .distinctUntilChanged\n</pre>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 07:34:51','2015-04-20 07:34:51','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n\r\n<pre class=\"prettyprint\">message Host {\r\n   optional UUID id = 1;\r\n   optional string name = 2;\r\n   repeated IPAddress addresses = 3;\r\n\r\n   message PortBinding {\r\n       optional UUID port_id = 1;\r\n       optional string interface_name = 2;\r\n   }\r\n   repeated PortBinding port_bindings = 4;\r\n   optional int32 flooding_proxy_weight = 5;\r\n\r\n   // Back-reference. Expected to be set from the Tunnel Zone side.\r\n   repeated UUID tunnel_zone_ids = 101;\r\n}\r\n\r\n@ZoomClass(clazz = classOf[TopologyHost])\r\nclass MidoHost extends ZoomObject with Device {\r\n    @ZoomField(name = \"id\", converter = classOf[UUIDConverter])\r\n    var id: UUID = _\r\n\r\n    @ZoomField(name = \"port_bindings\",\r\n               converter = classOf[PortBindingConverter])\r\n    var portBindings = Map.empty[UUID, String]\r\n\r\n    @ZoomField(name = \"tunnel_zone_ids\", converter = classOf[UUIDConverter])\r\n    var tunnelZoneIds = Set.empty[UUID]\r\n\r\n    // The IP address of the host in each one of the tunnel zones\r\n    // (this is stored outside of the host proto).\r\n    var tunnelZones = Map.empty[UUID, IPAddr]\r\n    // The alive status of the host (this is stored outside of the host proto).\r\n    var alive: Boolean = false\r\n      ...\r\n}\r\n</pre>\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\n<pre class=\"prettyprint\">val hostObservable = \r\n    zoom.observable(classOf[TopologyHost], hostId)\r\n        .observeOn(vt.vtScheduler)\r\n        .doOnCompleted(hostDeleted())\r\n</pre>\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n<pre class=\"prettyprint\">val observable: Observable[MidoHost] =\r\n    Observable.merge[Any](Observable.merge(tunnelZonesSubject),\r\n                          aliveObservable,\r\n                          hostObservable)\r\n        .filter(isHostReady)\r\n        .map[MidoHost](deviceUpdated)\r\n        .distinctUntilChanged\r\n</pre>\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(316,14,'2015-04-20 08:51:14','2015-04-20 08:51:14','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\n<pre class=\"prettyprint\">/**\n* A map function that creates the host simulation\n* device from the current host, tunnel-zones and\n* alive information.\n*/\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\n   log.debug(\"Processing and creating host {} device\", hostId)\n   // Ensure that we are executing on the correct thread.\n   assertThread()\n\n   // Compute the tunnel zones to IP mapping for this host.\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\n           case None =&gt;\n       }\n   }\n\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\n   host.alive = alive.get\n   host.tunnelZones = tunnelZoneIps.toMap\n   host\n})\n</pre>\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 08:51:14','2015-04-20 08:51:14','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\n\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\n<pre class=\"prettyprint\">/**\r\n* A map function that creates the host simulation\r\n* device from the current host, tunnel-zones and\r\n* alive information.\r\n*/\r\nprivate val deviceUpdated = makeFunc1((update: Any) =&gt; {\r\n   log.debug(\"Processing and creating host {} device\", hostId)\r\n   // Ensure that we are executing on the correct thread.\r\n   assertThread()\r\n\r\n   // Compute the tunnel zones to IP mapping for this host.\r\n   val tunnelZoneIps = new mutable.HashMap[UUID, IPAddr]()\r\n   for ((tunnelZoneId, tunnelZoneState)  tunnelZoneIps += tunnelZoneId -&gt; addr\r\n           case None =&gt;\r\n       }\r\n   }\r\n\r\n   val host = ZoomConvert.fromProto(currentHost, classOf[SimulationHost])\r\n   host.alive = alive.get\r\n   host.tunnelZones = tunnelZoneIps.toMap\r\n   host\r\n})\r\n</pre>\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(318,14,'2015-04-20 08:58:37','2015-04-20 08:58:37','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-midohostobservable\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-midohostdeviceupdated\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 08:58:37','2015-04-20 08:58:37','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\n\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-midohostobservable\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-midohostdeviceupdated\r\n\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(319,14,'2015-04-20 09:02:30','2015-04-20 09:02:30','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-midohostobservable\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-midohostdeviceupdated\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 09:02:30','2015-04-20 09:02:30','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-midohostobservable\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-midohostdeviceupdated\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0);
INSERT INTO `wp_posts` VALUES (320,14,'2015-04-20 09:09:45','2015-04-20 09:09:45','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-midohostdeviceupdated\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 09:09:45','2015-04-20 09:09:45','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-midohostdeviceupdated\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(321,14,'2015-04-20 09:13:13','2015-04-20 09:13:13','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\n\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 09:13:13','2015-04-20 09:13:13','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\r\n\r\n<p class=\"prettyprint\">This method converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>and it sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream). The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(322,14,'2015-04-20 09:15:25','2015-04-20 09:15:25','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\n\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>. The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 09:15:25','2015-04-20 09:15:25','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\r\n\r\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert </i>. The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(323,14,'2015-04-20 09:16:47','2015-04-20 09:16:47','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\n\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-04-20 09:16:47','2015-04-20 09:16:47','Midonet 2.0 includes a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\r\n\r\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(326,8,'2015-04-29 22:20:54','2015-04-29 22:20:54','The MidoNet project is pleased to announce the release of MidoNet 2015.03.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.03 release','','publish','open','open','','midonet-2015-03-release','','','2015-04-29 22:25:23','2015-04-29 22:25:23','The MidoNet project is pleased to announce the release of MidoNet 2015.03.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',0,'http://blog.midonet.org/?p=326',0,'post','',0),(327,8,'2015-04-29 05:42:29','2015-04-29 05:42:29','','13929071811_7f2d8e4d72_z','','inherit','open','open','','13929071811_7f2d8e4d72_z','','','2015-04-29 05:42:29','2015-04-29 05:42:29','',326,'http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z.jpg',0,'attachment','image/jpeg',0),(328,8,'2015-04-29 05:48:20','2015-04-29 05:48:20','<a href=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z.jpg\"><img class=\"alignnone  wp-image-327\" src=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z-300x200.jpg\" alt=\"13929071811_7f2d8e4d72_z\" width=\"450\" height=\"300\" /></a>\n\n<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.03.\n\n</div>\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.03 release','','inherit','open','open','','326-revision-v1','','','2015-04-29 05:48:20','2015-04-29 05:48:20','<a href=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z.jpg\"><img class=\"alignnone  wp-image-327\" src=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z-300x200.jpg\" alt=\"13929071811_7f2d8e4d72_z\" width=\"450\" height=\"300\" /></a>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2015.03.\r\n\r\n</div>\r\n&nbsp;\r\n\r\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute manpower to the team.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',326,'http://blog.midonet.org/326-revision-v1/',0,'revision','',0),(329,8,'2015-04-29 05:50:09','2015-04-29 05:50:09','<a href=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z.jpg\"><img class=\"alignnone  wp-image-327\" src=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z-300x200.jpg\" alt=\"13929071811_7f2d8e4d72_z\" width=\"450\" height=\"300\" /></a>\n\n<div>\n<div>\n<div>\n<div>\n<div>\n\nThe MidoNet project is pleased to announce the release of MidoNet 2015.03.\n\n</div>\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.03 release','','inherit','open','open','','326-revision-v1','','','2015-04-29 05:50:09','2015-04-29 05:50:09','<a href=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z.jpg\"><img class=\"alignnone  wp-image-327\" src=\"http://blog.midonet.org/wp-content/uploads/2015/04/13929071811_7f2d8e4d72_z-300x200.jpg\" alt=\"13929071811_7f2d8e4d72_z\" width=\"450\" height=\"300\" /></a>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n\r\nThe MidoNet project is pleased to announce the release of MidoNet 2015.03.\r\n\r\n</div>\r\n&nbsp;\r\n\r\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',326,'http://blog.midonet.org/326-revision-v1/',0,'revision','',0),(330,2,'2015-04-29 18:03:00','2015-04-29 18:03:00','The MidoNet project is pleased to announce the release of MidoNet 2015.03.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.03 release','','inherit','open','open','','326-revision-v1','','','2015-04-29 18:03:00','2015-04-29 18:03:00','The MidoNet project is pleased to announce the release of MidoNet 2015.03.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.',326,'http://blog.midonet.org/326-revision-v1/',0,'revision','',0),(331,8,'2015-04-29 22:29:33','2015-04-29 22:29:33','The MidoNet project is pleased to announce the release of MidoNet 2015.03.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"http://wiki.midonet.org/ReleaseNotes2015.03\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"https://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"http://wiki.midonet.org/How%20to%20contribute\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.03 release','','inherit','open','open','','326-autosave-v1','','','2015-04-29 22:29:33','2015-04-29 22:29:33','',326,'http://blog.midonet.org/326-autosave-v1/',0,'revision','',0),(333,14,'2015-05-06 09:38:50','2015-05-06 09:38:50','In its upcoming version, Midonet will include a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\n\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-autosave-v1','','','2015-05-06 09:38:50','2015-05-06 09:38:50','',293,'http://blog.midonet.org/293-autosave-v1/',0,'revision','',0),(335,14,'2015-05-06 09:39:00','2015-05-06 09:39:00','In its upcoming version, Midonet will include a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\n\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\n\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\n\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\n\n<h2>Using Zoom</h2>\n\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\n\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\n\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\n\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\n\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\n\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\n\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\n\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\n\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\n\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\n\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\n\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\n\nZoom is a general-purpose object store and offers:\n\n<ul>\n    <li>High-availability</li>\n    <li>Read/Write operations and transactions</li>\n    <li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\n    <li>ReactiveX stream of updates for the objects it stores</li>\n    <li>Caching</li>\n</ul>\n\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>','Zoom: Reactive Programming with Zookeeper','','inherit','open','open','','293-revision-v1','','','2015-05-06 09:39:00','2015-05-06 09:39:00','In its upcoming version, Midonet will include a new storage system that we call Zoom. Zoom is a general purpose storage system for objects built on top of Zookeeper. It supports read/write operations, transactions, and referential integrity (e.g., the reference to an object is automatically cleared when that object is deleted). Zoom also offers the ability to obtain streams of updates for the objects it stores.\r\n<p style=\"text-align: justify\">Midonet uses Zoom to store data such as the virtual network topology and configuration information. Agents consume streams of updates of the network topology and perform network simulation based on the latest received network state.</p>\r\n<p style=\"text-align: justify\">Streams are built on top of <a href=\"http://www.reactivex.io\">ReactiveX</a>. Whenever a client, <i>an observer</i> in ReactiveX terminology, subscribes to a stream, the client starts receiving notifications for every update to the object corresponding to this stream. The client is also notified when the object is deleted (with an <i>onComplete</i> notification) and when an exception is raised.</p>\r\n<p style=\"text-align: justify\">ReactiveX lets us easily combine streams of <i>raw</i> devices coming straight from Zoom (e.g., bridges, routers, ports etc...) into streams of more complex network device required to perform network simulation. Streams are cached and as a consequence subscribers to a stream will obtain the last cached state of a raw device upon subscription (if the stream was opened previously). As a consequence, the number of reads issued to Zookeeper is reduced.</p>\r\n\r\n<h2>Using Zoom</h2>\r\nWe explain Zoom in more detail by illustrating how a host object is built. Objects in Zoom are stored as <a href=\"https://developers.google.com/protocol-buffers/\">protocol-buffers</a>. Serialization and deserialization of zoom objects is carried out in a <i>declarative manner</i> by tagging fields of the corresponding object class and specifying to which fields of the protocol-buffer they correspond. Here is a snippet of the host protocol-buffer and the host class:\r\n\r\nhttps://gist.github.com/nschiper/cee0e70722604fc3faee#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The class MidoHost is tagged with @<i>ZoomClass</i> to indicate that an object of this class can automatically be created from an object of class <i>TopologyHost</i>, the class generated from the host protocol-buffer. Each field of MidoHost that corresponds to a protocol buffer field is tagged with <i>@ZoomField</i>. An optional converter can be specified to indicate how the conversion is performed. We explain below how the remaining fields of a host are set, namely <em>tunnelZones</em> and <em>alive</em>.</p>\r\n<p style=\"text-align: justify\">Using Zoom’s API, we obtain a stream of updates for the host object as follows:</p>\r\nhttps://gist.github.com/nschiper/8d205360f6ac954f04cb#file-gistfile1-scala\r\n<p style=\"text-align: justify\">The <i>observeOn</i> method specifies on which thread updates are handled (so that methods accessing common data structures are scheduled on the same thread) and <i>doOnCompleted</i> allows us to pass a method to be execute when the host is deleted. These methods are part of the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">ReactiveX API</a>.</p>\r\n<p class=\"prettyprint\">To build the host object we merge the host stream, a stream indicating whether the host is alive or not, and the stream of tunnel zones the host is a member of. Each box in the illustration below corresponds to a method acting on update notifications:</p>\r\n<p class=\"prettyprint\"><img class=\" aligncenter\" src=\"https://docs.google.com/a/midokura.com/drawings/d/srFgMXD77v40aSJJ5Ng3g2g/image?w=531&amp;h=228&amp;rev=581&amp;ac=1\" alt=\"\" width=\"531px;\" height=\"228px;\" /></p>\r\n<p class=\"prettyprint\">We combine these streams as follows in Scala:</p>\r\n\r\nhttps://gist.github.com/nschiper/e0cfdbd5f5a8a98f636b#file-gistfile1-scala\r\n\r\n<p style=\"text-align: justify\">The call to <i>filter</i> lets updates pass through only when all needed information has been gathered, namely the host object and its alive information as well as all needed tunnel zones. Depending on the update type, different actions are carried out. Among others, we handle tunnel zones that the host joined or left by respectively subscribing to and unsubscribing from these tunnel zones (not shown in this example). When all the needed information has been received, the <i>deviceUpdated </i>method is called:</p>\r\n\r\nhttps://gist.github.com/nschiper/57e63f36b9dcf3d9ca17#file-gistfile1-scala\r\n\r\n<p class=\"prettyprint\">This method sets the host’s IP in each one of its tunnel zones (<i>tunnelZones </i>is a map of identifiers and tunnel zones that is populated using the tunnel zone stream) and it converts the host protocol-buffer into the corresponding host simulation object using <i>ZoomConvert</i>. The alive status of the host is set and the method returns the host simulation object.</p>\r\n\r\n<h2 class=\"prettyprint\">Zoom Benefits</h2>\r\nZoom is a general-purpose object store and offers:\r\n<ul>\r\n	<li>High-availability</li>\r\n	<li>Read/Write operations and transactions</li>\r\n	<li>Automatic conversion of protocol-buffers using <i>ZoomConvert</i></li>\r\n	<li>ReactiveX stream of updates for the objects it stores</li>\r\n	<li>Caching</li>\r\n</ul>\r\n<p class=\"prettyprint\" style=\"text-align: justify\">We showed how we use Zoom in Midonet to combine several streams to build more complex objects needed for network simulation. We plan on releasing Zoom separately and hope it will be useful to the open-source community!</p>',293,'http://blog.midonet.org/293-revision-v1/',0,'revision','',0),(345,15,'2015-06-08 03:09:40','2015-06-08 03:09:40','<p style=\"text-align: justify;\">This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more--></p>\n\n<h2 style=\"text-align: justify;\">About myself</h2>\n\n<p style=\"text-align: justify;\">In case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.</p>\n\n<p style=\"text-align: justify;\">Geek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me: I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!</p>\n\n<p style=\"text-align: justify;\">Looking forward to meeting and working with you either on- or offline!</p>','New MidoNet Community Manager: Sandro Mathys','','publish','open','open','','new-midonet-community-manager-sandro-mathys','','','2015-06-08 03:09:40','2015-06-08 03:09:40','<p style=\"text-align: justify;\">This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more--></p>\r\n\r\n<h2 style=\"text-align: justify;\">About myself</h2>\r\n<p style=\"text-align: justify;\">In case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.</p>\r\n<p style=\"text-align: justify;\">Geek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me: I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!</p>\r\n<p style=\"text-align: justify;\">Looking forward to meeting and working with you either on- or offline!</p>',0,'http://blog.midonet.org/?p=345',0,'post','',0),(348,15,'2015-06-08 00:49:07','2015-06-08 00:49:07','This is long overdue, but here we go. My name is Sandro Mathys and, since April 20, I\'m the MidoNet Community Manager at project-sponsor Midokura. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration to the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to any\n\nIn case you\'re wondering, I\'m a 30-year old Swiss based in central Tokyo, Japan.','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 00:49:07','2015-06-08 00:49:07','This is long overdue, but here we go. My name is Sandro Mathys and, since April 20, I\'m the MidoNet Community Manager at project-sponsor Midokura. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration to the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to any\r\n\r\nIn case you\'re wondering, I\'m a 30-year old Swiss based in central Tokyo, Japan.',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(349,15,'2015-06-08 01:38:03','2015-06-08 01:38:03','This self-introduction is long overdue, but here we go! My name is Sandro Mathys and I\'m the new MidoNet Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration to the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count!\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Geek by day and night, I consider myself a digital citizen. Professionally, I\'ve worked both as a Java Software Engineer and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nIf you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, +SandroMathys on Google+, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and red on MidoNet Slack.','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 01:38:03','2015-06-08 01:38:03','This self-introduction is long overdue, but here we go! My name is Sandro Mathys and I\'m the new MidoNet Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration to the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count!\r\n\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Geek by day and night, I consider myself a digital citizen. Professionally, I\'ve worked both as a Java Software Engineer and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nIf you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, +SandroMathys on Google+, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and red on MidoNet Slack.',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(350,15,'2015-06-08 01:52:04','2015-06-08 01:52:04','This self-introduction is long overdue, but here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software Engineer and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 01:52:04','2015-06-08 01:52:04','This self-introduction is long overdue, but here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!\r\n\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software Engineer and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(351,15,'2015-06-08 02:11:32','2015-06-08 02:11:32','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software Engineer and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:11:32','2015-06-08 02:11:32','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!\r\n\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software Engineer and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(352,15,'2015-06-08 02:14:19','2015-06-08 02:14:19','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:14:19','2015-06-08 02:14:19','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!\r\n\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(353,15,'2015-06-08 02:19:49','2015-06-08 02:19:49','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:19:49','2015-06-08 02:19:49','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(354,15,'2015-06-08 02:28:23','2015-06-08 02:28:23','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\n\nLooking forward to','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:28:23','2015-06-08 02:28:23','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\r\n\r\nLooking forward to',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(355,15,'2015-06-08 02:29:50','2015-06-08 02:29:50','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\n\nLooking forward to meeting and working with you either on- or offline!','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:29:50','2015-06-08 02:29:50','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. Tasked with fostering the community, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything, I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\r\n\r\nLooking forward to meeting and working with you either on- or offline!',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(356,15,'2015-06-08 02:34:52','2015-06-08 02:34:52','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! To do so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\n\nLooking forward to meeting and working with you either on- or offline!','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:34:52','2015-06-08 02:34:52','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! To do so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past, including working on an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\r\n\r\nLooking forward to meeting and working with you either on- or offline!',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(357,15,'2015-06-08 02:37:04','2015-06-08 02:37:04','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! To do so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. I\'ve participated in an OpenStack proof of concept project in the latter role. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\n\nLooking forward to meeting and working with you either on- or offline!','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:37:04','2015-06-08 02:37:04','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! To do so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. I\'ve participated in an OpenStack proof of concept project in the latter role. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too.\r\n\r\nLooking forward to meeting and working with you either on- or offline!',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(358,15,'2015-06-08 02:40:07','2015-06-08 02:40:07','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!\n\nLooking forward to meeting and working with you either on- or offline!','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:40:07','2015-06-08 02:40:07','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a> - and yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!\r\n\r\nLooking forward to meeting and working with you either on- or offline!',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(359,15,'2015-06-08 02:40:28','2015-06-08 02:40:28','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!\n\nLooking forward to meeting and working with you either on- or offline!','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:40:28','2015-06-08 02:40:28','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me, I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!\r\n\r\nLooking forward to meeting and working with you either on- or offline!',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(360,15,'2015-06-08 02:40:45','2015-06-08 02:40:45','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\n\n<h2>About myself</h2>\n\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\n\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me: I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!\n\nLooking forward to meeting and working with you either on- or offline!','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:40:45','2015-06-08 02:40:45','This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more-->\r\n<h2>About myself</h2>\r\nIn case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.\r\n\r\nGeek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me: I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!\r\n\r\nLooking forward to meeting and working with you either on- or offline!',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(361,15,'2015-06-08 02:42:22','2015-06-08 02:42:22','<p style=\"text-align: justify;\">This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more--></p>\n\n<h2 style=\"text-align: justify;\">About myself</h2>\n\n<p style=\"text-align: justify;\">In case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.</p>\n\n<p style=\"text-align: justify;\">Geek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me: I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!</p>\n\n<p style=\"text-align: justify;\">Looking forward to meeting and working with you either on- or offline!</p>','New MidoNet Community Manager: Sandro Mathys','','inherit','open','open','','345-revision-v1','','','2015-06-08 02:42:22','2015-06-08 02:42:22','<p style=\"text-align: justify;\">This self-introduction is long overdue, so here we go! My name is Sandro Mathys and I\'m the new <a href=\"http://www.midonet.org/\">MidoNet</a> Community Manager at project-sponsor <a href=\"http://www.midokura.com/\" target=\"_blank\">Midokura</a>. As such I\'m exclusively tasked with fostering our young community! Doing so, I keep myself busy spreading the word about MidoNet, and facilitating collaboration in the project among many other things. Frankly, I\'m currently <del>somewhat</del> the community\'s maid of all work (that no other contributor is picking up). That said, please don\'t expect that I\'m an expert in everything or even just equally good at everything. I\'m most certainly not - and that\'s why we need YOU! Looking forward to collaborating with all of you, even the smallest <a href=\"http://wiki.midonet.org/HowToContribute\" target=\"_blank\">contributions</a> count. See you on <a href=\"https://slack.midonet.org/\">MidoNet Slack</a> or on one of our <a href=\"http://lists.midonet.org/\">mailing lists</a>!<!--more--></p>\r\n\r\n<h2 style=\"text-align: justify;\">About myself</h2>\r\n<p style=\"text-align: justify;\">In case you\'re wondering about myself, I\'m a 30-year old Swiss based in central Tokyo, Japan. Professionally, I\'ve worked both as a Java Software and a Linux Systems Engineer in the past. In the latter role, I\'ve first come in touch with clouds and even participated in an OpenStack proof of concept project. Furthermore, I\'m a long-standing contributor to the <a href=\"https://www.fedoraproject.org/\" target=\"_blank\">Fedora</a> and <a href=\"https://www.rdoproject.org/\" target=\"_blank\">RDO</a> projects, and have been involved with the <a href=\"http://www.openstack.org/\" target=\"_blank\">OpenStack</a> and <a href=\"https://wiki.openstack.org/wiki/Puppet\" target=\"_blank\">Puppet OpenStack</a> communities. For more details about my career, check out my <a href=\"https://jp.linkedin.com/in/sandromathys\" target=\"_blank\">LinkedIn profile</a> or my <a href=\"http://sandro.mathys.io/\" target=\"_blank\">online resume</a>.</p>\r\n<p style=\"text-align: justify;\">Geek by day and night, I consider myself a digital citizen. When I don\'t do any of the above or other tech-geeky things like playing video games, I enjoy travelling, photography, cooking and hiking a lot. If you\'d like to follow or reach out to me: I\'m <a href=\"https://twitter.com/red_trela\" target=\"_blank\">@red_trela on Twitter</a>, <a href=\"https://www.google.com/+SandroMathys\">+SandroMathys on Google+</a>, <a href=\"https://github.com/red-trela\" target=\"_blank\">red-trela on GitHub</a>, and <a href=\"https://midonet.slack.com/team/red\">red on MidoNet Slack</a>. And yes, it\'s totally okay to call me <em>red</em>, in case you didn\'t get the hint! Hopefully, my <a href=\"http://blog.mathys.io/\">personal blog</a> as well as my <a href=\"http://photography.mathys.io/\">picture gallery</a> will see more love again soon, too. Of course, you can also simply drop me an old-fashioned <a href=\"mailto:sandro@midokura.com\">email</a>!</p>\r\n<p style=\"text-align: justify;\">Looking forward to meeting and working with you either on- or offline!</p>',345,'http://blog.midonet.org/345-revision-v1/',0,'revision','',0),(368,20,'2015-06-22 09:50:25','2015-06-22 09:50:25','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n<img class=\"alignnone wp-image-429 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_bridged.png\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" width=\"623\" height=\"620\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\"><img class=\"alignnone size-full wp-image-430\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\" alt=\"depiction of the container based configuration, like Kubernetes pods\" width=\"623\" height=\"620\" /></a>\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\"><img class=\"alignnone size-full wp-image-431\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\" alt=\"depiction of the a typical flannel configuration\" width=\"721\" height=\"625\" /></a>\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\"><img class=\"alignnone size-full wp-image-432\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\" alt=\"the evented design shows how the actions originate in dockerd and midockerd reacts to those\" width=\"694\" height=\"300\" /></a>\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','publish','open','open','','docker-networking-midonet','','https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\nhttps://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/','2015-07-09 17:41:24','2015-07-09 17:41:24','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n<img class=\"alignnone wp-image-429 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_bridged.png\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" width=\"623\" height=\"620\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\"><img class=\"alignnone size-full wp-image-430\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\" alt=\"depiction of the container based configuration, like Kubernetes pods\" width=\"623\" height=\"620\" /></a>\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\"><img class=\"alignnone size-full wp-image-431\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\" alt=\"depiction of the a typical flannel configuration\" width=\"721\" height=\"625\" /></a>\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Prior to libnetwork</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\"><img class=\"alignnone size-full wp-image-432\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\" alt=\"the evented design shows how the actions originate in dockerd and midockerd reacts to those\" width=\"694\" height=\"300\" /></a>\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>With libnetwork</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nJoin the conversation. Talk with developers, get help, and contribute.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\r\n</ul>',0,'http://blog.midonet.org/?p=368',0,'post','',4),(369,1,'2015-06-19 23:36:16','2015-06-19 23:36:16','','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-19 23:36:16','2015-06-19 23:36:16','',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(371,1,'2015-06-20 21:48:43','2015-06-20 21:48:43','##Cans and Some String\n<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Atoni Puimedon</a>\n####Introduction\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n####Current State of Affairs\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n#####Docker Networking\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n#####<code>Flannel</code> and <code>Weave</code>\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n####<code>libnetwork</code> (*vendor certification not required)\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n####MidoNet and the <code>libnetwork</code> Future\n#####MidoNet\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n#####Now\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling    </li>\n<li>Load balancing                                            </li>\n<li>Cross host networking                                             </li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n####Contribute!\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a> </li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-20 21:48:43','2015-06-20 21:48:43','##Cans and Some String\r\n![The little daemon that could][midockerd]\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Atoni Puimedon](mailto:toni@midokura.com)\r\n####Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n####Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n#####Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n#####`Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n####`libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n####MidoNet and the `libnetwork` Future\r\n#####MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n - Lightweight. No need for complex listeners or loops\r\n - Simple. Uses the same tools and cli as normal `docker`\r\n - Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n - Reactionary event driven mechanism\r\n - One way street. No native mechanism for container awareness of network conditions\r\n - Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n#####Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n - Tunneling    \r\n - Load balancing                                            \r\n - Cross host networking                                             \r\n - And many more\r\n\r\n>There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n>[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n####Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n - [`libnetwork` on GitHub][libnetwork-repo]\r\n - [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n - `#docker-dev` and `#docker-networking` irc channels on freenode\r\n - MidoNet on [Slack][midonet-slack] \r\n\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet\r\n',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(372,1,'2015-06-20 21:53:34','2015-06-20 21:53:34','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Atoni Puimedon</a>\n####Introduction\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n####Current State of Affairs\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n#####Docker Networking\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n#####<code>Flannel</code> and <code>Weave</code>\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n####<code>libnetwork</code> (*vendor certification not required)\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n####MidoNet and the <code>libnetwork</code> Future\n#####MidoNet\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n#####Now\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling    </li>\n<li>Load balancing                                            </li>\n<li>Cross host networking                                             </li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n####Contribute!\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a> </li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-20 21:53:34','2015-06-20 21:53:34','![The little daemon that could][midockerd]\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Atoni Puimedon](mailto:toni@midokura.com)\r\n####Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n####Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n#####Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n#####`Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n####`libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n####MidoNet and the `libnetwork` Future\r\n#####MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n - Lightweight. No need for complex listeners or loops\r\n - Simple. Uses the same tools and cli as normal `docker`\r\n - Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n - Reactionary event driven mechanism\r\n - One way street. No native mechanism for container awareness of network conditions\r\n - Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n#####Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n - Tunneling    \r\n - Load balancing                                            \r\n - Cross host networking                                             \r\n - And many more\r\n\r\n>There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n>[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n####Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n - [`libnetwork` on GitHub][libnetwork-repo]\r\n - [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n - `#docker-dev` and `#docker-networking` irc channels on freenode\r\n - MidoNet on [Slack][midonet-slack] \r\n\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet\r\n',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(373,1,'2015-06-20 21:56:16','2015-06-20 21:56:16','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Atoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-20 21:56:16','2015-06-20 21:56:16','![The little daemon that could][midockerd]\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Atoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(374,17,'2015-06-22 06:54:28','2015-06-22 06:54:28','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-autosave-v1','','','2015-06-22 06:54:28','2015-06-22 06:54:28','',368,'http://blog.midonet.org/368-autosave-v1/',0,'revision','',0),(375,17,'2015-06-21 01:09:19','2015-06-21 01:09:19','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-21 01:09:19','2015-06-21 01:09:19','![The little daemon that could][midockerd]\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. They all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(376,17,'2015-06-22 01:34:46','2015-06-22 01:34:46','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:34:46','2015-06-22 01:34:46','![The little daemon that could][midockerd]\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(377,17,'2015-06-22 01:37:40','2015-06-22 01:37:40','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" /><code>(./pic/pic1s.png =250x)</code>\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:37:40','2015-06-22 01:37:40','![The little daemon that could][midockerd]<code>(./pic/pic1s.png =250x)</code>\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(378,17,'2015-06-22 01:39:43','2015-06-22 01:39:43','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" /><code>(./midockerd =250x)</code>\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:39:43','2015-06-22 01:39:43','![The little daemon that could][midockerd]<code>(./midockerd =250x)</code>\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(379,17,'2015-06-22 01:40:05','2015-06-22 01:40:05','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:40:05','2015-06-22 01:40:05','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configuration. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(380,17,'2015-06-22 01:43:31','2015-06-22 01:43:31','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:43:31','2015-06-22 01:43:31','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar as Kubernetes\' one of Pods that host containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(381,17,'2015-06-22 01:45:22','2015-06-22 01:45:22','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes that hosts containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:45:22','2015-06-22 01:45:22','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes that hosts containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(382,17,'2015-06-22 01:47:10','2015-06-22 01:47:10','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:47:10','2015-06-22 01:47:10','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in it\'s networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(383,17,'2015-06-22 01:48:33','2015-06-22 01:48:33','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:48:33','2015-06-22 01:48:33','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace but the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(384,17,'2015-06-22 01:49:00','2015-06-22 01:49:00','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:49:00','2015-06-22 01:49:00','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(385,17,'2015-06-22 01:51:43','2015-06-22 01:51:43','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:51:43','2015-06-22 01:51:43','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nBuilding on top of the NAT bridge model, some overlay solutions started to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(386,17,'2015-06-22 01:53:56','2015-06-22 01:53:56','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:53:56','2015-06-22 01:53:56','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon start up and will communicate to docker which address space the docker Linux Bridge has and make it be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(387,17,'2015-06-22 01:58:24','2015-06-22 01:58:24','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:58:24','2015-06-22 01:58:24','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new verbs to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(388,17,'2015-06-22 01:59:54','2015-06-22 01:59:54','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 01:59:54','2015-06-22 01:59:54','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and a more responsive network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(389,17,'2015-06-22 02:00:56','2015-06-22 02:00:56','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 02:00:56','2015-06-22 02:00:56','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, decentralized, software SDN. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(390,17,'2015-06-22 02:02:03','2015-06-22 02:02:03','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the <code>libnetwork</code> family.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 02:02:03','2015-06-22 02:02:03','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member of the `libnetwork` family.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(391,17,'2015-06-22 02:04:10','2015-06-22 02:04:10','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 02:04:10','2015-06-22 02:04:10','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit for the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(392,17,'2015-06-22 02:05:48','2015-06-22 02:05:48','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 02:05:48','2015-06-22 02:05:48','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. It therefore seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(393,17,'2015-06-22 02:06:25','2015-06-22 02:06:25','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 02:06:25','2015-06-22 02:06:25','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(394,17,'2015-06-22 02:30:13','2015-06-22 02:30:13','<img src=\"http://presentation.midokura.me/random-hosted/midockerd_big.png\" alt=\"The little daemon that could\" />\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n####Container Networking\n#####Yesterday\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n###Get Started\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 02:30:13','2015-06-22 02:30:13','![The little daemon that could][midockerd]\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n####Container Networking\r\n#####Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n###Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(395,20,'2015-06-22 04:12:14','2015-06-22 04:12:14','','Logical Topology With Background','','inherit','open','open','','logical-topology-with-background','','','2015-06-22 04:12:30','2015-06-22 04:12:30','',368,'http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png',0,'attachment','image/png',0),(396,20,'2015-06-22 04:28:14','2015-06-22 04:28:14','','midockerd_big','','inherit','open','open','','midockerd_big','','','2015-06-22 04:47:23','2015-06-22 04:47:23','',368,'http://blog.midonet.org/wp-content/uploads/2015/06/midockerd_big.png',0,'attachment','image/png',0),(397,20,'2015-06-22 05:21:07','2015-06-22 05:21:07','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-autosave-v1','','','2015-06-22 05:21:07','2015-06-22 05:21:07','',368,'http://blog.midonet.org/368-autosave-v1/',0,'revision','',0),(398,20,'2015-06-22 04:33:12','2015-06-22 04:33:12','&nbsp;\n\n<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\nThat\'s it! We forgot networking!\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:33:12','2015-06-22 04:33:12','&nbsp;\r\n\r\n_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\nThat\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(399,20,'2015-06-22 04:34:41','2015-06-22 04:34:41','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (*vendor certification not required)</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:34:41','2015-06-22 04:34:41','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (*vendor certification not required)\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(400,20,'2015-06-22 04:51:41','2015-06-22 04:51:41','<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\"><img class=\"alignnone size-medium wp-image-395\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background-300x179.png\" alt=\"MidoNet Logical Topology\" width=\"300\" height=\"179\" /></a>_by <a href=\"https://github.com/timfallmk\">Tim Fall</a>_ <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\"><img class=\"alignnone size-medium wp-image-395\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background-300x179.png\" alt=\"MidoNet Logical Topology\" width=\"300\" height=\"179\" /></a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:51:41','2015-06-22 04:51:41','<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\"><img class=\"alignnone size-medium wp-image-395\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background-300x179.png\" alt=\"MidoNet Logical Topology\" width=\"300\" height=\"179\" /></a>_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\"><img class=\"alignnone size-medium wp-image-395\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background-300x179.png\" alt=\"MidoNet Logical Topology\" width=\"300\" height=\"179\" /></a>\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(401,20,'2015-06-22 04:51:57','2015-06-22 04:51:57','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\"><img class=\"alignnone size-medium wp-image-395\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background-300x179.png\" alt=\"MidoNet Logical Topology\" width=\"300\" height=\"179\" /></a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:51:57','2015-06-22 04:51:57','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\"><img class=\"alignnone size-medium wp-image-395\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background-300x179.png\" alt=\"MidoNet Logical Topology\" width=\"300\" height=\"179\" /></a>\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(402,20,'2015-06-22 04:53:06','2015-06-22 04:53:06','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:53:06','2015-06-22 04:53:06','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(403,20,'2015-06-22 04:55:46','2015-06-22 04:55:46','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n!<a href=\"http://presentation.midokura.me/random-hosted/topology.png\">midonet-topology-diagram</a>\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented PoC shows how the actions originate in dockerd and midockerd reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:55:46','2015-06-22 04:55:46','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented PoC shows how the actions originate in dockerd and midockerd reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(404,20,'2015-06-22 04:58:40','2015-06-22 04:58:40','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://presentation.midokura.me/random-hosted/topology.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 04:58:40','2015-06-22 04:58:40','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![Layout of overlay networks in MidoNet][midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented design shows how the actions originate in `dockerd` and `midockerd` reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(405,20,'2015-06-22 05:02:46','2015-06-22 05:02:46','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<strong>That\'s it! We forgot networking!</strong>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://presentation.midokura.me/random-hosted/topology.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:02:46','2015-06-22 05:02:46','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n**That\'s it! We forgot networking!**\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![Layout of overlay networks in MidoNet][midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented design shows how the actions originate in `dockerd` and `midockerd` reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(406,20,'2015-06-22 05:03:09','2015-06-22 05:03:09','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<blockquote>\n  That\'s it! We forgot networking!\n</blockquote>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://presentation.midokura.me/random-hosted/topology.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:03:09','2015-06-22 05:03:09','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n&gt; That\'s it! We forgot networking!\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![Layout of overlay networks in MidoNet][midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented design shows how the actions originate in `dockerd` and `midockerd` reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(407,20,'2015-06-22 05:04:15','2015-06-22 05:04:15','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://presentation.midokura.me/random-hosted/topology.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:04:15','2015-06-22 05:04:15','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![Layout of overlay networks in MidoNet][midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented design shows how the actions originate in `dockerd` and `midockerd` reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://presentation.midokura.me/random-hosted/topology.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(408,20,'2015-06-22 05:06:52','2015-06-22 05:06:52','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:06:52','2015-06-22 05:06:52','_by [Tim Fall]_ [tim@midokura.com](https://cotap.me/timfall) and [Antoni Puimedon](mailto:toni@midokura.com)\r\n\r\n#### Introduction\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n\r\n*That\'s it! We forgot networking!*\r\n\r\n#### Current State of Affairs\r\nWell that\'s not **quite** fair, we didn\'t _forget_ it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n\r\n##### Docker Networking\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n\r\n###### NAT Bridge\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n![depiction of NAT bridge networking that shows how veths and bridges are used to provide networking](http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg)\r\n\r\n###### Host\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, _Host_ networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n\r\n###### Container\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n![depiction of the container based configuration, like Kubernetes pods](http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg)\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n\r\n###### None\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n\r\n##### `Flannel` and `Weave`\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n![depiction of the a typical flannel configuration](http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg)\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n\r\n#### `libnetwork` (a.k.a \"*vendor training not required\")\r\n[`libnetwork`][libnetwork-blog] is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the [aquisition of SocketPlane.io](https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/), an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n`libnetwork`\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like `docker network create`. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like `weave` and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe `libnetwork` project is designed to create a framework, that will live alongside other core frameworks in Docker (`libcontainer`,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how `libnetwork` works [on the blog post][libnetwork-blog] and on the [repository][libnetwork-repo].\r\n\r\n#### MidoNet and the `libnetwork` Future\r\n##### MidoNet\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage `libnetwork`.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n![Layout of overlay networks in MidoNet][midonet-topology-diagram]\r\n\r\nFor more detailed information on MidoNet see [midonet.org][midonet-org] and [midonet.github.com](midonet.github.com).\r\n\r\n#### Container Networking\r\n##### Yesterday\r\nBefore the introduction of `libnetwork` MidoNet relied on the `docker event` interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n![the evented design shows how the actions originate in `dockerd` and `midockerd` reacts to those](http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg)\r\n\r\nPros:\r\n\r\n- Lightweight. No need for complex listeners or loops\r\n- Simple. Uses the same tools and cli as normal `docker`\r\n- Functional. Enabled complex networking without touching `docker` core\r\n\r\nCons:\r\n\r\n- Reactionary event driven mechanism\r\n- One way street. No native mechanism for container awareness of network conditions\r\n- Additional tooling. Complex network changes required the use of the `midonet` cli to edit the network directly\r\n\r\n##### Now\r\n`libnetwork` allows for a mechanism driver to provide networking functions that core `docker` functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n\r\n- Tunneling\r\n- Load balancing\r\n- Cross host networking\r\n- And many more\r\n\r\n&gt;There is also work being done on supporting cross-engine networking.\r\n\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use `libnetwork` to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n\r\n### Get Started\r\nThe current working version of MidoNet with `docker` is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n\r\n&gt;[MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere][bees]\r\n\r\n#### Contribute!\r\nBoth `libnetwork` and MidoNet are open source. You can get source from the official repositories.\r\n\r\n- [`libnetwork` on GitHub][libnetwork-repo]\r\n- [`midonet` on GitHub][midonet-repo]\r\n\r\nYou can talk with developers, get help, and make contributions here.\r\n\r\n- `#docker-dev` and `#docker-networking` irc channels on freenode\r\n- MidoNet on [Slack][midonet-slack]\r\n\r\n[libnetwork-blog]: https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\r\n[bees]: https://github.com/midonet/bees\r\n[libnetwork-repo]: https://github.com/docker/libnetwork\r\n[midonet-org]: http://midonet.org\r\n[midonet-slack]: slack.midonet.org\r\n[midockerd]: http://presentation.midokura.me/random-hosted/midockerd_big.png\r\n[Tim Fall]: https://github.com/timfallmk\r\n[midonet-topology-diagram]: http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\r\n[midonet-repo]: https://github.com/midonet',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(409,20,'2015-06-22 05:07:40','2015-06-22 05:07:40','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:07:40','2015-06-22 05:07:40','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<em>That\'s it! We forgot networking!</em>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those\" />\n\nPros:\n\n<ul>\n<li>Lightweight. No need for complex listeners or loops</li>\n<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n<li>Reactionary event driven mechanism</li>\n<li>One way street. No native mechanism for container awareness of network conditions</li>\n<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n<li>Tunneling</li>\n<li>Load balancing</li>\n<li>Cross host networking</li>\n<li>And many more</li>\n</ul>\n\n<blockquote>\n  There is also work being done on supporting cross-engine networking.\n</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote>\n  <a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a>\n</blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(411,20,'2015-06-22 05:19:17','2015-06-22 05:19:17','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:19:17','2015-06-22 05:19:17','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n<h4><code>libnetwork</code> (a.k.a \"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(412,20,'2015-06-22 05:21:05','2015-06-22 05:21:05','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 05:21:05','2015-06-22 05:21:05','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. This will cover a whole lot of the cross-host use cases, but admittedly may be insufficient for production usage patterns that may need firewalling, load balancing, etc.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(413,17,'2015-06-22 06:22:47','2015-06-22 06:22:47','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from the design limitations in Open vSwitch.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:22:47','2015-06-22 06:22:47','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from the design limitations in Open vSwitch.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(414,17,'2015-06-22 06:24:50','2015-06-22 06:24:50','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:24:50','2015-06-22 06:24:50','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(415,17,'2015-06-22 06:26:26','2015-06-22 06:26:26','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:26:26','2015-06-22 06:26:26','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networkers has been hard at work making this a possibility.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(416,17,'2015-06-22 06:31:07','2015-06-22 06:31:07','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\n\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:31:07','2015-06-22 06:31:07','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nThe emergence of libnetwork will allow vendors not to have to rely on wrapping the docker API/cli like <code>weave</code> and others do. This will ensure much better integration, simpler deployment and more responsiveness in network performance.\r\n\r\nThe <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0);
INSERT INTO `wp_posts` VALUES (417,17,'2015-06-22 06:36:43','2015-06-22 06:36:43','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:36:43','2015-06-22 06:36:43','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(418,17,'2015-06-22 06:38:38','2015-06-22 06:38:38','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:38:38','2015-06-22 06:38:38','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. It has already been integrated with a number of different projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(419,17,'2015-06-22 06:39:32','2015-06-22 06:39:32','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:39:32','2015-06-22 06:39:32','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. All data is stored in a clustered database system, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(420,17,'2015-06-22 06:43:25','2015-06-22 06:43:25','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The state data and network topology are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which is directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:43:25','2015-06-22 06:43:25','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The state data and network topology are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which is directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(421,17,'2015-06-22 06:45:04','2015-06-22 06:45:04','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Yesterday</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>Now</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:45:04','2015-06-22 06:45:04','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Yesterday</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>Now</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(422,17,'2015-06-22 06:47:18','2015-06-22 06:47:18','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nYou can talk with developers, get help, and make contributions here.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:47:18','2015-06-22 06:47:18','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Prior to libnetwork</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>With libnetwork</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a larger project to make MidoNet compatible Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get source from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nYou can talk with developers, get help, and make contributions here.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(423,17,'2015-06-22 06:53:39','2015-06-22 06:53:39','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-22 06:53:39','2015-06-22 06:53:39','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Prior to libnetwork</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>With libnetwork</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nJoin the conversation. Talk with developers, get help, and contribute.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"slack.midonet.org\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(424,15,'2015-07-09 17:40:45','2015-07-09 17:40:45','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n<img class=\"alignnone wp-image-429 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_bridged.png\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" width=\"623\" height=\"620\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\"><img class=\"alignnone size-full wp-image-430\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\" alt=\"depiction of the container based configuration, like Kubernetes pods\" width=\"623\" height=\"620\" /></a>\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\"><img class=\"alignnone size-full wp-image-431\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\" alt=\"depiction of the a typical flannel configuration\" width=\"721\" height=\"625\" /></a>\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\"><img class=\"alignnone size-full wp-image-432\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\" alt=\"the evented design shows how the actions originate in dockerd and midockerd reacts to those\" width=\"694\" height=\"300\" /></a>\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-autosave-v1','','','2015-07-09 17:40:45','2015-07-09 17:40:45','',368,'http://blog.midonet.org/368-autosave-v1/',0,'revision','',0),(425,15,'2015-06-23 16:57:02','2015-06-23 16:57:02','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-06-23 16:57:02','2015-06-23 16:57:02','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Prior to libnetwork</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"http://presentation.midokura.me/openstack_israel_2015/assets/event_midonet_poc.svg\" alt=\"the evented design shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>With libnetwork</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nJoin the conversation. Talk with developers, get help, and contribute.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(426,15,'2015-07-09 16:13:34','2015-07-09 16:13:34','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting&nbsp;containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except&nbsp;the loopback device. This&nbsp;can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make&nbsp;it&nbsp;to&nbsp;be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many&nbsp;cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch&nbsp;will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions&nbsp;to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer&nbsp;have to rely on wrapping the docker API/cli like <code>weave</code> and others do.&nbsp;The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code>&nbsp;will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source,&nbsp;distributed network virtualization&nbsp;software. The MidoNet project&nbsp;has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are&nbsp;stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to&nbsp;libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/event_midonet_poc.svg\" alt=\"the evented design&nbsp;shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With&nbsp;libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader&nbsp;project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-07-09 16:13:34','2015-07-09 16:13:34','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n\r\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/docker_bridged.svg\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/docker_container.svg\" alt=\"depiction of the container based configuration, like Kubernetes pods\" />\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting&nbsp;containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except&nbsp;the loopback device. This&nbsp;can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/flannel.svg\" alt=\"depiction of the a typical flannel configuration\" />\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make&nbsp;it&nbsp;to&nbsp;be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many&nbsp;cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch&nbsp;will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions&nbsp;to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer&nbsp;have to rely on wrapping the docker API/cli like <code>weave</code> and others do.&nbsp;The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code>&nbsp;will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source,&nbsp;distributed network virtualization&nbsp;software. The MidoNet project&nbsp;has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are&nbsp;stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Prior to&nbsp;libnetwork</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<img src=\"https://raw.githubusercontent.com/celebdor/openstack_israel_2015/master/assets/event_midonet_poc.svg\" alt=\"the evented design&nbsp;shows how the actions originate in &lt;code&gt;dockerd&lt;/code&gt; and &lt;code&gt;midockerd&lt;/code&gt; reacts to those\" />\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>With&nbsp;libnetwork</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a broader&nbsp;project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nJoin the conversation. Talk with developers, get help, and contribute.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(429,15,'2015-07-09 17:31:51','2015-07-09 17:31:51','','docker_bridged','','inherit','open','open','','docker_bridged','','','2015-07-09 17:31:51','2015-07-09 17:31:51','',368,'http://blog.midonet.org/wp-content/uploads/2015/06/docker_bridged.png',0,'attachment','image/png',0),(430,15,'2015-07-09 17:37:46','2015-07-09 17:37:46','','depiction of the container based configuration, like Kubernetes pods','','inherit','open','open','','docker_container','','','2015-07-09 17:37:53','2015-07-09 17:37:53','',368,'http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png',0,'attachment','image/png',0),(431,15,'2015-07-09 17:39:04','2015-07-09 17:39:04','','depiction of the a typical flannel configuration','','inherit','open','open','','flannel','','','2015-07-09 17:39:15','2015-07-09 17:39:15','',368,'http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png',0,'attachment','image/png',0),(432,15,'2015-07-09 17:40:29','2015-07-09 17:40:29','','the evented design shows how the actions originate in <code>dockerd</code> and <code>midockerd</code> reacts to those','','inherit','open','open','','event_midonet_poc','','','2015-07-09 17:40:35','2015-07-09 17:40:35','',368,'http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png',0,'attachment','image/png',0),(433,15,'2015-07-09 17:41:24','2015-07-09 17:41:24','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\n\n<h4>Introduction</h4>\n\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\n\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\n\n<h4>Current State of Affairs</h4>\n\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\n\n<h5>Docker Networking</h5>\n\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\n\n<h6>NAT Bridge</h6>\n\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\n\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\n<img class=\"alignnone wp-image-429 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_bridged.png\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" width=\"623\" height=\"620\" />\n\n<h6>Host</h6>\n\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\n\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\n\n<h6>Container</h6>\n\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\"><img class=\"alignnone size-full wp-image-430\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\" alt=\"depiction of the container based configuration, like Kubernetes pods\" width=\"623\" height=\"620\" /></a>\n\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\n\n<h6>None</h6>\n\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\n\n<h5><code>Flannel</code> and <code>Weave</code></h5>\n\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\"><img class=\"alignnone size-full wp-image-431\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\" alt=\"depiction of the a typical flannel configuration\" width=\"721\" height=\"625\" /></a>\n\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\n\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\n\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\n\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\n\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\n\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\n\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\n\n<h5>MidoNet</h5>\n\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\n\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\n\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\n\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\n\n<h4>Container Networking</h4>\n\n<h5>Prior to libnetwork</h5>\n\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\"><img class=\"alignnone size-full wp-image-432\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\" alt=\"the evented design shows how the actions originate in dockerd and midockerd reacts to those\" width=\"694\" height=\"300\" /></a>\n\nPros:\n\n<ul>\n    <li>Lightweight. No need for complex listeners or loops</li>\n    <li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\n    <li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\n</ul>\n\nCons:\n\n<ul>\n    <li>Reactionary event driven mechanism</li>\n    <li>One way street. No native mechanism for container awareness of network conditions</li>\n    <li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\n</ul>\n\n<h5>With libnetwork</h5>\n\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\n\n<ul>\n    <li>Tunneling</li>\n    <li>Load balancing</li>\n    <li>Cross host networking</li>\n    <li>And many more</li>\n</ul>\n\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\n\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\n\n<h3>Get Started</h3>\n\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\n\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\n\n<h4>Contribute!</h4>\n\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\n\n<ul>\n    <li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\n    <li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\n</ul>\n\nJoin the conversation. Talk with developers, get help, and contribute.\n\n<ul>\n    <li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\n    <li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\n</ul>','Docker Networking sets sail with MidoNet','','inherit','open','open','','368-revision-v1','','','2015-07-09 17:41:24','2015-07-09 17:41:24','<em>by <a href=\"https://github.com/timfallmk\">Tim Fall</a></em> <a href=\"https://cotap.me/timfall\">tim@midokura.com</a> and <a href=\"mailto:toni@midokura.com\">Antoni Puimedon</a>\r\n<h4>Introduction</h4>\r\nSo we can all agree that Docker is cool. Containers are cool, repeatability is cool, portability is cool, we\'re all cool. Everything is unicorns and rainbows. But something is missing in this fairy tale land, and it\'s something we all like to forget about. With this wide world of other containers and services out there (\"world wide web\" anyone?), we want to make use of these things and connect ourselves in.\r\n<p style=\"text-align: center;\"><em>That\'s it! We forgot networking!</em></p>\r\n\r\n<h4>Current State of Affairs</h4>\r\nWell that\'s not <strong>quite</strong> fair, we didn\'t <em>forget</em> it per say, we just all got caught up in building cool containers and plumb left it for later.\r\n<h5>Docker Networking</h5>\r\nOut of the box, docker gives you four networking modes that cover a lot of the container usages that have propelled docker into popularity. However, they all lack multi-host and advanced network capabilities but offer different degrees of Isolation. They are:\r\n<h6>NAT Bridge</h6>\r\nThis is the default networking docker option and, in this case, the default is almost invariably the setting everybody uses.\r\n\r\nIt provides network namespace isolation, communication between containers in the same host and by leveraging iptables, it allows ports in the address space of the host.\r\n<img class=\"alignnone wp-image-429 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_bridged.png\" alt=\"depiction of NAT bridge networking that shows how veths and bridges are used to provide networking\" width=\"623\" height=\"620\" />\r\n<h6>Host</h6>\r\nUnder this networking setting, the containers are spawned in the same networking namespace in which the docker daemon is running. This allows the containers to see exactly the same networking as the Host.\r\n\r\nIt is easy to see that using this mode means that you should trust the container that runs, because it is capable of negatively impacting your networking configurations. It is important to note that if one were to build containers for doing network plumbing, <em>Host</em> networking would be really useful, as one could make an image of a daemon that has dependencies and then let it work on the base network namespace.\r\n<h6>Container</h6>\r\nIt allows multiple containers to communicate with each other over the loopback device, since they share the same networking namespace.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\"><img class=\"alignnone size-full wp-image-430\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/docker_container.png\" alt=\"depiction of the container based configuration, like Kubernetes pods\" width=\"623\" height=\"620\" /></a>\r\n\r\nIf you are familiar with Kubernetes, you will notice that when two containers share the networking namespace, the picture you get is very similar to one of the Pods in Kubernetes hosting containers.\r\n<h6>None</h6>\r\nA container with nothing in its networking namespace except the loopback device. This can prove useful when having multiple non-networking container tasks that one does not want to build a namespace and infrastructure for.\r\n<h5><code>Flannel</code> and <code>Weave</code></h5>\r\nSome overlay solutions building on top of the NAT bridge model like Flannel and Weave are starting to emerge. Flannel, for instance, will set up a /24 subnetting for each host and have them be routable with each other. The packets travel between containers in different hosts thanks to the tunneling that Flannel sets up.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\"><img class=\"alignnone size-full wp-image-431\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/flannel.png\" alt=\"depiction of the a typical flannel configuration\" width=\"721\" height=\"625\" /></a>\r\n\r\nIn order to make things more convenient, Flannel prepares a configuration file for the docker daemon that is picked up by the docker daemon at start up and will communicate to docker which address space the docker Linux Bridge has and make it to be used to put the veths when launching containers.\r\n\r\nThis was a big step as it finally allowed for cross-host communication in an effective and simple way. While this will cover many cross-host use cases in Dev &amp; Test, solutions leveraging Open vSwitch will be insufficient for production deployment because they suffering from design limitations inherent in the technology.\r\n<h4><code>libnetwork</code> (\"*vendor training not required\")</h4>\r\n<a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\"><code>libnetwork</code></a> is a new project from Docker designed to bring full-fledged networking to containers and make them a first class citizen. With the <a href=\"https://blog.docker.com/2015/03/socketplane-excited-to-be-joining-docker-to-collaborate-with-networking-ecosystem/\">aquisition of SocketPlane.io</a>, an experienced team of networking experts at Docker has been hard at work making networking a reality.\r\n\r\n<code>libnetwork</code>\'s integration to Docker, which will come in the coming weeks, is backwards compatible and, at the same time, adds new actions to the docker api/cli like <code>docker network create</code>. Then, when running a container, it will be possible to specify which network should be used.\r\n\r\nWith the emergence of libnetwork, vendors no longer have to rely on wrapping the docker API/cli like <code>weave</code> and others do. The <code>libnetwork</code> project is designed to create a framework, that will live alongside other core frameworks in Docker (<code>libcontainer</code>,Compose, Machine, Registry, and Kinematic) and provide support for networking options. This will primarily take the form of a set of APIs against which people can create container-based options for a range of networking solutions. <code>libnetwork</code> will ensure much better integration, simpler deployment and more responsiveness in network performance. You can check out more about how <code>libnetwork</code> works <a href=\"https://blog.docker.com/2015/04/docker-networking-takes-a-step-in-the-right-direction-2/\">on the blog post</a> and on the <a href=\"https://github.com/docker/libnetwork\">repository</a>.\r\n<h4>MidoNet and the <code>libnetwork</code> Future</h4>\r\n<h5>MidoNet</h5>\r\nMidoNet is an open source, distributed network virtualization software. The MidoNet project has already been integrated with a number of different open source projects, including OpenStack and OPNFV. Therefore, it seemed like a natural fit to be the first member to leverage <code>libnetwork</code>.\r\n\r\nMidoNet uses an agent to manage connections between containers (intra and inter-host) and creates point to point tunnels for passage of traffic. The network topology and all systems data are stored in a clustered database system, built on open source technology like Cassandra and Zookeeper, which are directly accessible via a number of API endpoints.\r\n\r\n<img src=\"http://i0.wp.com/blog.midonet.org/wp-content/uploads/2015/06/Logical-Topology-With-Background.png\" alt=\"Layout of overlay networks in MidoNet\" />\r\n\r\nFor more detailed information on MidoNet see <a href=\"http://midonet.org\">midonet.org</a> and <a href=\"midonet.github.com\">midonet.github.com</a>.\r\n<h4>Container Networking</h4>\r\n<h5>Prior to libnetwork</h5>\r\nBefore the introduction of <code>libnetwork</code> MidoNet relied on the <code>docker event</code> interface to gather state information about running containers and to watch for new events. This approach worked, but did not provide the truly native support of a first-class citizen network.\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\"><img class=\"alignnone size-full wp-image-432\" src=\"http://blog.midonet.org/wp-content/uploads/2015/06/event_midonet_poc.png\" alt=\"the evented design shows how the actions originate in dockerd and midockerd reacts to those\" width=\"694\" height=\"300\" /></a>\r\n\r\nPros:\r\n<ul>\r\n	<li>Lightweight. No need for complex listeners or loops</li>\r\n	<li>Simple. Uses the same tools and cli as normal <code>docker</code></li>\r\n	<li>Functional. Enabled complex networking without touching <code>docker</code> core</li>\r\n</ul>\r\nCons:\r\n<ul>\r\n	<li>Reactionary event driven mechanism</li>\r\n	<li>One way street. No native mechanism for container awareness of network conditions</li>\r\n	<li>Additional tooling. Complex network changes required the use of the <code>midonet</code> cli to edit the network directly</li>\r\n</ul>\r\n<h5>With libnetwork</h5>\r\n<code>libnetwork</code> allows for a mechanism driver to provide networking functions that core <code>docker</code> functions are aware of. A \"plugin\" framework allows for direct support of networking between containers and across hosts including functions like:\r\n<ul>\r\n	<li>Tunneling</li>\r\n	<li>Load balancing</li>\r\n	<li>Cross host networking</li>\r\n	<li>And many more</li>\r\n</ul>\r\n<blockquote>There is also work being done on supporting cross-engine networking.</blockquote>\r\nMidoNet will leverage this new model to support feature-rich and highly scalable fast networking. Rather than relying on “wrapping” a new set of tools around the existing Docker API and cli, MidoNet will now use <code>libnetwork</code> to provide a network “driver” service that advertises functions directly into the other Docker components. This will expand the networking capabilities of Docker, while also substantially simplifying interaction with networking drivers.\r\n<h3>Get Started</h3>\r\nThe current working version of MidoNet with <code>docker</code> is part of a broader project to make MidoNet compatible with Docker, Swarm, Kubernetes, Mesosphere, and other distributed systems. You can find the project and instructions for how to run it on the project\'s repository.\r\n<blockquote><a href=\"https://github.com/midonet/bees\">MidoNet Bees: MidoNet for Swarm, Kubernetes, and Mesosphere</a></blockquote>\r\n<h4>Contribute!</h4>\r\nBoth <code>libnetwork</code> and MidoNet are open source. You can get the source code from the official repositories.\r\n<ul>\r\n	<li><a href=\"https://github.com/docker/libnetwork\"><code>libnetwork</code> on GitHub</a></li>\r\n	<li><a href=\"https://github.com/midonet\"><code>midonet</code> on GitHub</a></li>\r\n</ul>\r\nJoin the conversation. Talk with developers, get help, and contribute.\r\n<ul>\r\n	<li><code>#docker-dev</code> and <code>#docker-networking</code> irc channels on freenode</li>\r\n	<li>MidoNet on <a href=\"https://slack.midonet.org\" target=\"_blank\">Slack</a></li>\r\n</ul>',368,'http://blog.midonet.org/368-revision-v1/',0,'revision','',0),(442,8,'2015-07-16 13:44:53','2015-07-16 13:44:53','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of new features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\n\n</div>\n\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.06 release','','publish','open','open','','midonet-2015-06-release','','','2015-07-16 13:44:53','2015-07-16 13:44:53','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of new features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\r\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.',0,'http://blog.midonet.org/?p=442',0,'post','',0),(443,8,'2015-07-16 13:18:26','2015-07-16 13:18:26','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\n\n</div>\n\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\n</div>\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.06 release','','inherit','open','open','','442-revision-v1','','','2015-07-16 13:18:26','2015-07-16 13:18:26','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of added features and fixes issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\r\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\n</div>\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.',442,'http://blog.midonet.org/442-revision-v1/',0,'revision','',0),(444,8,'2015-07-16 13:34:39','2015-07-16 13:34:39','','Beach','','inherit','open','open','','16127176356_fe249cb6e5_z','','','2015-07-16 13:35:30','2015-07-16 13:35:30','',442,'http://blog.midonet.org/wp-content/uploads/2015/07/16127176356_fe249cb6e5_z.jpg',0,'attachment','image/jpeg',0),(445,8,'2015-07-16 13:36:01','2015-07-16 13:36:01','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of added features and fixes issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\n\n</div>\n\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.06 release','','inherit','open','open','','442-revision-v1','','','2015-07-16 13:36:01','2015-07-16 13:36:01','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of added features and fixes issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\r\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.',442,'http://blog.midonet.org/442-revision-v1/',0,'revision','',0),(446,8,'2015-07-16 13:36:05','2015-07-16 13:36:05','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\n\n<div>\n<div>\n<div>\n&nbsp;\nDetails of added features and fixes issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\nThe packages are available in the repositories (see URL in the release notes).\n&nbsp;\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n</div>\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.<br />\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\n</div>\n\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.06 release','','inherit','open','open','','442-revision-v1','','','2015-07-16 13:36:05','2015-07-16 13:36:05','<p>The MidoNet project is pleased to announce the release of MidoNet 2015.06.</p>\r\n<div>\r\n<div>\r\n<div>\r\n<p>&nbsp;</p>\r\n<p>Details of added features and fixes issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.</p>\r\n<p>The packages are available in the repositories (see URL in the release notes).</p>\r\n<p>&nbsp;</p>\r\n</div>\r\n<p>See the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.</p>\r\n</div>\r\n<p>This release introduces a change in the release cycle: we will now produce patch releases including bug fixes.<br />\r\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.</p>\r\n</div>\r\n<p>If you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;</p>\r\n<p>We would like to thank everyone who contributed to this release, great job!</p>\r\n<p>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</p>\r\n<p>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.</p>\r\n',442,'http://blog.midonet.org/442-revision-v1/',0,'revision','',0),(447,8,'2015-07-16 13:36:54','2015-07-16 13:36:54','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of added features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\n\n</div>\n\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.06 release','','inherit','open','open','','442-revision-v1','','','2015-07-16 13:36:54','2015-07-16 13:36:54','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of added features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\r\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.',442,'http://blog.midonet.org/442-revision-v1/',0,'revision','',0),(448,8,'2015-07-16 13:44:53','2015-07-16 13:44:53','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\n\n<div>\n<div>\n<div>\n\n&nbsp;\n\nDetails of new features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n&nbsp;\n\n</div>\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\n\n</div>\n\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.','MidoNet 2015.06 release','','inherit','open','open','','442-revision-v1','','','2015-07-16 13:44:53','2015-07-16 13:44:53','The MidoNet project is pleased to announce the release of MidoNet 2015.06.\r\n<div>\r\n<div>\r\n<div>\r\n\r\n&nbsp;\r\n\r\nDetails of new features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n&nbsp;\r\n\r\n</div>\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nThis release introduces a change in the release cycle: we will now produce patch releases including bug fixes.\r\nActually, the first patch release, 2015.06.1 is <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.1-Release-Notes\" target=\"_blank\">out</a>.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.',442,'http://blog.midonet.org/442-revision-v1/',0,'revision','',0),(458,12,'2015-08-04 22:00:43','2015-08-04 22:00:43','','dell-s6000','','inherit','open','open','','dell-s6000','','','2015-08-04 22:00:43','2015-08-04 22:00:43','',0,'http://blog.midonet.org/wp-content/uploads/2015/08/dell-s6000.jpg',0,'attachment','image/jpeg',0),(461,15,'2015-08-05 09:02:20','2015-08-05 09:02:20','We\'re continuously looking for presentations on topics related to MidoNet for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','publish','open','open','','continuous-call-for-papers-midonet-online-meetups','','','2015-08-05 09:02:20','2015-08-05 09:02:20','We\'re continuously looking for presentations on topics related to MidoNet for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\nWe\'re looking forward to your proposal(s)!',0,'http://blog.midonet.org/?p=461',0,'post','',0),(463,15,'2015-08-05 08:46:46','2015-08-05 08:46:46','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\n&nbsp;\n\n&nbsp;\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','inherit','open','open','','461-revision-v1','','','2015-08-05 08:46:46','2015-08-05 08:46:46','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\n&nbsp;\r\n\r\n&nbsp;\r\n\r\nWe\'re looking forward to your proposal(s)!',461,'http://blog.midonet.org/461-revision-v1/',0,'revision','',0),(464,15,'2015-08-05 08:47:18','2015-08-05 08:47:18','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','inherit','open','open','','461-revision-v1','','','2015-08-05 08:47:18','2015-08-05 08:47:18','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\nWe\'re looking forward to your proposal(s)!',461,'http://blog.midonet.org/461-revision-v1/',0,'revision','',0),(466,15,'2015-08-05 08:54:12','2015-08-05 08:54:12','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled.png\"><img class=\" wp-image-465 size-full alignnone\" src=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled.png\" alt=\"MidoNet Online Meetups: There are no upcoming Meetups. You can schedule one!\" width=\"596\" height=\"262\" /></a>\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','inherit','open','open','','461-revision-v1','','','2015-08-05 08:54:12','2015-08-05 08:54:12','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled.png\"><img class=\" wp-image-465 size-full alignnone\" src=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled.png\" alt=\"MidoNet Online Meetups: There are no upcoming Meetups. You can schedule one!\" width=\"596\" height=\"262\" /></a>\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\nWe\'re looking forward to your proposal(s)!',461,'http://blog.midonet.org/461-revision-v1/',0,'revision','',0),(467,15,'2015-08-05 08:56:30','2015-08-05 08:56:30','','no-meetups-scheduled','','inherit','open','open','','no-meetups-scheduled-2','','','2015-08-05 08:56:30','2015-08-05 08:56:30','',461,'http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled1.png',0,'attachment','image/png',0),(468,15,'2015-08-05 08:56:55','2015-08-05 08:56:55','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled1.png\"><img class=\"alignnone wp-image-467 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled1.png\" alt=\"\" width=\"560\" height=\"258\" /></a>\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','inherit','open','open','','461-revision-v1','','','2015-08-05 08:56:55','2015-08-05 08:56:55','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled1.png\"><img class=\"alignnone wp-image-467 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/08/no-meetups-scheduled1.png\" alt=\"\" width=\"560\" height=\"258\" /></a>\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\nWe\'re looking forward to your proposal(s)!',461,'http://blog.midonet.org/461-revision-v1/',0,'revision','',0),(469,15,'2015-08-05 08:58:42','2015-08-05 08:58:42','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','inherit','open','open','','461-revision-v1','','','2015-08-05 08:58:42','2015-08-05 08:58:42','We\'re continuously looking for presentations on MidoNet-related topics for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try out various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\nWe\'re looking forward to your proposal(s)!',461,'http://blog.midonet.org/461-revision-v1/',0,'revision','',0),(470,15,'2015-08-05 09:02:20','2015-08-05 09:02:20','We\'re continuously looking for presentations on topics related to MidoNet for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\n\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\n\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try various things and see what works best!\n\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\n\nWe\'re looking forward to your proposal(s)!','Continuous Call for Papers: MidoNet Online Meetups','','inherit','open','open','','461-revision-v1','','','2015-08-05 09:02:20','2015-08-05 09:02:20','We\'re continuously looking for presentations on topics related to MidoNet for our <a href=\"http://www.meetup.com/Online-MidoNet-Meetup/\" target=\"_blank\">MidoNet Online Meetups</a>!\r\n\r\nIf you would like to present, please get in touch with <a href=\"mailto:sandro@midokura.com\">Sandro Mathys</a> at your earliest convenience. Just a line or two about your (rough or early) idea is good enough to get things started, we can then work on the details and a full abstract together. Please note that we accept proposals starting immediately and that this CFP is of continuous nature, i.e. it has no due or end date. We\'ll probably host only just one presentation per meetup, but we can host as many virtual meetups as we want! Therefore, we also gladly welcome several proposals by the same speaker.\r\n\r\nOf course, the topic is up to you and as long as it\'s related to MidoNet, it\'s unlikely we\'ll turn your proposal down. Also, you\'re quite free to choose the length and form of your session as well as what audience you\'re targeting. Different approaches might work for different people, so we\'re happy to try various things and see what works best!\r\n\r\nThe medium will likely be Hangouts on Air, which we\'d augment with a chat room to allow for questions from all viewers despite Hangouts\' limit of 15 <em>active</em> participants. This way, we can have an unlimited number of watchers and still make it interactive. This will also allow for an easy transition to open discussions among all participants after the session.\r\n\r\nWe\'re looking forward to your proposal(s)!',461,'http://blog.midonet.org/461-revision-v1/',0,'revision','',0),(474,7,'2015-08-10 14:45:36','2015-08-10 14:45:36','The MidoNet project is pleased to announce the release of MidoNet 2015.06.2\n\n<div>\n\nDetails of fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.2-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n</div>\n\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.\n\n</div>','MidoNet 2015.06.2 release','','publish','open','open','','midonet-2015-06-2-release','','','2015-08-10 14:45:36','2015-08-10 14:45:36','The MidoNet project is pleased to announce the release of MidoNet 2015.06.2\r\n<div>\r\n\r\nDetails of fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.2-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n</div>\r\n\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.\r\n\r\n</div>',0,'http://blog.midonet.org/?p=474',0,'post','',0),(475,7,'2015-08-10 14:43:30','2015-08-10 14:43:30','The MidoNet project is pleased to announce the release of MidoNet 2015.06.2\n\n<div>\n\nDetails of new features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n</div>\n\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.\n\n</div>','MidoNet 2015.06.2 release','','inherit','open','open','','474-revision-v1','','','2015-08-10 14:43:30','2015-08-10 14:43:30','The MidoNet project is pleased to announce the release of MidoNet 2015.06.2\r\n<div>\r\n\r\nDetails of new features and fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n</div>\r\n\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.\r\n\r\n</div>',474,'http://blog.midonet.org/474-revision-v1/',0,'revision','',0),(476,7,'2015-08-10 14:44:58','2015-08-10 14:44:58','The MidoNet project is pleased to announce the release of MidoNet 2015.06.2\n\n<div>\n\nDetails of fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.2-Release-Notes\" target=\"_blank\">here</a>.\n\nThe packages are available in the repositories (see URL in the release notes).\n\n</div>\n\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\n\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\n\nWe would like to thank everyone who contributed to this release, great job!\n\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\n\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.\n\n</div>','MidoNet 2015.06.2 release','','inherit','open','open','','474-revision-v1','','','2015-08-10 14:44:58','2015-08-10 14:44:58','The MidoNet project is pleased to announce the release of MidoNet 2015.06.2\r\n<div>\r\n\r\nDetails of fixed issues are available <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-2015.06.2-Release-Notes\" target=\"_blank\">here</a>.\r\n\r\nThe packages are available in the repositories (see URL in the release notes).\r\n\r\n</div>\r\n\r\nSee the MidoNet <a href=\"http://docs.midonet.org/index.html\" target=\"_blank\">documentation</a> for details on installing and operating MidoNet.\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\" target=\"_blank\">Slack</a> or on the MidoNet user list &lt;<a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">midonet@lists.midonet.org</a>&gt;\r\n\r\nWe would like to thank everyone who contributed to this release, great job!\r\n\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.\r\n\r\nYou can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\" target=\"_blank\">MidoNet wiki</a>.\r\n\r\n</div>',474,'http://blog.midonet.org/474-revision-v1/',0,'revision','',0),(482,15,'2015-09-01 04:42:18','2015-09-01 04:42:18','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','publish','open','open','','announcing-midonet-mini-summit-tokyo-october-26','','','2015-10-01 18:19:06','2015-10-01 18:19:06','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',0,'http://blog.midonet.org/?p=482',0,'post','',2),(483,15,'2015-09-01 03:03:35','2015-09-01 03:03:35','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. You can even get half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!\n\n&nbsp;','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:03:35','2015-09-01 03:03:35','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. You can even get half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!\r\n\r\n&nbsp;',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(484,15,'2015-09-01 03:18:28','2015-09-01 03:18:28','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:18:28','2015-09-01 03:18:28','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(485,15,'2015-09-01 03:20:57','2015-09-01 03:20:57','','','','inherit','open','closed','','mini-summit-tokyo','','','2015-09-01 03:21:37','2015-09-01 03:21:37','',482,'http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png',0,'attachment','image/png',0),(486,15,'2015-09-01 03:21:44','2015-09-01 03:21:44','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"alignnone size-full wp-image-485\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:21:44','2015-09-01 03:21:44','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"alignnone size-full wp-image-485\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(487,15,'2015-09-01 03:22:28','2015-09-01 03:22:28','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"alignnone size-full wp-image-485\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:22:28','2015-09-01 03:22:28','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"alignnone size-full wp-image-485\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(488,15,'2015-09-01 03:22:40','2015-09-01 03:22:40','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:22:40','2015-09-01 03:22:40','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(489,15,'2015-09-01 03:23:31','2015-09-01 03:23:31','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing first MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:23:31','2015-09-01 03:23:31','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(490,15,'2015-09-01 03:23:43','2015-09-01 03:23:43','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing first MidoNet Mini-Summit Tokyo (Oct 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:23:43','2015-09-01 03:23:43','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(491,15,'2015-09-01 03:27:04','2015-09-01 03:27:04','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing first MidoNet Mini-Summit Tokyo (Oct 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:27:04','2015-09-01 03:27:04','<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(492,15,'2015-09-01 03:28:38','2015-09-01 03:28:38','&nbsp;\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing first MidoNet Mini-Summit Tokyo (Oct 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:28:38','2015-09-01 03:28:38','&nbsp;\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(493,15,'2015-09-01 03:28:49','2015-09-01 03:28:49','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing first MidoNet Mini-Summit Tokyo (Oct 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 03:28:49','2015-09-01 03:28:49','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(494,15,'2015-09-01 04:41:52','2015-09-01 04:41:52','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26, 2015)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 04:41:52','2015-09-01 04:41:52','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(495,15,'2015-09-01 04:42:01','2015-09-01 04:42:01','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-09-01 04:42:01','2015-09-01 04:42:01','We\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(497,16,'2015-09-09 17:15:10','2015-09-09 17:15:10','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) that will be released in late September 2015.\n\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways managing a set of datapath ports that reside physically in the same host as the daemon and are bound to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the adequate kernel flow to direct any further matching packets to its destination. MidoNet Agents collaborate to manage some distributed state (e.g. MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved distributed state management is delegated to well-established data stores such as Apache ZooKeeper.\n\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among the MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches.  Since VTEPs store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances, a primary task of our VxLAN Gateway controller is to synchronize several data stores.  This will hopeflly give a glimpse into the rabbit hole of constraints and complexity involved, and why it makes sense to move such control functions into a different type of node: the MidoNet Cluster.\n\nOther good candidates for future Cluster services include controllers for underlay health checks or routing.  The MidoNet REST API is already implemented as a Cluster service, and we\'ll soon provide ZooKeeper as an embedded service.  Both will simplify the MidoNet installation and bootstrapping experience by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\n\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  Developers are free to implement services as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\n\nHowever heterogeneous, most services will also share basic infrastructure.  They need reliable access to the overlay configuration using the NSDB API (<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>), the ability to interact with other components and services, or the ability to expose its own metrics inside the JMX endpoint of its host Cluster daemon.   All this plumbing is provided by the MidoNet Cluster and available to all services with no added development effort.\n\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes and every service included with the OSS packages will be immediately available on each of them.  The operator has the option to selectively enable or disable any of them.  As noted above, instances of the same service running on different Cluster nodes will use their own coordination strategies in order to carry out their designated tasks, so there will be minimal operational cost for operators.\n\nAn additional feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\n\nWe will follow up with a new post demonstrating how developers can write a simple, yet fully functional MidoNet Cluster service.','Introducing MidoNet Cluster services','','publish','open','open','','introducing-midonet-cluster-services','','\nhttp://blog.midonet.org/zoom-reactive-programming-zookeeper/','2015-09-09 17:48:29','2015-09-09 17:48:29','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) that will be released in late September 2015.\r\n\r\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways managing a set of datapath ports that reside physically in the same host as the daemon and are bound to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the adequate kernel flow to direct any further matching packets to its destination. MidoNet Agents collaborate to manage some distributed state (e.g. MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved distributed state management is delegated to well-established data stores such as Apache ZooKeeper.\r\n\r\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among the MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches.  Since VTEPs store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances, a primary task of our VxLAN Gateway controller is to synchronize several data stores.  This will hopeflly give a glimpse into the rabbit hole of constraints and complexity involved, and why it makes sense to move such control functions into a different type of node: the MidoNet Cluster.\r\n\r\nOther good candidates for future Cluster services include controllers for underlay health checks or routing.  The MidoNet REST API is already implemented as a Cluster service, and we\'ll soon provide ZooKeeper as an embedded service.  Both will simplify the MidoNet installation and bootstrapping experience by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\r\n\r\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  Developers are free to implement services as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\r\n\r\nHowever heterogeneous, most services will also share basic infrastructure.  They need reliable access to the overlay configuration using the NSDB API (<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>), the ability to interact with other components and services, or the ability to expose its own metrics inside the JMX endpoint of its host Cluster daemon.   All this plumbing is provided by the MidoNet Cluster and available to all services with no added development effort.\r\n\r\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes and every service included with the OSS packages will be immediately available on each of them.  The operator has the option to selectively enable or disable any of them.  As noted above, instances of the same service running on different Cluster nodes will use their own coordination strategies in order to carry out their designated tasks, so there will be minimal operational cost for operators.\r\n\r\nAn additional feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\r\n\r\nWe will follow up with a new post demonstrating how developers can write a simple, yet fully functional MidoNet Cluster service.',0,'http://blog.midonet.org/?p=497',0,'post','',0),(498,16,'2015-09-09 13:20:34','2015-09-09 13:20:34','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) targeted for release in September 2015.\n\nThose acquainted with the MidoNet architecture <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">1</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways, taking as primary responsibility to manage a set of datapath ports that reside physically in the same host as the daemon and are connected to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the appropriate kernel flow to direct any further matching packet to its destination.  MidoNet Agents collaborate to manage some distributed state (such as MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mapppings), but in practise most of the complexity involved in these tasks is delegated to external systems: well-established distributed datastores such as Apache ZooKeeper.\n\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent.  An example appears in the synchronization of MAC and ARP tables among virtual networks and VTEP-capable switches.  These have their own mechanisms to manage state associated to devices plugged to its physical ports, as well as their own datastore (the OpenVSwitch DataBase, <a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>).  As a consequence, one of the primary responsibilities of our VxGW controller ends up being synchronozing two different datastores.  We won\'t go into the implications, but hopefully this should give a glimpse of when management functions may become complex enough that it makes sense to locate them in a different type of node, the MidoNet Cluster.\n\nThe use cases for this kind of services are numerous.  ZooKeeper itself\nis a clear example of this, which is why we will support embedding\nZooKeeper in MidoNet Cluster nodes.  Another example is our existing\nMidoNet REST API, which will also reside in the Cluster.  A nice side\neffect is that we\'ll stop depending on ZooKeeper and Tomcat as external\ndependencies, simplifying the MidoNet installation and bootstrapping.\nThe Cluster can also support other daemons taking care of functions like\nhealth checks, routing, etc.\n\nIn order to support this hegerogeneity, the MidoNet Cluster includes a\nvery thin execution framework to manage arbitrary services.  These can be\nimplemented as highly autonomous pieces of logic, which define their own\nstrategies for redundancy, failover, etc.\n\nDespite this heterogeneity most services share basic needs like reliable\naccess to the NSDB or the ability to interact with other components\nusing the MidoNet models as the common language.  To provide this basic\n\"plumbing\", the MidoNet Cluster also includes basic infrascturcture such\nas RPC mechanisms or APIs to manipulate the overlay configuration\n(ZOOM) <a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">2</a>.\n\nOn a MidoNet deployment operators install the midonet-cluster package on\nthose machines designated to run as Cluster nodes.  Every service will\nbe immediately available on each of them, but the operator has the\noption to selectively enable or disable any of them.  As noted above,\nfor each given service all the instances running on different nodes will\nuse their own coordination strategies.\n\nA nice feature of the MidoNet Cluster is the possibility to write and\ndeploy these services as add-ons.  This enables OSS contributors and\nproprietary organisations to develop new Cluster services and make them\navailable in existing MidoNet deployments.\n\nIn our next post we will demonstrate how to develop a simple, yet fully\nfunctional MidoNet Cluster service.','','','inherit','closed','closed','','497-revision-v1','','','2015-09-09 13:20:34','2015-09-09 13:20:34','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) targeted for release in September 2015.\r\n\r\nThose acquainted with the MidoNet architecture [1] will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways, taking as primary responsibility to manage a set of datapath ports that reside physically in the same host as the daemon and are connected to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the appropriate kernel flow to direct any further matching packet to its destination.  MidoNet Agents collaborate to manage some distributed state (such as MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mapppings), but in practise most of the complexity involved in these tasks is delegated to external systems: well-established distributed datastores such as Apache ZooKeeper.\r\n\r\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent.  An example appears in the synchronization of MAC and ARP tables among virtual networks and VTEP-capable switches.  These have their own mechanisms to manage state associated to devices plugged to its physical ports, as well as their own datastore (the OpenVSwitch DataBase, <a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>).  As a consequence, one of the primary responsibilities of our VxGW controller ends up being synchronozing two different datastores.  We won\'t go into the implications, but hopefully this should give a glimpse of when management functions may become complex enough that it makes sense to locate them in a different type of node, the MidoNet Cluster.\r\n\r\nThe use cases for this kind of services are numerous.  ZooKeeper itself\r\nis a clear example of this, which is why we will support embedding\r\nZooKeeper in MidoNet Cluster nodes.  Another example is our existing\r\nMidoNet REST API, which will also reside in the Cluster.  A nice side\r\neffect is that we\'ll stop depending on ZooKeeper and Tomcat as external\r\ndependencies, simplifying the MidoNet installation and bootstrapping.\r\nThe Cluster can also support other daemons taking care of functions like\r\nhealth checks, routing, etc.\r\n\r\nIn order to support this hegerogeneity, the MidoNet Cluster includes a\r\nvery thin execution framework to manage arbitrary services.  These can be\r\nimplemented as highly autonomous pieces of logic, which define their own\r\nstrategies for redundancy, failover, etc.\r\n\r\nDespite this heterogeneity most services share basic needs like reliable\r\naccess to the NSDB or the ability to interact with other components\r\nusing the MidoNet models as the common language.  To provide this basic\r\n\"plumbing\", the MidoNet Cluster also includes basic infrascturcture such\r\nas RPC mechanisms or APIs to manipulate the overlay configuration\r\n(ZOOM) [2].\r\n\r\nOn a MidoNet deployment operators install the midonet-cluster package on\r\nthose machines designated to run as Cluster nodes.  Every service will\r\nbe immediately available on each of them, but the operator has the\r\noption to selectively enable or disable any of them.  As noted above,\r\nfor each given service all the instances running on different nodes will\r\nuse their own coordination strategies.\r\n\r\nA nice feature of the MidoNet Cluster is the possibility to write and\r\ndeploy these services as add-ons.  This enables OSS contributors and\r\nproprietary organisations to develop new Cluster services and make them\r\navailable in existing MidoNet deployments.\r\n\r\nIn our next post we will demonstrate how to develop a simple, yet fully\r\nfunctional MidoNet Cluster service.\r\n\r\n[1]: http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\r\n[2]: http://blog.midonet.org/zoom-reactive-programming-zookeeper/',497,'http://blog.midonet.org/497-revision-v1/',0,'revision','',0),(499,16,'2015-09-09 13:50:05','2015-09-09 13:50:05','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) targeted for release in September 2015.\n\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways, taking as primary responsibility to manage a set of datapath ports that reside physically in the same host as the daemon and are connected to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the appropriate kernel flow to direct any further matching packet to its destination. MidoNet Agents collaborate to manage some distributed state (such as MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved in these tasks is delegated to external systems: well-established distributed datastores such as Apache ZooKeeper.\n\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches that store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances. Our VxLAN Gateway controller must effectively take responsibility for synchronizing several data stores. This should give a glimpse into the rabbit hole of constraints and complexity involved in performing this task, that justify taking these control functions into a different type of node: the MidoNet Cluster.\n\nOther good candidates for Cluster services could include controllers for underlay health checks or routing. The MidoNet REST API is now implemented as a Cluster service, and we\'ll soon provide also ZooKeeper as an embedded service. Both will simplify the MidoNet installation and bootstrapping by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\n\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  These can be implemented as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\n\nDespite this heterogeneity most services share basic needs like reliable access to the NSDB or the ability to interact with other components using the MidoNet models as the common language.  To provide this basic plumbing, the MidoNet Cluster also includes basic infrastructure such as RPC mechanisms or APIs to manipulate the overlay configuration\n(<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>) [2].\n\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes.  Every service will be immediately available on each of them, but the operator has the option to selectively enable or disable any of them.  As noted above, for each given service all the instances running on different nodes will use their own coordination strategies.\n\nA nice feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\n\nIn our next post we will demonstrate how to develop a simple, yet fully functional MidoNet Cluster service.','','','inherit','closed','closed','','497-revision-v1','','','2015-09-09 13:50:05','2015-09-09 13:50:05','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) targeted for release in September 2015.\r\n\r\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways, taking as primary responsibility to manage a set of datapath ports that reside physically in the same host as the daemon and are connected to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the appropriate kernel flow to direct any further matching packet to its destination. MidoNet Agents collaborate to manage some distributed state (such as MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved in these tasks is delegated to external systems: well-established distributed datastores such as Apache ZooKeeper.\r\n\r\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches that store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances. Our VxLAN Gateway controller must effectively take responsibility for synchronizing several data stores. This should give a glimpse into the rabbit hole of constraints and complexity involved in performing this task, that justify taking these control functions into a different type of node: the MidoNet Cluster.\r\n\r\nOther good candidates for Cluster services could include controllers for underlay health checks or routing. The MidoNet REST API is now implemented as a Cluster service, and we\'ll soon provide also ZooKeeper as an embedded service. Both will simplify the MidoNet installation and bootstrapping by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\r\n\r\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  These can be implemented as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\r\n\r\nDespite this heterogeneity most services share basic needs like reliable access to the NSDB or the ability to interact with other components using the MidoNet models as the common language.  To provide this basic plumbing, the MidoNet Cluster also includes basic infrastructure such as RPC mechanisms or APIs to manipulate the overlay configuration\r\n(<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>) [2].\r\n\r\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes.  Every service will be immediately available on each of them, but the operator has the option to selectively enable or disable any of them.  As noted above, for each given service all the instances running on different nodes will use their own coordination strategies.\r\n\r\nA nice feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\r\n\r\nIn our next post we will demonstrate how to develop a simple, yet fully functional MidoNet Cluster service.',497,'http://blog.midonet.org/497-revision-v1/',0,'revision','',0),(500,16,'2015-09-09 13:50:52','2015-09-09 13:50:52','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) targeted for release in September 2015.\n\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways, taking as primary responsibility to manage a set of datapath ports that reside physically in the same host as the daemon and are connected to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the appropriate kernel flow to direct any further matching packet to its destination. MidoNet Agents collaborate to manage some distributed state (such as MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved in these tasks is delegated to external systems: well-established distributed datastores such as Apache ZooKeeper.\n\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches that store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances. Our VxLAN Gateway controller must effectively take responsibility for synchronizing several data stores. This should give a glimpse into the rabbit hole of constraints and complexity involved in performing this task, that justify taking these control functions into a different type of node: the MidoNet Cluster.\n\nOther good candidates for Cluster services could include controllers for underlay health checks or routing. The MidoNet REST API is now implemented as a Cluster service, and we\'ll soon provide also ZooKeeper as an embedded service. Both will simplify the MidoNet installation and bootstrapping by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\n\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  These can be implemented as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\n\nDespite this heterogeneity most services share basic needs like reliable access to the NSDB or the ability to interact with other components using the MidoNet models as the common language.  To provide this basic plumbing, the MidoNet Cluster also includes basic infrastructure such as RPC mechanisms or APIs to manipulate the overlay configuration\n(<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>).\n\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes.  Every service will be immediately available on each of them, but the operator has the option to selectively enable or disable any of them.  As noted above, for each given service all the instances running on different nodes will use their own coordination strategies.\n\nA nice feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\n\nIn our next post we will demonstrate how to develop a simple, yet fully functional MidoNet Cluster service.','','','inherit','closed','closed','','497-revision-v1','','','2015-09-09 13:50:52','2015-09-09 13:50:52','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) targeted for release in September 2015.\r\n\r\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways, taking as primary responsibility to manage a set of datapath ports that reside physically in the same host as the daemon and are connected to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the appropriate kernel flow to direct any further matching packet to its destination. MidoNet Agents collaborate to manage some distributed state (such as MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved in these tasks is delegated to external systems: well-established distributed datastores such as Apache ZooKeeper.\r\n\r\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches that store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances. Our VxLAN Gateway controller must effectively take responsibility for synchronizing several data stores. This should give a glimpse into the rabbit hole of constraints and complexity involved in performing this task, that justify taking these control functions into a different type of node: the MidoNet Cluster.\r\n\r\nOther good candidates for Cluster services could include controllers for underlay health checks or routing. The MidoNet REST API is now implemented as a Cluster service, and we\'ll soon provide also ZooKeeper as an embedded service. Both will simplify the MidoNet installation and bootstrapping by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\r\n\r\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  These can be implemented as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\r\n\r\nDespite this heterogeneity most services share basic needs like reliable access to the NSDB or the ability to interact with other components using the MidoNet models as the common language.  To provide this basic plumbing, the MidoNet Cluster also includes basic infrastructure such as RPC mechanisms or APIs to manipulate the overlay configuration\r\n(<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>).\r\n\r\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes.  Every service will be immediately available on each of them, but the operator has the option to selectively enable or disable any of them.  As noted above, for each given service all the instances running on different nodes will use their own coordination strategies.\r\n\r\nA nice feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\r\n\r\nIn our next post we will demonstrate how to develop a simple, yet fully functional MidoNet Cluster service.',497,'http://blog.midonet.org/497-revision-v1/',0,'revision','',0),(501,16,'2015-09-09 17:15:10','2015-09-09 17:15:10','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) that will be released in late September 2015.\n\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways managing a set of datapath ports that reside physically in the same host as the daemon and are bound to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the adequate kernel flow to direct any further matching packets to its destination. MidoNet Agents collaborate to manage some distributed state (e.g. MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved distributed state management is delegated to well-established data stores such as Apache ZooKeeper.\n\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among the MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches.  Since VTEPs store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances, a primary task of our VxLAN Gateway controller is to synchronize several data stores.  This will hopeflly give a glimpse into the rabbit hole of constraints and complexity involved, and why it makes sense to move such control functions into a different type of node: the MidoNet Cluster.\n\nOther good candidates for future Cluster services include controllers for underlay health checks or routing.  The MidoNet REST API is already implemented as a Cluster service, and we\'ll soon provide ZooKeeper as an embedded service.  Both will simplify the MidoNet installation and bootstrapping experience by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\n\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  Developers are free to implement services as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\n\nHowever heterogeneous, most services will also share basic infrastructure.  They need reliable access to the overlay configuration using the NSDB API (<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>), the ability to interact with other components and services, or the ability to expose its own metrics inside the JMX endpoint of its host Cluster daemon.   All this plumbing is provided by the MidoNet Cluster and available to all services with no added development effort.\n\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes and every service included with the OSS packages will be immediately available on each of them.  The operator has the option to selectively enable or disable any of them.  As noted above, instances of the same service running on different Cluster nodes will use their own coordination strategies in order to carry out their designated tasks, so there will be minimal operational cost for operators.\n\nAn additional feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\n\nWe will follow up with a new post demonstrating how developers can write a simple, yet fully functional MidoNet Cluster service.','Introducing MidoNet Cluster services','','inherit','closed','closed','','497-revision-v1','','','2015-09-09 17:15:10','2015-09-09 17:15:10','In this post, we\'ll introduce the new management services included with the next version of MidoNet (5.0.0) that will be released in late September 2015.\r\n\r\nThose acquainted with the <a href=\"http://docs.midonet.org/docs/latest/reference-architecture/content/index.html\">MidoNet architecture</a> will already be familiar with the MidoNet Agent.  This daemon runs on hypervisors and gateways managing a set of datapath ports that reside physically in the same host as the daemon and are bound to virtual ports in the overlay topology.  A packet first ingressing these ports (typically from VMs) will be handed-off by the kernel to the MidoNet Agent so it can simulate the packet\'s path through the overlay topology and install the adequate kernel flow to direct any further matching packets to its destination. MidoNet Agents collaborate to manage some distributed state (e.g. MAC and ARP tables in virtual devices, Routing tables, connection tracking or NAT mappings), but in practise most of the complexity involved distributed state management is delegated to well-established data stores such as Apache ZooKeeper.\r\n\r\nCertain management functions aren\'t bound to specific physical elements co-located with the MidoNet Agent. An example appears in our implementation of VxLAN Gateways, where we need to synchronize state among the MidoNet\'s Network State DataBase (NSDB) and multiple VTEP-capable hardware switches.  Since VTEPs store state in local OpenVSwitch DataBase (<a href=\"https://tools.ietf.org/html/rfc7047\">OVSDB</a>) instances, a primary task of our VxLAN Gateway controller is to synchronize several data stores.  This will hopeflly give a glimpse into the rabbit hole of constraints and complexity involved, and why it makes sense to move such control functions into a different type of node: the MidoNet Cluster.\r\n\r\nOther good candidates for future Cluster services include controllers for underlay health checks or routing.  The MidoNet REST API is already implemented as a Cluster service, and we\'ll soon provide ZooKeeper as an embedded service.  Both will simplify the MidoNet installation and bootstrapping experience by removing the need for users to install ZooKeeper and Tomcat as external dependencies.\r\n\r\nIn order to support this heterogeneity, the MidoNet Cluster includes a very thin execution framework to manage arbitrary services.  Developers are free to implement services as highly autonomous pieces of logic, which define their own strategies for redundancy, fail over, etc.\r\n\r\nHowever heterogeneous, most services will also share basic infrastructure.  They need reliable access to the overlay configuration using the NSDB API (<a href=\"http://blog.midonet.org/zoom-reactive-programming-zookeeper/\">ZOOM</a>), the ability to interact with other components and services, or the ability to expose its own metrics inside the JMX endpoint of its host Cluster daemon.   All this plumbing is provided by the MidoNet Cluster and available to all services with no added development effort.\r\n\r\nOn a MidoNet deployment, operators install the midonet-cluster package on those machines designated to run as Cluster nodes and every service included with the OSS packages will be immediately available on each of them.  The operator has the option to selectively enable or disable any of them.  As noted above, instances of the same service running on different Cluster nodes will use their own coordination strategies in order to carry out their designated tasks, so there will be minimal operational cost for operators.\r\n\r\nAn additional feature of the MidoNet Cluster is the possibility to write and deploy these services as add-ons.  This enables OSS contributors and proprietary organisations to develop new Cluster services and make them available in existing MidoNet deployments.\r\n\r\nWe will follow up with a new post demonstrating how developers can write a simple, yet fully functional MidoNet Cluster service.',497,'http://blog.midonet.org/497-revision-v1/',0,'revision','',0),(505,15,'2015-09-29 05:24:42','2015-09-29 05:24:42','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Cody Herriges (Puppetlabs): Our journey to OpenStack with MidoNet</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n    <li>06:00 PM Networking Reception</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','publish','open','open','','announcing-the-agenda-of-the-midonet-mini-summit-tokyo','','\nhttp://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/','2015-10-08 01:28:39','2015-10-08 01:28:39','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\r\n\r\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Cody Herriges (Puppetlabs): Our journey to OpenStack with MidoNet</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n	<li>06:00 PM Networking Reception</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',0,'http://blog.midonet.org/?p=505',0,'post','',0),(506,15,'2015-09-29 05:10:32','2015-09-29 05:10:32','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\n\nWithout further due, here it is, the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.','Announcing MidoNet Mini-Summit Tokyo Agenda','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:10:32','2015-09-29 05:10:32','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\r\n\r\nWithout further due, here it is, the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(507,15,'2015-09-29 05:11:07','2015-09-29 05:11:07','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here it is, the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.','Announcing MidoNet Mini-Summit Tokyo Agenda','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:11:07','2015-09-29 05:11:07','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here it is, the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(508,15,'2015-09-29 05:14:15','2015-09-29 05:14:15','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the MidoNet Mini-Summit Tokyo agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.','Announcing MidoNet Mini-Summit Tokyo Agenda','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:14:15','2015-09-29 05:14:15','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the MidoNet Mini-Summit Tokyo agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(509,15,'2015-09-29 05:15:00','2015-09-29 05:15:00','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.','Announcing MidoNet Mini-Summit Tokyo Agenda','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:15:00','2015-09-29 05:15:00','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(510,15,'2015-09-29 05:15:54','2015-09-29 05:15:54','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo 2015','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:15:54','2015-09-29 05:15:54','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input as to what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all of the wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\">sign up for free on eventbrite</a>! There\'s also a mini-training available in the afternoon, which you can book on the same page.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(511,15,'2015-09-29 05:24:42','2015-09-29 05:24:42','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo 2015','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:24:42','2015-09-29 05:24:42','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(512,15,'2015-09-29 05:26:04','2015-09-29 05:26:04','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-09-29 05:26:04','2015-09-29 05:26:04','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Jaume Devesa (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(513,15,'2015-09-30 07:59:43','2015-09-30 07:59:43','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubiños (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-09-30 07:59:43','2015-09-30 07:59:43','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubiños (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(514,15,'2015-09-30 08:00:37','2015-09-30 08:00:37','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-09-30 08:00:37','2015-09-30 08:00:37','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(515,15,'2015-10-08 01:28:20','2015-10-08 01:28:20','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Cody Herriges (Puppetlabs): Our journey to OpenStack with MidoNet</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n    <li>06:00 PM Networking Reception</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-autosave-v1','','','2015-10-08 01:28:20','2015-10-08 01:28:20','',505,'http://blog.midonet.org/505-autosave-v1/',0,'revision','',0),(516,15,'2015-09-30 09:03:02','2015-09-30 09:03:02','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n    <li>06:00 PM Networking Reception</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-09-30 09:03:02','2015-09-30 09:03:02','You might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n	<li>06:00 PM Networking Reception</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(517,15,'2015-10-01 18:19:42','2015-10-01 18:19:42','Heads-up: the MidoNet Mini-Summit has a new name: MidoDay Tokyo 2015! Shorter and more to the point, isn\'t it? After all, we offer a full day MidoNet experience!\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\"><img class=\"size-full wp-image-518 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\" alt=\"MidoDay Tokyo 2015\" width=\"457\" height=\"214\" /></a>\n\nThe <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\">Eventbrite registration page</a> as well as the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">wiki page with the full details</a> of the event are still the same, check them out now!','MidoNet Mini-Summit is now MidoDay Tokyo 2015','','publish','open','open','','midonet-mini-summit-is-now-midoday-tokyo-2015','','','2015-10-01 18:19:42','2015-10-01 18:19:42','Heads-up: the MidoNet Mini-Summit has a new name: MidoDay Tokyo 2015! Shorter and more to the point, isn\'t it? After all, we offer a full day MidoNet experience!\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\"><img class=\"size-full wp-image-518 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\" alt=\"MidoDay Tokyo 2015\" width=\"457\" height=\"214\" /></a>\r\n\r\nThe <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\">Eventbrite registration page</a> as well as the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">wiki page with the full details</a> of the event are still the same, check them out now!',0,'http://blog.midonet.org/?p=517',0,'post','',0),(518,15,'2015-10-01 18:03:22','2015-10-01 18:03:22','','MidoDay Tokyo 2015','','inherit','open','closed','','midominisummit-tokyo','','','2015-10-01 18:03:34','2015-10-01 18:03:34','',517,'http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png',0,'attachment','image/png',0),(519,15,'2015-10-01 18:17:35','2015-10-01 18:17:35','Heads-up: the MidoNet Mini-Summit has a new name: MidoDay Tokyo 2015! Shorter and more to the point, isn\'t it? After all, we offer a full day MidoNet experience!\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\"><img class=\"size-full wp-image-518 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\" alt=\"MidoDay Tokyo 2015\" width=\"457\" height=\"214\" /></a>\n\nThe <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\">Eventbrite registration page</a> as well as the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">wiki page with the full details</a> of the event are still the same, check them out now!','MidoNet Mini-Summit is now MidoDay Tokyo 2015','','inherit','closed','closed','','517-revision-v1','','','2015-10-01 18:17:35','2015-10-01 18:17:35','Heads-up: the MidoNet Mini-Summit has a new name: MidoDay Tokyo 2015! Shorter and more to the point, isn\'t it? After all, we offer a full day MidoNet experience!\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\"><img class=\"size-full wp-image-518 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/10/midominisummit-tokyo.png\" alt=\"MidoDay Tokyo 2015\" width=\"457\" height=\"214\" /></a>\r\n\r\nThe <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\">Eventbrite registration page</a> as well as the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\">wiki page with the full details</a> of the event are still the same, check them out now!',517,'http://blog.midonet.org/517-revision-v1/',0,'revision','',0),(520,15,'2015-10-01 18:19:06','2015-10-01 18:19:06','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\n\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\n\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\n\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!','Announcing MidoNet Mini-Summit Tokyo (October 26)','','inherit','closed','closed','','482-revision-v1','','','2015-10-01 18:19:06','2015-10-01 18:19:06','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\r\n\r\nWe\'re happy to announce the first-ever MidoNet Mini-Summit! If you\'re going to the OpenStack Summit Tokyo, this is an ideal chance for you to meet fellow operators and our developers. Of course, you\'ll also learn about the current state of MidoNet, get an outlook of what to expect next, and much more. For beginners, we even offer half a day of hands-on training from our experts!\r\n\r\n<a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\"><img class=\"size-full wp-image-485 aligncenter\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nIf you\'re interested, please mark your calendar on Monday, October 26 (i.e. the day before the OpenStack Summit) and register <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tickets-18235379511\" target=\"_blank\">here</a>! Participation is free of charge, just the hands-on training costs a nominal fee of USD 50 (plus a booking fee of USD 3.74) to cover some of our costs.\r\n\r\nNote, that we\'re still in the early planning phase and that\'s why you can help shape the event by taking part in our <a href=\"http://goo.gl/forms/79ka9e2fgB\" target=\"_blank\">quick survey</a>. Also, if you have a story to share and would like to present at our Mini-Summit, don\'t hesitate to drop <a href=\"mailto:sandro@midokura.com\">Sandro</a> a line!\r\n\r\nLast but not least, if you can\'t make it to Tokyo this time, stay tuned: the MidoNet Mini-Summit is going global in 2016!',482,'http://blog.midonet.org/482-revision-v1/',0,'revision','',0),(521,15,'2015-10-01 18:19:18','2015-10-01 18:19:18','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n    <li>06:00 PM Networking Reception</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-10-01 18:19:18','2015-10-01 18:19:18','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\r\n\r\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Richard Raseley (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n	<li>06:00 PM Networking Reception</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(524,15,'2015-10-06 00:41:17','2015-10-06 00:41:17','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Cody Herriges (Puppetlabs): Community User Story (Title TBD)</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n    <li>06:00 PM Networking Reception</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-10-06 00:41:17','2015-10-06 00:41:17','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\r\n\r\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Cody Herriges (Puppetlabs): Community User Story (Title TBD)</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n	<li>06:00 PM Networking Reception</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(525,15,'2015-10-08 01:28:23','2015-10-08 01:28:23','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\n\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\n\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\n\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\n\n<ul>\n    <li>09:00 AM Registration &amp; Breakfast</li>\n    <li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\n    <li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\n    <li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\n    <li>11:00 AM Cody Herriges (Puppetlabs): Our journey to OpenStack with MidoNet</li>\n    <li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\n    <li>12:00 PM Networking Lunch</li>\n    <li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\n    <li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\n    <li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\n    <li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\n    <li>03:30 PM Coffee Break</li>\n    <li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\n    <li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\n    <li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\n    <li>06:00 PM Networking Reception</li>\n</ul>\n\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\n\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.','Announcing the Agenda of the MidoNet Mini-Summit Tokyo','','inherit','closed','closed','','505-revision-v1','','','2015-10-08 01:28:23','2015-10-08 01:28:23','[Update: MidoNet Mini-Summit has since been renamed MidoDay Tokyo 2015]\r\n\r\nYou might have heard about the first-ever <a href=\"http://blog.midonet.org/announcing-midonet-mini-summit-tokyo-october-26/\" target=\"_blank\">MidoNet Mini-Summit in Tokyo on October 26, 2015</a> - the day before OpenStack Summit Tokyo. But back when we announced it, we were still working on the agenda, and asking for input about what people would like to see. Now, we\'re delighted to announce the event\'s agenda and are happy to say that we can live up to (almost) all wishes we\'ve received!<!--more-->\r\n\r\n<a href=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\"><img class=\"aligncenter wp-image-485 size-full\" src=\"http://blog.midonet.org/wp-content/uploads/2015/09/mini-summit-tokyo.png\" alt=\"MidoNet Mini-Summit Tokyo\" width=\"300\" height=\"150\" /></a>\r\n\r\nWithout further due, here\'s the agenda of the MidoNet Mini-Summit Tokyo 2015:\r\n<ul>\r\n	<li>09:00 AM Registration &amp; Breakfast</li>\r\n	<li>09:45 AM Sandro Mathys (Midokura): Welcome &amp; Agenda</li>\r\n	<li>10:00 AM Dan Mihai Dumitriu (Midokura): Keynote: A Brief History of MidoNet</li>\r\n	<li>10:30 AM JF Joly (Midokura): MidoNet Vision &amp; Roadmap</li>\r\n	<li>11:00 AM Cody Herriges (Puppetlabs): Our journey to OpenStack with MidoNet</li>\r\n	<li>11:30 AM Chris Janiszewski (Red Hat): Walk Through a Software Defined Everything PoC (featuring MidoNet &amp; Ceph)</li>\r\n	<li>12:00 PM Networking Lunch</li>\r\n	<li>01:30 PM Toni Segura (Midokura) &amp; Carmela Rubinos (Midokura): Puppet, and Fuel, and Juju, and RDO Manager! Oh, my!</li>\r\n	<li>02:00 PM Taku Fukushima (Midokura): Container Orchestration Integration: OpenStack Kuryr &amp; Apache Mesos</li>\r\n	<li>02:30 PM Samir Ibradzic (Midokura): Operations Experience</li>\r\n	<li>03:00 PM Tomoe Sugihara (Midokura): Testing MidoNet</li>\r\n	<li>03:30 PM Coffee Break</li>\r\n	<li>04:00 PM Pino de Candia (Midokura): Technical Deep Dive into MidoNet</li>\r\n	<li>05:00 PM Mike Ford (Midokura): Troubleshooting MidoNet</li>\r\n	<li>05:30 PM Sandro Mathys (Midokura): MidoNet Community</li>\r\n	<li>06:00 PM Networking Reception</li>\r\n</ul>\r\nRead more about the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-Mini-Summit-Tokyo-2015\" target=\"_blank\">event in our wiki</a> and <a href=\"http://www.eventbrite.com/e/midonet-mini-summit-tokyo-tickets-18235379511\" target=\"_blank\">sign up for free on Eventbrite</a>! There\'s also a hands-on mini-training available in the afternoon, which you can book on the same page.\r\n\r\nAt this time, we would also encourage you to join #tokyo-mini-summit on the <a href=\"https://slack.midonet.org/\" target=\"_blank\">MidoNet Slack</a> if you\'re interested in joining us in Tokyo.',505,'http://blog.midonet.org/505-revision-v1/',0,'revision','',0),(527,15,'2015-10-15 08:42:28','2015-10-15 08:42:28','http://blog.midonet.org/wp-content/uploads/2014/11/cropped-512pxMidoMark-5466cc4e_site_icon.png','cropped-512pxMidoMark-5466cc4e_site_icon.png','','inherit','open','closed','','cropped-512pxmidomark-5466cc4e_site_icon-png','','','2015-10-15 08:42:28','2015-10-15 08:42:28','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/cropped-512pxMidoMark-5466cc4e_site_icon.png',0,'attachment','image/png',0),(528,15,'2015-10-15 08:44:09','2015-10-15 08:44:09','','','','inherit','open','closed','','cropped-512pxmidomark-5466cc4e_site_icon1-png','','','2015-10-15 09:30:46','2015-10-15 09:30:46','',0,'http://blog.midonet.org/wp-content/uploads/2014/11/cropped-512pxMidoMark-5466cc4e_site_icon1.png',0,'attachment','image/png',0),(529,19,'2015-11-03 17:00:41','2015-11-03 17:00:41','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\n\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nYou can find the slides <a href=\"http://schd.ws/hosted_files/mitakadesignsummit/4b/kuryr_tokyo.pdf\" target=\"_blank\">here</a>.\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','publish','open','open','','kuryr-midonet-networking-docker-1-9','','','2015-11-04 07:33:46','2015-11-04 07:33:46','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\r\n\r\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nYou can find the slides <a href=\"http://schd.ws/hosted_files/mitakadesignsummit/4b/kuryr_tokyo.pdf\" target=\"_blank\">here</a>.\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',0,'https://blog.midonet.org/?p=529',0,'post','',1),(531,19,'2015-11-03 05:11:32','2015-11-03 05:11:32','','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 05:11:32','2015-11-03 05:11:32','',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(533,19,'2015-11-03 17:32:31','2015-11-03 17:32:31','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\n\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-autosave-v1','','','2015-11-03 17:32:31','2015-11-03 17:32:31','',529,'https://blog.midonet.org/529-autosave-v1/',0,'revision','',0),(534,19,'2015-11-03 16:38:48','2015-11-03 16:38:48','','kuryr_logo_yellow','Kuryr\'s mascot','inherit','open','closed','','kuryr_logo_yellow','','','2015-11-03 16:39:33','2015-11-03 16:39:33','',529,'https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png',0,'attachment','image/png',0),(535,19,'2015-11-03 16:58:06','2015-11-03 16:58:06','','contribution_by_companies_reviews','','inherit','open','closed','','contribution_by_companies_reviews','','','2015-11-03 16:58:24','2015-11-03 16:58:24','',529,'https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png',0,'attachment','image/png',0),(536,19,'2015-11-03 17:13:07','2015-11-03 17:13:07','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. Initially the plugging points are storage and networking. Networking wise, the new networking stack was made following the principle of least surprise by giving the users the familiar networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:13:07','2015-11-03 17:13:07','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. Initially the plugging points are storage and networking. Networking wise, the new networking stack was made following the principle of least surprise by giving the users the familiar networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(537,19,'2015-11-03 17:16:47','2015-11-03 17:16:47','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem with plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the familiar networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:16:47','2015-11-03 17:16:47','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem with plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the familiar networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(538,19,'2015-11-03 17:22:23','2015-11-03 17:22:23','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the familiar networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:22:23','2015-11-03 17:22:23','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the familiar networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(539,19,'2015-11-03 17:23:19','2015-11-03 17:23:19','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:23:19','2015-11-03 17:23:19','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(540,19,'2015-11-03 17:23:44','2015-11-03 17:23:44','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:23:44','2015-11-03 17:23:44','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations.\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(541,19,'2015-11-03 17:24:19','2015-11-03 17:24:19','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:24:19','2015-11-03 17:24:19','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack got released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(542,19,'2015-11-03 17:25:09','2015-11-03 17:25:09','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:25:09','2015-11-03 17:25:09','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying API and for fast answers to issues encountered during development.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(543,19,'2015-11-03 17:26:34','2015-11-03 17:26:34','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\n\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:26:34','2015-11-03 17:26:34','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\r\n\r\nEven though Kuryr will not see a release until the Mitaka cycle, it has been developed by the MidoNet team and demoed both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. You will be able to see the container ports both in MidoNet and Neutron cli tools as well as in Horizon. Currently Kuryr does not yet provide an IPAM driver (since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(544,19,'2015-11-03 17:30:08','2015-11-03 17:30:08','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\n\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:30:08','2015-11-03 17:30:08','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\r\n\r\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(545,19,'2015-11-03 17:32:57','2015-11-03 17:32:57','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\n\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\n\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\n\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\n\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\n\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\n\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\n\nYou can find the slides <a href=\"http://schd.ws/hosted_files/mitakadesignsummit/4b/kuryr_tokyo.pdf\" target=\"_blank\">here</a>.\n\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>','Kuryr brings MidoNet networking to Docker 1.9','','inherit','closed','closed','','529-revision-v1','','','2015-11-03 17:32:57','2015-11-03 17:32:57','<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow.png\"><img class=\"size-medium wp-image-534 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/kuryr_logo_yellow-214x300.png\" alt=\"OpenStack kuryr logo\" width=\"214\" height=\"300\" /></a>\r\n\r\nLast week at the OpenStack Summit in Tokyo, <a href=\"https://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a>, the project in which we have been working hard the past few months was unveiled as the community solution to bring all the benefits from Neutron to the container networking world. OpenStack Kuryr is part of Neutron\'s stadium and has seen contribution from quite a few of the companies that work on Neutron.<a href=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews.png\"><img class=\"size-medium wp-image-535 aligncenter\" src=\"https://blog.midonet.org/wp-content/uploads/2015/11/contribution_by_companies_reviews-300x207.png\" alt=\"companies that contributed to Kuryr with reviews\" width=\"300\" height=\"207\" /></a>\r\n\r\nThe effort to bring Enterprise, production ready networking to the container world with Kuryr started and was enabled by the work that Docker did with refactoring its networking parts out into a pluggable external library, <a href=\"https://github.com/docker/libnetwork\" target=\"_blank\">libnetwork</a>. With the advent of 1.8 experimental, Docker finally opened its community favorite, easy to use workflow of working with containers for third parties to bring more value to its growing ecosystem. All thanks to the new plugging points in the storage and networking subsystems.\r\n\r\nNetworking wise, the new stack was made following the principle of least surprise by giving the users the already familiar Docker networking options of having no networking, sharing networking namespaces or using the good old docker0 bridged networking that we all had come to use as default. It also added two more options, overlay and remote that are taking a step into future by allowing multi host scenarios and vendor implementations respectively (vendor implementations can be single or multinode, with Kuryr providing the latter).\r\n\r\nIt is with Docker 1.9, that all the effort that the Docker, libnetwork and plugin developers have poured into enhancing the stack came into fruition by being released into the wild and for all the Docker users to try. Between the initial experimental 1.8 release and the 1.9 general availability there have been quite a few changes resulting from lessons learned and the communication established among the remote driver community and the Docker/libnetwork teams. While changing APIs can sometimes make the development experience a bit vertiginous, it has allowed for a rapid iteration that got us to a satisfying and sensible API.\r\n\r\nEven though Kuryr will not see a formal release until the Mitaka cycle, the latest snapshots have allowed its MidoNet core developers to use it and even demo it both in <a href=\"https://drive.google.com/file/d/0BwURaz1ic-5tVndGU1Jfb0FTQnM/view?usp=sharing\" target=\"_blank\">recorded sessions</a> and live at the OpenStack Summit (yes, the demo that you can see below runs on top of MidoNet). So you can make today your deployment of MidoNet, Neutron and Keystone in a VM and try out Docker 1.9 with Kuryr for networking your machines. Not only will you be able to network the containers but you will also be able to see the container ports that Kuryr plugs both in the MidoNet and Neutron cli tools as well as in the Horizon dashboard. Currently Kuryr does not yet provide an IPAM driver since that was added to the Docker release after we \"froze\" development in preparation for the OpenStack Summit but it will come in shortly.\r\n\r\n[embed]https://youtu.be/crVi30bgOt0?t=30m50s[/embed]\r\n\r\nYou can find the slides <a href=\"http://schd.ws/hosted_files/mitakadesignsummit/4b/kuryr_tokyo.pdf\" target=\"_blank\">here</a>.\r\n\r\nSo grab the latest MidoNet, Docker, and Neutron. Checkout <a href=\"http://github.com/openstack/kuryr\" target=\"_blank\">Kuryr</a> and start bringing your containers to your MidoNet virtual networks! Join us at the #containers channel at <a href=\"https://slack.midonet.org\" target=\"_blank\">https://slack.midonet.org</a>',529,'https://blog.midonet.org/529-revision-v1/',0,'revision','',0),(548,16,'2015-11-10 13:58:33','0000-00-00 00:00:00','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<blockquote>\n<pre>$ git clone https://github.com/midonet/midonet</pre>\n</blockquote>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<pre  style=\"padding-left: 30px;font-size:80%\">$ ./gradlew assemble</pre>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<pre style=\"padding-left: 30px;font-size:80%\">$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</pre>\n\nIf you\'re on OSX you can use Homebrew:\n\n<pre  style=\"padding-left: 30px;font-size:80%\">$ brew install zookeeper\n$ zkServer start\n</pre>\n\nA Cluster service looks like this:\n\n<pre style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\n\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend,\n        conf: ClusterConfig) extends Minion(nodeCtx) {\n\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n        notifyStarted()\n        log.debug(\"Started\")\n    }\n}\n</pre>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n<pre style=\"padding-left: 30px;font-size:80%\">/** Define a sub-service that runs as part of the Midonet Cluster. This\n  * should expose the necessary API to let the Daemon babysit its minions.\n  *\n  * @param nodeContext metadata about the node where this Minion is running\n  */\nabstract class Minion(nodeContext: Context) extends AbstractService {\n    /** Whether the service is enabled on this Cluster node. */\n    def isEnabled: Boolean\n}\n</pre>\n\nWe will hardcode \'isEnabled\' for now.\n\n<pre style=\"padding-left: 30px;font-size:80%\">override def isEnabled: Boolean = true</pre>\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n<pre style=\"padding-left: 30px;font-size:80%\">@ClusterService(name = \"direct_connect\")</pre>\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\n<pre  style=\"padding-left: 30px;font-size:80%\">class DirectConnectService @Inject()(\n nodeCtx: ClusterNode.Context,\n backend: MidonetBackend) extends Minion(nodeCtx) {\n</pre>\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n<pre style=\"padding-left: 30px;font-size:80%\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\n<pre style=\"padding-left: 30px;font-size:80%\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\n<pre style=\"padding-left: 30px;font-size:80%\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\n\nAnd a bit further down, it\'ll be started:\n\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n</pre>\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\n<pre  style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend) extends Minion(nodeCtx) {\n\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n    private val mySubscriptions = new CompositeSubscription()\n\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n      * an Observable.\n      */\n    private class RouterObserver extends Observer[Router] {\n\n        // On creation we still haven\'t seen any element emitted, so we\n        // don\'t know our Router\'s id.\n        @volatile private var id: UUID = null\n\n        override def onCompleted(): Unit = {\n            log.info(s\"Router $id was deleted, stop watching.\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(s\"Error in Router $id update stream: \", t)\n        }\n        override def onNext(o: Router): Unit = {\n            if (id == null) {\n                id = o.getId.asJava\n                log.info(s\"Loaded initial state of router $id\")\n            } else {\n                log.info(s\"Router $id updated\")\n            }\n        }\n    }\n\n    /** This class listens on the stream of streams of Routers that\n      * represents the full set of Routers in the system.  When a new Router\n      * appears on the system the stream will emit an Observable[Router] that\n      * will emit updates occurred on that Router until it\'s deleted.\n      *\n      * When we first subscribe to the stream, it will emit an\n      * Observable[Router] for each Router that exists in the system.\n      * Similarly, each Observable[Router] will always emit at least 1\n      * element, with the initial state of the Router.\n      */\n    private val routersObserver = new Observer[Observable[Router]] {\n        override def onCompleted(): Unit = {\n            log.info(\"Completed stream of router updates (no more updates \" +\n                     \"will be pushed to the VTEPs)\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(\"Router update stream emits an error: \", t)\n        }\n        override def onNext(o: Observable[Router]): Unit = {\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\n        }\n    }\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        mySubscriptions.unsubscribe()\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n\n        mySubscriptions.add (\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\n        )\n\n        notifyStarted()\n\n        log.debug(\"Started\")\n    }\n\n}\n</pre>\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.','Writing your own MidoNet v5 Cluster Service','','draft','open','open','','','','','2015-11-10 13:58:33','2015-11-10 13:58:33','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<blockquote>\r\n<pre>$ git clone https://github.com/midonet/midonet</pre>\r\n</blockquote>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">$ ./gradlew assemble</pre>\r\n\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</pre>\r\n\r\nIf you\'re on OSX you can use Homebrew:\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">$ brew install zookeeper\r\n$ zkServer start\r\n</pre>\r\n\r\nA Cluster service looks like this:\r\n<pre style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\n\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend,\r\n        conf: ClusterConfig) extends Minion(nodeCtx) {\r\n\r\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n        notifyStarted()\r\n        log.debug(\"Started\")\r\n    }\r\n}\r\n</pre>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n<pre style=\"padding-left: 30px;font-size:80%\">/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n  * should expose the necessary API to let the Daemon babysit its minions.\r\n  *\r\n  * @param nodeContext metadata about the node where this Minion is running\r\n  */\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n    /** Whether the service is enabled on this Cluster node. */\r\n    def isEnabled: Boolean\r\n}\r\n</pre>\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">override def isEnabled: Boolean = true</pre>\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n<pre style=\"padding-left: 30px;font-size:80%\">@ClusterService(name = \"direct_connect\")</pre>\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">class DirectConnectService @Inject()(\r\n nodeCtx: ClusterNode.Context,\r\n backend: MidonetBackend) extends Minion(nodeCtx) {\r\n</pre>\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\n\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n</pre>\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n    private val mySubscriptions = new CompositeSubscription()\r\n\r\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n      * an Observable.\r\n      */\r\n    private class RouterObserver extends Observer[Router] {\r\n\r\n        // On creation we still haven\'t seen any element emitted, so we\r\n        // don\'t know our Router\'s id.\r\n        @volatile private var id: UUID = null\r\n\r\n        override def onCompleted(): Unit = {\r\n            log.info(s\"Router $id was deleted, stop watching.\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(s\"Error in Router $id update stream: \", t)\r\n        }\r\n        override def onNext(o: Router): Unit = {\r\n            if (id == null) {\r\n                id = o.getId.asJava\r\n                log.info(s\"Loaded initial state of router $id\")\r\n            } else {\r\n                log.info(s\"Router $id updated\")\r\n            }\r\n        }\r\n    }\r\n\r\n    /** This class listens on the stream of streams of Routers that\r\n      * represents the full set of Routers in the system.  When a new Router\r\n      * appears on the system the stream will emit an Observable[Router] that\r\n      * will emit updates occurred on that Router until it\'s deleted.\r\n      *\r\n      * When we first subscribe to the stream, it will emit an\r\n      * Observable[Router] for each Router that exists in the system.\r\n      * Similarly, each Observable[Router] will always emit at least 1\r\n      * element, with the initial state of the Router.\r\n      */\r\n    private val routersObserver = new Observer[Observable[Router]] {\r\n        override def onCompleted(): Unit = {\r\n            log.info(\"Completed stream of router updates (no more updates \" +\r\n                     \"will be pushed to the VTEPs)\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(\"Router update stream emits an error: \", t)\r\n        }\r\n        override def onNext(o: Observable[Router]): Unit = {\r\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\r\n        }\r\n    }\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        mySubscriptions.unsubscribe()\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n\r\n        mySubscriptions.add (\r\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n        )\r\n\r\n        notifyStarted()\r\n\r\n        log.debug(\"Started\")\r\n    }\r\n\r\n}\r\n</pre>\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.',0,'https://blog.midonet.org/?p=548',0,'post','',0),(549,16,'2015-11-10 11:37:51','2015-11-10 11:37:51','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n$ git clone https://github.com/midonet/midonet\n\nFollow the instructions in DEVELOPMENT.md to setup your environment.\nWhen you can run:\n\n$ ./gradlew assemble\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n$ sudo apt-get install zookeeper\n$ sudo service zookeeper start\n\nIf you\'re on OSX you can use Homebrew:\n\n$ brew install zookeeper\n$ zkServer start\n\nA Cluster service looks like this:\n\npackage org.midonet.cluster.services.direct_connect\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\n\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend,\nconf: ClusterConfig) extends Minion(nodeCtx) {\n\nval log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n\noverride def isEnabled: Boolean = true\n\noverride def doStop(): Unit = {\nlog.debug(\"Stopping\")\nnotifyStopped()\nlog.debug(\"Stopped\")\n}\n\noverride def doStart(): Unit = {\nlog.debug(\"Starting\")\nnotifyStarted()\nlog.debug(\"Started\")\n}\n}\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n/** Define a sub-service that runs as part of the Midonet Cluster. This\n* should expose the necessary API to let the Daemon babysit its minions.\n*\n* @param nodeContext metadata about the node where this Minion is running\n*/\nabstract class Minion(nodeContext: Context) extends AbstractService {\n/** Whether the service is enabled on this Cluster node. */\ndef isEnabled: Boolean\n}\n\nWe will hardcode \'isEnabled\' for now.\n\noverride def isEnabled: Boolean = true\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n@ClusterService(name = \"direct_connect\")\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\n\nAnd a bit further down, it\'ll be started:\n\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\npackage org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\nprivate val mySubscriptions = new CompositeSubscription()\n\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n* an Observable.\n*/\nprivate class RouterObserver extends Observer[Router] {\n\n// On creation we still haven\'t seen any element emitted, so we\n// don\'t know our Router\'s id.\n@volatile private var id: UUID = null\n\noverride def onCompleted(): Unit = {\nlog.info(s\"Router $id was deleted, stop watching.\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(s\"Error in Router $id update stream: \", t)\n}\noverride def onNext(o: Router): Unit = {\nif (id == null) {\nid = o.getId.asJava\nlog.info(s\"Loaded initial state of router $id\")\n} else {\nlog.info(s\"Router $id updated\")\n}\n}\n}\n\n/** This class listens on the stream of streams of Routers that\n* represents the full set of Routers in the system.  When a new Router\n* appears on the system the stream will emit an Observable[Router] that\n* will emit updates occurred on that Router until it\'s deleted.\n*\n* When we first subscribe to the stream, it will emit an\n* Observable[Router] for each Router that exists in the system.\n* Similarly, each Observable[Router] will always emit at least 1\n* element, with the initial state of the Router.\n*/\nprivate val routersObserver = new Observer[Observable[Router]] {\noverride def onCompleted(): Unit = {\nlog.info(\"Completed stream of router updates (no more updates \" +\n\"will be pushed to the VTEPs)\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(\"Router update stream emits an error: \", t)\n}\noverride def onNext(o: Observable[Router]): Unit = {\nmySubscriptions.add(o.subscribe(new RouterObserver()))\n}\n}\n\noverride def isEnabled: Boolean = true\n\noverride def doStop(): Unit = {\nlog.debug(\"Stopping\")\nmySubscriptions.unsubscribe()\nnotifyStopped()\nlog.debug(\"Stopped\")\n}\n\noverride def doStart(): Unit = {\nlog.debug(\"Starting\")\n\nmySubscriptions.add (\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\n)\n\nnotifyStarted()\n\nlog.debug(\"Started\")\n}\n\n}\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\n\n&nbsp;','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 11:37:51','2015-11-10 11:37:51','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n\r\n$ git clone https://github.com/midonet/midonet\r\n\r\nFollow the instructions in DEVELOPMENT.md to setup your environment.\r\nWhen you can run:\r\n\r\n$ ./gradlew assemble\r\n\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n\r\n$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start\r\n\r\nIf you\'re on OSX you can use Homebrew:\r\n\r\n$ brew install zookeeper\r\n$ zkServer start\r\n\r\nA Cluster service looks like this:\r\n\r\npackage org.midonet.cluster.services.direct_connect\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\n\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend,\r\nconf: ClusterConfig) extends Minion(nodeCtx) {\r\n\r\nval log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\noverride def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}\r\n\r\noverride def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\nnotifyStarted()\r\nlog.debug(\"Started\")\r\n}\r\n}\r\n\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n\r\n/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n* should expose the necessary API to let the Daemon babysit its minions.\r\n*\r\n* @param nodeContext metadata about the node where this Minion is running\r\n*/\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n/** Whether the service is enabled on this Cluster node. */\r\ndef isEnabled: Boolean\r\n}\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n\r\n@ClusterService(name = \"direct_connect\")\r\n\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\npackage org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\nprivate val mySubscriptions = new CompositeSubscription()\r\n\r\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n* an Observable.\r\n*/\r\nprivate class RouterObserver extends Observer[Router] {\r\n\r\n// On creation we still haven\'t seen any element emitted, so we\r\n// don\'t know our Router\'s id.\r\n@volatile private var id: UUID = null\r\n\r\noverride def onCompleted(): Unit = {\r\nlog.info(s\"Router $id was deleted, stop watching.\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(s\"Error in Router $id update stream: \", t)\r\n}\r\noverride def onNext(o: Router): Unit = {\r\nif (id == null) {\r\nid = o.getId.asJava\r\nlog.info(s\"Loaded initial state of router $id\")\r\n} else {\r\nlog.info(s\"Router $id updated\")\r\n}\r\n}\r\n}\r\n\r\n/** This class listens on the stream of streams of Routers that\r\n* represents the full set of Routers in the system.  When a new Router\r\n* appears on the system the stream will emit an Observable[Router] that\r\n* will emit updates occurred on that Router until it\'s deleted.\r\n*\r\n* When we first subscribe to the stream, it will emit an\r\n* Observable[Router] for each Router that exists in the system.\r\n* Similarly, each Observable[Router] will always emit at least 1\r\n* element, with the initial state of the Router.\r\n*/\r\nprivate val routersObserver = new Observer[Observable[Router]] {\r\noverride def onCompleted(): Unit = {\r\nlog.info(\"Completed stream of router updates (no more updates \" +\r\n\"will be pushed to the VTEPs)\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(\"Router update stream emits an error: \", t)\r\n}\r\noverride def onNext(o: Observable[Router]): Unit = {\r\nmySubscriptions.add(o.subscribe(new RouterObserver()))\r\n}\r\n}\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\noverride def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nmySubscriptions.unsubscribe()\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}\r\n\r\noverride def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\n\r\nmySubscriptions.add (\r\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n)\r\n\r\nnotifyStarted()\r\n\r\nlog.debug(\"Started\")\r\n}\r\n\r\n}\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\r\n\r\n&nbsp;',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(550,16,'2015-11-10 11:41:56','2015-11-10 11:41:56','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<p style=\"padding-left: 30px;\">$ git clone https://github.com/midonet/midonet</p>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<p style=\"padding-left: 30px;\">$ ./gradlew assemble</p>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<p style=\"padding-left: 30px;\">$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</p>\n\nIf you\'re on OSX you can use Homebrew:\n\n<p style=\"padding-left: 30px;\">$ brew install zookeeper\n$ zkServer start</p>\n\nA Cluster service looks like this:\n\n<p style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect</p>\n\n<p style=\"padding-left: 30px;\">import com.google.inject.Inject\nimport org.slf4j.LoggerFactory</p>\n\n<p style=\"padding-left: 30px;\">import org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}</p>\n\n<p style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend,\nconf: ClusterConfig) extends Minion(nodeCtx) {</p>\n\n<p style=\"padding-left: 30px;\">    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")</p>\n\n<p style=\"padding-left: 30px;\">    override def isEnabled: Boolean = true</p>\n\n<p style=\"padding-left: 30px;\">    override def doStop(): Unit = {\nlog.debug(\"Stopping\")\nnotifyStopped()\nlog.debug(\"Stopped\")\n}</p>\n\n<p style=\"padding-left: 30px;\">    override def doStart(): Unit = {\nlog.debug(\"Starting\")\nnotifyStarted()\nlog.debug(\"Started\")\n}\n}</p>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n/** Define a sub-service that runs as part of the Midonet Cluster. This\n* should expose the necessary API to let the Daemon babysit its minions.\n*\n* @param nodeContext metadata about the node where this Minion is running\n*/\nabstract class Minion(nodeContext: Context) extends AbstractService {\n/** Whether the service is enabled on this Cluster node. */\ndef isEnabled: Boolean\n}\n\nWe will hardcode \'isEnabled\' for now.\n\noverride def isEnabled: Boolean = true\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n@ClusterService(name = \"direct_connect\")\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\n\nAnd a bit further down, it\'ll be started:\n\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\npackage org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\nprivate val mySubscriptions = new CompositeSubscription()\n\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n* an Observable.\n*/\nprivate class RouterObserver extends Observer[Router] {\n\n// On creation we still haven\'t seen any element emitted, so we\n// don\'t know our Router\'s id.\n@volatile private var id: UUID = null\n\noverride def onCompleted(): Unit = {\nlog.info(s\"Router $id was deleted, stop watching.\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(s\"Error in Router $id update stream: \", t)\n}\noverride def onNext(o: Router): Unit = {\nif (id == null) {\nid = o.getId.asJava\nlog.info(s\"Loaded initial state of router $id\")\n} else {\nlog.info(s\"Router $id updated\")\n}\n}\n}\n\n/** This class listens on the stream of streams of Routers that\n* represents the full set of Routers in the system.  When a new Router\n* appears on the system the stream will emit an Observable[Router] that\n* will emit updates occurred on that Router until it\'s deleted.\n*\n* When we first subscribe to the stream, it will emit an\n* Observable[Router] for each Router that exists in the system.\n* Similarly, each Observable[Router] will always emit at least 1\n* element, with the initial state of the Router.\n*/\nprivate val routersObserver = new Observer[Observable[Router]] {\noverride def onCompleted(): Unit = {\nlog.info(\"Completed stream of router updates (no more updates \" +\n\"will be pushed to the VTEPs)\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(\"Router update stream emits an error: \", t)\n}\noverride def onNext(o: Observable[Router]): Unit = {\nmySubscriptions.add(o.subscribe(new RouterObserver()))\n}\n}\n\noverride def isEnabled: Boolean = true\n\noverride def doStop(): Unit = {\nlog.debug(\"Stopping\")\nmySubscriptions.unsubscribe()\nnotifyStopped()\nlog.debug(\"Stopped\")\n}\n\noverride def doStart(): Unit = {\nlog.debug(\"Starting\")\n\nmySubscriptions.add (\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\n)\n\nnotifyStarted()\n\nlog.debug(\"Started\")\n}\n\n}\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\n\n&nbsp;','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 11:41:56','2015-11-10 11:41:56','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<p style=\"padding-left: 30px;\">$ git clone https://github.com/midonet/midonet</p>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n<p style=\"padding-left: 30px;\">$ ./gradlew assemble</p>\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n<p style=\"padding-left: 30px;\">$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</p>\r\nIf you\'re on OSX you can use Homebrew:\r\n<p style=\"padding-left: 30px;\">$ brew install zookeeper\r\n$ zkServer start</p>\r\nA Cluster service looks like this:\r\n<p style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect</p>\r\n<p style=\"padding-left: 30px;\">import com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory</p>\r\n<p style=\"padding-left: 30px;\">import org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}</p>\r\n<p style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend,\r\nconf: ClusterConfig) extends Minion(nodeCtx) {</p>\r\n<p style=\"padding-left: 30px;\">    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")</p>\r\n<p style=\"padding-left: 30px;\">    override def isEnabled: Boolean = true</p>\r\n<p style=\"padding-left: 30px;\">    override def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}</p>\r\n<p style=\"padding-left: 30px;\">    override def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\nnotifyStarted()\r\nlog.debug(\"Started\")\r\n}\r\n}</p>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n\r\n/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n* should expose the necessary API to let the Daemon babysit its minions.\r\n*\r\n* @param nodeContext metadata about the node where this Minion is running\r\n*/\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n/** Whether the service is enabled on this Cluster node. */\r\ndef isEnabled: Boolean\r\n}\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n\r\n@ClusterService(name = \"direct_connect\")\r\n\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\npackage org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\nprivate val mySubscriptions = new CompositeSubscription()\r\n\r\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n* an Observable.\r\n*/\r\nprivate class RouterObserver extends Observer[Router] {\r\n\r\n// On creation we still haven\'t seen any element emitted, so we\r\n// don\'t know our Router\'s id.\r\n@volatile private var id: UUID = null\r\n\r\noverride def onCompleted(): Unit = {\r\nlog.info(s\"Router $id was deleted, stop watching.\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(s\"Error in Router $id update stream: \", t)\r\n}\r\noverride def onNext(o: Router): Unit = {\r\nif (id == null) {\r\nid = o.getId.asJava\r\nlog.info(s\"Loaded initial state of router $id\")\r\n} else {\r\nlog.info(s\"Router $id updated\")\r\n}\r\n}\r\n}\r\n\r\n/** This class listens on the stream of streams of Routers that\r\n* represents the full set of Routers in the system.  When a new Router\r\n* appears on the system the stream will emit an Observable[Router] that\r\n* will emit updates occurred on that Router until it\'s deleted.\r\n*\r\n* When we first subscribe to the stream, it will emit an\r\n* Observable[Router] for each Router that exists in the system.\r\n* Similarly, each Observable[Router] will always emit at least 1\r\n* element, with the initial state of the Router.\r\n*/\r\nprivate val routersObserver = new Observer[Observable[Router]] {\r\noverride def onCompleted(): Unit = {\r\nlog.info(\"Completed stream of router updates (no more updates \" +\r\n\"will be pushed to the VTEPs)\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(\"Router update stream emits an error: \", t)\r\n}\r\noverride def onNext(o: Observable[Router]): Unit = {\r\nmySubscriptions.add(o.subscribe(new RouterObserver()))\r\n}\r\n}\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\noverride def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nmySubscriptions.unsubscribe()\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}\r\n\r\noverride def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\n\r\nmySubscriptions.add (\r\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n)\r\n\r\nnotifyStarted()\r\n\r\nlog.debug(\"Started\")\r\n}\r\n\r\n}\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\r\n\r\n&nbsp;',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(551,16,'2015-11-10 11:43:24','2015-11-10 11:43:24','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<p style=\"padding-left: 30px;\"><span style=\"color: #008000;\">$ git clone https://github.com/midonet/midonet</span></p>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<p style=\"padding-left: 30px;\">$ ./gradlew assemble</p>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<p style=\"padding-left: 30px;\">$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</p>\n\nIf you\'re on OSX you can use Homebrew:\n\n<p style=\"padding-left: 30px;\">$ brew install zookeeper\n$ zkServer start</p>\n\nA Cluster service looks like this:\n\n<p style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect</p>\n\n<p style=\"padding-left: 30px;\">import com.google.inject.Inject\nimport org.slf4j.LoggerFactory</p>\n\n<p style=\"padding-left: 30px;\">import org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}</p>\n\n<p style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend,\nconf: ClusterConfig) extends Minion(nodeCtx) {</p>\n\n<p style=\"padding-left: 30px;\">    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")</p>\n\n<p style=\"padding-left: 30px;\">    override def isEnabled: Boolean = true</p>\n\n<p style=\"padding-left: 30px;\">    override def doStop(): Unit = {\nlog.debug(\"Stopping\")\nnotifyStopped()\nlog.debug(\"Stopped\")\n}</p>\n\n<p style=\"padding-left: 30px;\">    override def doStart(): Unit = {\nlog.debug(\"Starting\")\nnotifyStarted()\nlog.debug(\"Started\")\n}\n}</p>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n/** Define a sub-service that runs as part of the Midonet Cluster. This\n* should expose the necessary API to let the Daemon babysit its minions.\n*\n* @param nodeContext metadata about the node where this Minion is running\n*/\nabstract class Minion(nodeContext: Context) extends AbstractService {\n/** Whether the service is enabled on this Cluster node. */\ndef isEnabled: Boolean\n}\n\nWe will hardcode \'isEnabled\' for now.\n\noverride def isEnabled: Boolean = true\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n@ClusterService(name = \"direct_connect\")\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\n\nAnd a bit further down, it\'ll be started:\n\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\npackage org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\nprivate val mySubscriptions = new CompositeSubscription()\n\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n* an Observable.\n*/\nprivate class RouterObserver extends Observer[Router] {\n\n// On creation we still haven\'t seen any element emitted, so we\n// don\'t know our Router\'s id.\n@volatile private var id: UUID = null\n\noverride def onCompleted(): Unit = {\nlog.info(s\"Router $id was deleted, stop watching.\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(s\"Error in Router $id update stream: \", t)\n}\noverride def onNext(o: Router): Unit = {\nif (id == null) {\nid = o.getId.asJava\nlog.info(s\"Loaded initial state of router $id\")\n} else {\nlog.info(s\"Router $id updated\")\n}\n}\n}\n\n/** This class listens on the stream of streams of Routers that\n* represents the full set of Routers in the system.  When a new Router\n* appears on the system the stream will emit an Observable[Router] that\n* will emit updates occurred on that Router until it\'s deleted.\n*\n* When we first subscribe to the stream, it will emit an\n* Observable[Router] for each Router that exists in the system.\n* Similarly, each Observable[Router] will always emit at least 1\n* element, with the initial state of the Router.\n*/\nprivate val routersObserver = new Observer[Observable[Router]] {\noverride def onCompleted(): Unit = {\nlog.info(\"Completed stream of router updates (no more updates \" +\n\"will be pushed to the VTEPs)\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(\"Router update stream emits an error: \", t)\n}\noverride def onNext(o: Observable[Router]): Unit = {\nmySubscriptions.add(o.subscribe(new RouterObserver()))\n}\n}\n\noverride def isEnabled: Boolean = true\n\noverride def doStop(): Unit = {\nlog.debug(\"Stopping\")\nmySubscriptions.unsubscribe()\nnotifyStopped()\nlog.debug(\"Stopped\")\n}\n\noverride def doStart(): Unit = {\nlog.debug(\"Starting\")\n\nmySubscriptions.add (\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\n)\n\nnotifyStarted()\n\nlog.debug(\"Started\")\n}\n\n}\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\n\n&nbsp;','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 11:43:24','2015-11-10 11:43:24','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<p style=\"padding-left: 30px;\"><span style=\"color: #008000;\">$ git clone https://github.com/midonet/midonet</span></p>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n<p style=\"padding-left: 30px;\">$ ./gradlew assemble</p>\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n<p style=\"padding-left: 30px;\">$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</p>\r\nIf you\'re on OSX you can use Homebrew:\r\n<p style=\"padding-left: 30px;\">$ brew install zookeeper\r\n$ zkServer start</p>\r\nA Cluster service looks like this:\r\n<p style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect</p>\r\n<p style=\"padding-left: 30px;\">import com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory</p>\r\n<p style=\"padding-left: 30px;\">import org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}</p>\r\n<p style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend,\r\nconf: ClusterConfig) extends Minion(nodeCtx) {</p>\r\n<p style=\"padding-left: 30px;\">    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")</p>\r\n<p style=\"padding-left: 30px;\">    override def isEnabled: Boolean = true</p>\r\n<p style=\"padding-left: 30px;\">    override def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}</p>\r\n<p style=\"padding-left: 30px;\">    override def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\nnotifyStarted()\r\nlog.debug(\"Started\")\r\n}\r\n}</p>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n\r\n/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n* should expose the necessary API to let the Daemon babysit its minions.\r\n*\r\n* @param nodeContext metadata about the node where this Minion is running\r\n*/\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n/** Whether the service is enabled on this Cluster node. */\r\ndef isEnabled: Boolean\r\n}\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n\r\n@ClusterService(name = \"direct_connect\")\r\n\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\npackage org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\nprivate val mySubscriptions = new CompositeSubscription()\r\n\r\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n* an Observable.\r\n*/\r\nprivate class RouterObserver extends Observer[Router] {\r\n\r\n// On creation we still haven\'t seen any element emitted, so we\r\n// don\'t know our Router\'s id.\r\n@volatile private var id: UUID = null\r\n\r\noverride def onCompleted(): Unit = {\r\nlog.info(s\"Router $id was deleted, stop watching.\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(s\"Error in Router $id update stream: \", t)\r\n}\r\noverride def onNext(o: Router): Unit = {\r\nif (id == null) {\r\nid = o.getId.asJava\r\nlog.info(s\"Loaded initial state of router $id\")\r\n} else {\r\nlog.info(s\"Router $id updated\")\r\n}\r\n}\r\n}\r\n\r\n/** This class listens on the stream of streams of Routers that\r\n* represents the full set of Routers in the system.  When a new Router\r\n* appears on the system the stream will emit an Observable[Router] that\r\n* will emit updates occurred on that Router until it\'s deleted.\r\n*\r\n* When we first subscribe to the stream, it will emit an\r\n* Observable[Router] for each Router that exists in the system.\r\n* Similarly, each Observable[Router] will always emit at least 1\r\n* element, with the initial state of the Router.\r\n*/\r\nprivate val routersObserver = new Observer[Observable[Router]] {\r\noverride def onCompleted(): Unit = {\r\nlog.info(\"Completed stream of router updates (no more updates \" +\r\n\"will be pushed to the VTEPs)\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(\"Router update stream emits an error: \", t)\r\n}\r\noverride def onNext(o: Observable[Router]): Unit = {\r\nmySubscriptions.add(o.subscribe(new RouterObserver()))\r\n}\r\n}\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\noverride def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nmySubscriptions.unsubscribe()\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}\r\n\r\noverride def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\n\r\nmySubscriptions.add (\r\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n)\r\n\r\nnotifyStarted()\r\n\r\nlog.debug(\"Started\")\r\n}\r\n\r\n}\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\r\n\r\n&nbsp;',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(552,16,'2015-11-10 11:44:29','2015-11-10 11:44:29','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<blockquote>\n<pre><span style=\"color: #000000;\">$ git clone https://github.com/midonet/midonet</span></pre>\n</blockquote>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<p style=\"padding-left: 30px;\">$ ./gradlew assemble</p>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<p style=\"padding-left: 30px;\">$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</p>\n\nIf you\'re on OSX you can use Homebrew:\n\n<p style=\"padding-left: 30px;\">$ brew install zookeeper\n$ zkServer start</p>\n\nA Cluster service looks like this:\n\n<p style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect</p>\n\n<p style=\"padding-left: 30px;\">import com.google.inject.Inject\nimport org.slf4j.LoggerFactory</p>\n\n<p style=\"padding-left: 30px;\">import org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}</p>\n\n<p style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend,\nconf: ClusterConfig) extends Minion(nodeCtx) {</p>\n\n<p style=\"padding-left: 30px;\">    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")</p>\n\n<p style=\"padding-left: 30px;\">    override def isEnabled: Boolean = true</p>\n\n<p style=\"padding-left: 30px;\">    override def doStop(): Unit = {\nlog.debug(\"Stopping\")\nnotifyStopped()\nlog.debug(\"Stopped\")\n}</p>\n\n<p style=\"padding-left: 30px;\">    override def doStart(): Unit = {\nlog.debug(\"Starting\")\nnotifyStarted()\nlog.debug(\"Started\")\n}\n}</p>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n/** Define a sub-service that runs as part of the Midonet Cluster. This\n* should expose the necessary API to let the Daemon babysit its minions.\n*\n* @param nodeContext metadata about the node where this Minion is running\n*/\nabstract class Minion(nodeContext: Context) extends AbstractService {\n/** Whether the service is enabled on this Cluster node. */\ndef isEnabled: Boolean\n}\n\nWe will hardcode \'isEnabled\' for now.\n\noverride def isEnabled: Boolean = true\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n@ClusterService(name = \"direct_connect\")\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\n\nAnd a bit further down, it\'ll be started:\n\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\npackage org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\nprivate val mySubscriptions = new CompositeSubscription()\n\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n* an Observable.\n*/\nprivate class RouterObserver extends Observer[Router] {\n\n// On creation we still haven\'t seen any element emitted, so we\n// don\'t know our Router\'s id.\n@volatile private var id: UUID = null\n\noverride def onCompleted(): Unit = {\nlog.info(s\"Router $id was deleted, stop watching.\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(s\"Error in Router $id update stream: \", t)\n}\noverride def onNext(o: Router): Unit = {\nif (id == null) {\nid = o.getId.asJava\nlog.info(s\"Loaded initial state of router $id\")\n} else {\nlog.info(s\"Router $id updated\")\n}\n}\n}\n\n/** This class listens on the stream of streams of Routers that\n* represents the full set of Routers in the system.  When a new Router\n* appears on the system the stream will emit an Observable[Router] that\n* will emit updates occurred on that Router until it\'s deleted.\n*\n* When we first subscribe to the stream, it will emit an\n* Observable[Router] for each Router that exists in the system.\n* Similarly, each Observable[Router] will always emit at least 1\n* element, with the initial state of the Router.\n*/\nprivate val routersObserver = new Observer[Observable[Router]] {\noverride def onCompleted(): Unit = {\nlog.info(\"Completed stream of router updates (no more updates \" +\n\"will be pushed to the VTEPs)\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(\"Router update stream emits an error: \", t)\n}\noverride def onNext(o: Observable[Router]): Unit = {\nmySubscriptions.add(o.subscribe(new RouterObserver()))\n}\n}\n\noverride def isEnabled: Boolean = true\n\noverride def doStop(): Unit = {\nlog.debug(\"Stopping\")\nmySubscriptions.unsubscribe()\nnotifyStopped()\nlog.debug(\"Stopped\")\n}\n\noverride def doStart(): Unit = {\nlog.debug(\"Starting\")\n\nmySubscriptions.add (\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\n)\n\nnotifyStarted()\n\nlog.debug(\"Started\")\n}\n\n}\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\n\n&nbsp;','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 11:44:29','2015-11-10 11:44:29','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<blockquote>\r\n<pre><span style=\"color: #000000;\">$ git clone https://github.com/midonet/midonet</span></pre>\r\n</blockquote>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n<p style=\"padding-left: 30px;\">$ ./gradlew assemble</p>\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n<p style=\"padding-left: 30px;\">$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</p>\r\nIf you\'re on OSX you can use Homebrew:\r\n<p style=\"padding-left: 30px;\">$ brew install zookeeper\r\n$ zkServer start</p>\r\nA Cluster service looks like this:\r\n<p style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect</p>\r\n<p style=\"padding-left: 30px;\">import com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory</p>\r\n<p style=\"padding-left: 30px;\">import org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}</p>\r\n<p style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend,\r\nconf: ClusterConfig) extends Minion(nodeCtx) {</p>\r\n<p style=\"padding-left: 30px;\">    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")</p>\r\n<p style=\"padding-left: 30px;\">    override def isEnabled: Boolean = true</p>\r\n<p style=\"padding-left: 30px;\">    override def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}</p>\r\n<p style=\"padding-left: 30px;\">    override def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\nnotifyStarted()\r\nlog.debug(\"Started\")\r\n}\r\n}</p>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n\r\n/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n* should expose the necessary API to let the Daemon babysit its minions.\r\n*\r\n* @param nodeContext metadata about the node where this Minion is running\r\n*/\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n/** Whether the service is enabled on this Cluster node. */\r\ndef isEnabled: Boolean\r\n}\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n\r\n@ClusterService(name = \"direct_connect\")\r\n\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\npackage org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\nprivate val mySubscriptions = new CompositeSubscription()\r\n\r\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n* an Observable.\r\n*/\r\nprivate class RouterObserver extends Observer[Router] {\r\n\r\n// On creation we still haven\'t seen any element emitted, so we\r\n// don\'t know our Router\'s id.\r\n@volatile private var id: UUID = null\r\n\r\noverride def onCompleted(): Unit = {\r\nlog.info(s\"Router $id was deleted, stop watching.\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(s\"Error in Router $id update stream: \", t)\r\n}\r\noverride def onNext(o: Router): Unit = {\r\nif (id == null) {\r\nid = o.getId.asJava\r\nlog.info(s\"Loaded initial state of router $id\")\r\n} else {\r\nlog.info(s\"Router $id updated\")\r\n}\r\n}\r\n}\r\n\r\n/** This class listens on the stream of streams of Routers that\r\n* represents the full set of Routers in the system.  When a new Router\r\n* appears on the system the stream will emit an Observable[Router] that\r\n* will emit updates occurred on that Router until it\'s deleted.\r\n*\r\n* When we first subscribe to the stream, it will emit an\r\n* Observable[Router] for each Router that exists in the system.\r\n* Similarly, each Observable[Router] will always emit at least 1\r\n* element, with the initial state of the Router.\r\n*/\r\nprivate val routersObserver = new Observer[Observable[Router]] {\r\noverride def onCompleted(): Unit = {\r\nlog.info(\"Completed stream of router updates (no more updates \" +\r\n\"will be pushed to the VTEPs)\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(\"Router update stream emits an error: \", t)\r\n}\r\noverride def onNext(o: Observable[Router]): Unit = {\r\nmySubscriptions.add(o.subscribe(new RouterObserver()))\r\n}\r\n}\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\noverride def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nmySubscriptions.unsubscribe()\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}\r\n\r\noverride def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\n\r\nmySubscriptions.add (\r\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n)\r\n\r\nnotifyStarted()\r\n\r\nlog.debug(\"Started\")\r\n}\r\n\r\n}\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\r\n\r\n&nbsp;',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(553,16,'2015-11-10 11:51:59','2015-11-10 11:51:59','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<blockquote>\n<pre><span style=\"color: #000000;\">$ git clone https://github.com/midonet/midonet</span></pre>\n</blockquote>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<blockquote>\n<pre>$ ./gradlew assemble</pre>\n</blockquote>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<blockquote>\n<pre>$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</pre>\n</blockquote>\n\nIf you\'re on OSX you can use Homebrew:\n\n<blockquote>\n<pre>$ brew install zookeeper\n$ zkServer start</pre>\n</blockquote>\n\nA Cluster service looks like this:\n\n&nbsp;\n\n[embed]https://gist.github.com/srvaroa/c2c56624a2f34c4da239#file-a-simple-midonet-cluster-service[/embed]\n\n&nbsp;\n\n[embed]https://gist.github.com/srvaroa/c2c56624a2f34c4da239#file-a-simple-midonet-cluster-service[/embed]\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n/** Define a sub-service that runs as part of the Midonet Cluster. This\n* should expose the necessary API to let the Daemon babysit its minions.\n*\n* @param nodeContext metadata about the node where this Minion is running\n*/\nabstract class Minion(nodeContext: Context) extends AbstractService {\n/** Whether the service is enabled on this Cluster node. */\ndef isEnabled: Boolean\n}\n\nWe will hardcode \'isEnabled\' for now.\n\noverride def isEnabled: Boolean = true\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n@ClusterService(name = \"direct_connect\")\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\n\nAnd a bit further down, it\'ll be started:\n\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\npackage org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\nnodeCtx: ClusterNode.Context,\nbackend: MidonetBackend) extends Minion(nodeCtx) {\n\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\nprivate val mySubscriptions = new CompositeSubscription()\n\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n* an Observable.\n*/\nprivate class RouterObserver extends Observer[Router] {\n\n// On creation we still haven\'t seen any element emitted, so we\n// don\'t know our Router\'s id.\n@volatile private var id: UUID = null\n\noverride def onCompleted(): Unit = {\nlog.info(s\"Router $id was deleted, stop watching.\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(s\"Error in Router $id update stream: \", t)\n}\noverride def onNext(o: Router): Unit = {\nif (id == null) {\nid = o.getId.asJava\nlog.info(s\"Loaded initial state of router $id\")\n} else {\nlog.info(s\"Router $id updated\")\n}\n}\n}\n\n/** This class listens on the stream of streams of Routers that\n* represents the full set of Routers in the system.  When a new Router\n* appears on the system the stream will emit an Observable[Router] that\n* will emit updates occurred on that Router until it\'s deleted.\n*\n* When we first subscribe to the stream, it will emit an\n* Observable[Router] for each Router that exists in the system.\n* Similarly, each Observable[Router] will always emit at least 1\n* element, with the initial state of the Router.\n*/\nprivate val routersObserver = new Observer[Observable[Router]] {\noverride def onCompleted(): Unit = {\nlog.info(\"Completed stream of router updates (no more updates \" +\n\"will be pushed to the VTEPs)\")\n}\noverride def onError(t: Throwable): Unit = {\nlog.warn(\"Router update stream emits an error: \", t)\n}\noverride def onNext(o: Observable[Router]): Unit = {\nmySubscriptions.add(o.subscribe(new RouterObserver()))\n}\n}\n\noverride def isEnabled: Boolean = true\n\noverride def doStop(): Unit = {\nlog.debug(\"Stopping\")\nmySubscriptions.unsubscribe()\nnotifyStopped()\nlog.debug(\"Stopped\")\n}\n\noverride def doStart(): Unit = {\nlog.debug(\"Starting\")\n\nmySubscriptions.add (\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\n)\n\nnotifyStarted()\n\nlog.debug(\"Started\")\n}\n\n}\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\n\n&nbsp;','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 11:51:59','2015-11-10 11:51:59','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<blockquote>\r\n<pre><span style=\"color: #000000;\">$ git clone https://github.com/midonet/midonet</span></pre>\r\n</blockquote>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n<blockquote>\r\n<pre>$ ./gradlew assemble</pre>\r\n</blockquote>\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n<blockquote>\r\n<pre>$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</pre>\r\n</blockquote>\r\nIf you\'re on OSX you can use Homebrew:\r\n<blockquote>\r\n<pre>$ brew install zookeeper\r\n$ zkServer start</pre>\r\n</blockquote>\r\nA Cluster service looks like this:\r\n\r\n&nbsp;\r\n\r\n[embed]https://gist.github.com/srvaroa/c2c56624a2f34c4da239#file-a-simple-midonet-cluster-service[/embed]\r\n\r\n&nbsp;\r\n\r\n[embed]https://gist.github.com/srvaroa/c2c56624a2f34c4da239#file-a-simple-midonet-cluster-service[/embed]\r\n\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n\r\n/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n* should expose the necessary API to let the Daemon babysit its minions.\r\n*\r\n* @param nodeContext metadata about the node where this Minion is running\r\n*/\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n/** Whether the service is enabled on this Cluster node. */\r\ndef isEnabled: Boolean\r\n}\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n\r\n@ClusterService(name = \"direct_connect\")\r\n\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\ntail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\ncp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\npackage org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\nnodeCtx: ClusterNode.Context,\r\nbackend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\nprivate val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\nprivate val mySubscriptions = new CompositeSubscription()\r\n\r\n/** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n* an Observable.\r\n*/\r\nprivate class RouterObserver extends Observer[Router] {\r\n\r\n// On creation we still haven\'t seen any element emitted, so we\r\n// don\'t know our Router\'s id.\r\n@volatile private var id: UUID = null\r\n\r\noverride def onCompleted(): Unit = {\r\nlog.info(s\"Router $id was deleted, stop watching.\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(s\"Error in Router $id update stream: \", t)\r\n}\r\noverride def onNext(o: Router): Unit = {\r\nif (id == null) {\r\nid = o.getId.asJava\r\nlog.info(s\"Loaded initial state of router $id\")\r\n} else {\r\nlog.info(s\"Router $id updated\")\r\n}\r\n}\r\n}\r\n\r\n/** This class listens on the stream of streams of Routers that\r\n* represents the full set of Routers in the system.  When a new Router\r\n* appears on the system the stream will emit an Observable[Router] that\r\n* will emit updates occurred on that Router until it\'s deleted.\r\n*\r\n* When we first subscribe to the stream, it will emit an\r\n* Observable[Router] for each Router that exists in the system.\r\n* Similarly, each Observable[Router] will always emit at least 1\r\n* element, with the initial state of the Router.\r\n*/\r\nprivate val routersObserver = new Observer[Observable[Router]] {\r\noverride def onCompleted(): Unit = {\r\nlog.info(\"Completed stream of router updates (no more updates \" +\r\n\"will be pushed to the VTEPs)\")\r\n}\r\noverride def onError(t: Throwable): Unit = {\r\nlog.warn(\"Router update stream emits an error: \", t)\r\n}\r\noverride def onNext(o: Observable[Router]): Unit = {\r\nmySubscriptions.add(o.subscribe(new RouterObserver()))\r\n}\r\n}\r\n\r\noverride def isEnabled: Boolean = true\r\n\r\noverride def doStop(): Unit = {\r\nlog.debug(\"Stopping\")\r\nmySubscriptions.unsubscribe()\r\nnotifyStopped()\r\nlog.debug(\"Stopped\")\r\n}\r\n\r\noverride def doStart(): Unit = {\r\nlog.debug(\"Starting\")\r\n\r\nmySubscriptions.add (\r\nbackend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n)\r\n\r\nnotifyStarted()\r\n\r\nlog.debug(\"Started\")\r\n}\r\n\r\n}\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.\r\n\r\n&nbsp;',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(555,16,'2015-11-10 12:02:35','2015-11-10 12:02:35','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<blockquote>\n<pre><span style=\"color: #000000;\">$ git clone https://github.com/midonet/midonet</span></pre>\n</blockquote>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<blockquote>\n<pre>$ ./gradlew assemble</pre>\n</blockquote>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<blockquote>\n<pre>$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</pre>\n</blockquote>\n\nIf you\'re on OSX you can use Homebrew:\n\n<blockquote>\n<pre>$ brew install zookeeper\n$ zkServer start</pre>\n</blockquote>\n\nA Cluster service looks like this:\n\n<pre style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\n\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend,\n        conf: ClusterConfig) extends Minion(nodeCtx) {\n\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n        notifyStarted()\n        log.debug(\"Started\")\n    }\n}\n</pre>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n<pre style=\"padding-left: 30px;\">/** Define a sub-service that runs as part of the Midonet Cluster. This\n  * should expose the necessary API to let the Daemon babysit its minions.\n  *\n  * @param nodeContext metadata about the node where this Minion is running\n  */\nabstract class Minion(nodeContext: Context) extends AbstractService {\n    /** Whether the service is enabled on this Cluster node. */\n    def isEnabled: Boolean\n}\n</pre>\n\nWe will hardcode \'isEnabled\' for now.\n\n<pre style=\"padding-left: 30px;\">override def isEnabled: Boolean = true</pre>\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n<pre style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")</pre>\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\n<pre style=\"padding-left: 30px;\">class DirectConnectService @Inject()(\n nodeCtx: ClusterNode.Context,\n backend: MidonetBackend) extends Minion(nodeCtx) {</pre>\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n<pre style=\"padding-left: 30px;\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\n<pre style=\"padding-left: 30px;\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\n<pre style=\"padding-left: 30px;\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n<pre>2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\n\nAnd a bit further down, it\'ll be started:\n\n<pre>2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n</pre>\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\n<pre>package org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend) extends Minion(nodeCtx) {\n\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n    private val mySubscriptions = new CompositeSubscription()\n\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n      * an Observable.\n      */\n    private class RouterObserver extends Observer[Router] {\n\n        // On creation we still haven\'t seen any element emitted, so we\n        // don\'t know our Router\'s id.\n        @volatile private var id: UUID = null\n\n        override def onCompleted(): Unit = {\n            log.info(s\"Router $id was deleted, stop watching.\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(s\"Error in Router $id update stream: \", t)\n        }\n        override def onNext(o: Router): Unit = {\n            if (id == null) {\n                id = o.getId.asJava\n                log.info(s\"Loaded initial state of router $id\")\n            } else {\n                log.info(s\"Router $id updated\")\n            }\n        }\n    }\n\n    /** This class listens on the stream of streams of Routers that\n      * represents the full set of Routers in the system.  When a new Router\n      * appears on the system the stream will emit an Observable[Router] that\n      * will emit updates occurred on that Router until it\'s deleted.\n      *\n      * When we first subscribe to the stream, it will emit an\n      * Observable[Router] for each Router that exists in the system.\n      * Similarly, each Observable[Router] will always emit at least 1\n      * element, with the initial state of the Router.\n      */\n    private val routersObserver = new Observer[Observable[Router]] {\n        override def onCompleted(): Unit = {\n            log.info(\"Completed stream of router updates (no more updates \" +\n                     \"will be pushed to the VTEPs)\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(\"Router update stream emits an error: \", t)\n        }\n        override def onNext(o: Observable[Router]): Unit = {\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\n        }\n    }\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        mySubscriptions.unsubscribe()\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n\n        mySubscriptions.add (\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\n        )\n\n        notifyStarted()\n\n        log.debug(\"Started\")\n    }\n\n}\n</pre>\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 12:02:35','2015-11-10 12:02:35','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<blockquote>\r\n<pre><span style=\"color: #000000;\">$ git clone https://github.com/midonet/midonet</span></pre>\r\n</blockquote>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n<blockquote>\r\n<pre>$ ./gradlew assemble</pre>\r\n</blockquote>\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n<blockquote>\r\n<pre>$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</pre>\r\n</blockquote>\r\nIf you\'re on OSX you can use Homebrew:\r\n<blockquote>\r\n<pre>$ brew install zookeeper\r\n$ zkServer start</pre>\r\n</blockquote>\r\nA Cluster service looks like this:\r\n<pre style=\"padding-left: 30px;\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\n\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend,\r\n        conf: ClusterConfig) extends Minion(nodeCtx) {\r\n\r\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n        notifyStarted()\r\n        log.debug(\"Started\")\r\n    }\r\n}\r\n</pre>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n<pre style=\"padding-left: 30px;\">/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n  * should expose the necessary API to let the Daemon babysit its minions.\r\n  *\r\n  * @param nodeContext metadata about the node where this Minion is running\r\n  */\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n    /** Whether the service is enabled on this Cluster node. */\r\n    def isEnabled: Boolean\r\n}\r\n</pre>\r\nWe will hardcode \'isEnabled\' for now.\r\n<pre style=\"padding-left: 30px;\">override def isEnabled: Boolean = true</pre>\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n<pre style=\"padding-left: 30px;\">@ClusterService(name = \"direct_connect\")</pre>\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n<pre style=\"padding-left: 30px;\">class DirectConnectService @Inject()(\r\n nodeCtx: ClusterNode.Context,\r\n backend: MidonetBackend) extends Minion(nodeCtx) {</pre>\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n<pre style=\"padding-left: 30px;\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n<pre style=\"padding-left: 30px;\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n<pre style=\"padding-left: 30px;\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n<pre>2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\r\nAnd a bit further down, it\'ll be started:\r\n<pre>2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n</pre>\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n<pre>package org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n    private val mySubscriptions = new CompositeSubscription()\r\n\r\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n      * an Observable.\r\n      */\r\n    private class RouterObserver extends Observer[Router] {\r\n\r\n        // On creation we still haven\'t seen any element emitted, so we\r\n        // don\'t know our Router\'s id.\r\n        @volatile private var id: UUID = null\r\n\r\n        override def onCompleted(): Unit = {\r\n            log.info(s\"Router $id was deleted, stop watching.\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(s\"Error in Router $id update stream: \", t)\r\n        }\r\n        override def onNext(o: Router): Unit = {\r\n            if (id == null) {\r\n                id = o.getId.asJava\r\n                log.info(s\"Loaded initial state of router $id\")\r\n            } else {\r\n                log.info(s\"Router $id updated\")\r\n            }\r\n        }\r\n    }\r\n\r\n    /** This class listens on the stream of streams of Routers that\r\n      * represents the full set of Routers in the system.  When a new Router\r\n      * appears on the system the stream will emit an Observable[Router] that\r\n      * will emit updates occurred on that Router until it\'s deleted.\r\n      *\r\n      * When we first subscribe to the stream, it will emit an\r\n      * Observable[Router] for each Router that exists in the system.\r\n      * Similarly, each Observable[Router] will always emit at least 1\r\n      * element, with the initial state of the Router.\r\n      */\r\n    private val routersObserver = new Observer[Observable[Router]] {\r\n        override def onCompleted(): Unit = {\r\n            log.info(\"Completed stream of router updates (no more updates \" +\r\n                     \"will be pushed to the VTEPs)\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(\"Router update stream emits an error: \", t)\r\n        }\r\n        override def onNext(o: Observable[Router]): Unit = {\r\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\r\n        }\r\n    }\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        mySubscriptions.unsubscribe()\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n\r\n        mySubscriptions.add (\r\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n        )\r\n\r\n        notifyStarted()\r\n\r\n        log.debug(\"Started\")\r\n    }\r\n\r\n}\r\n</pre>\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(556,16,'2015-11-10 12:05:25','2015-11-10 12:05:25','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<blockquote>\n<pre>$ git clone https://github.com/midonet/midonet</pre>\n</blockquote>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<pre  style=\"padding-left: 30px;font-size:90%\">$ ./gradlew assemble</pre>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<pre style=\"padding-left: 30px;font-size:90%\">$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</pre>\n\nIf you\'re on OSX you can use Homebrew:\n\n<pre  style=\"padding-left: 30px;font-size:90%\">$ brew install zookeeper\n$ zkServer start\n</pre>\n\nA Cluster service looks like this:\n\n<pre style=\"padding-left: 30px;font-size:90%\">package org.midonet.cluster.services.direct_connect\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\n\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend,\n        conf: ClusterConfig) extends Minion(nodeCtx) {\n\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n        notifyStarted()\n        log.debug(\"Started\")\n    }\n}\n</pre>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n<pre style=\"padding-left: 30px;font-size:90%\">/** Define a sub-service that runs as part of the Midonet Cluster. This\n  * should expose the necessary API to let the Daemon babysit its minions.\n  *\n  * @param nodeContext metadata about the node where this Minion is running\n  */\nabstract class Minion(nodeContext: Context) extends AbstractService {\n    /** Whether the service is enabled on this Cluster node. */\n    def isEnabled: Boolean\n}\n</pre>\n\nWe will hardcode \'isEnabled\' for now.\n\n<pre style=\"padding-left: 30px;font-size:90%\">override def isEnabled: Boolean = true</pre>\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n<pre style=\"padding-left: 30px;font-size:90%\">@ClusterService(name = \"direct_connect\")</pre>\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\n<pre  style=\"padding-left: 30px;font-size:90%\">class DirectConnectService @Inject()(\n nodeCtx: ClusterNode.Context,\n backend: MidonetBackend) extends Minion(nodeCtx) {\n</pre>\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n<pre style=\"padding-left: 30px;font-size:90%\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\n<pre style=\"padding-left: 30px;font-size:90%\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\n<pre style=\"padding-left: 30px;font-size:90%\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n<pre style=\"padding-left: 30px;font-size:90%\">2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\n\nAnd a bit further down, it\'ll be started:\n\n<pre style=\"padding-left: 30px;font-size:90%\">2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n</pre>\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\n<pre  style=\"padding-left: 30px;font-size:90%\">package org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend) extends Minion(nodeCtx) {\n\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n    private val mySubscriptions = new CompositeSubscription()\n\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n      * an Observable.\n      */\n    private class RouterObserver extends Observer[Router] {\n\n        // On creation we still haven\'t seen any element emitted, so we\n        // don\'t know our Router\'s id.\n        @volatile private var id: UUID = null\n\n        override def onCompleted(): Unit = {\n            log.info(s\"Router $id was deleted, stop watching.\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(s\"Error in Router $id update stream: \", t)\n        }\n        override def onNext(o: Router): Unit = {\n            if (id == null) {\n                id = o.getId.asJava\n                log.info(s\"Loaded initial state of router $id\")\n            } else {\n                log.info(s\"Router $id updated\")\n            }\n        }\n    }\n\n    /** This class listens on the stream of streams of Routers that\n      * represents the full set of Routers in the system.  When a new Router\n      * appears on the system the stream will emit an Observable[Router] that\n      * will emit updates occurred on that Router until it\'s deleted.\n      *\n      * When we first subscribe to the stream, it will emit an\n      * Observable[Router] for each Router that exists in the system.\n      * Similarly, each Observable[Router] will always emit at least 1\n      * element, with the initial state of the Router.\n      */\n    private val routersObserver = new Observer[Observable[Router]] {\n        override def onCompleted(): Unit = {\n            log.info(\"Completed stream of router updates (no more updates \" +\n                     \"will be pushed to the VTEPs)\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(\"Router update stream emits an error: \", t)\n        }\n        override def onNext(o: Observable[Router]): Unit = {\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\n        }\n    }\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        mySubscriptions.unsubscribe()\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n\n        mySubscriptions.add (\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\n        )\n\n        notifyStarted()\n\n        log.debug(\"Started\")\n    }\n\n}\n</pre>\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 12:05:25','2015-11-10 12:05:25','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<blockquote>\r\n<pre>$ git clone https://github.com/midonet/midonet</pre>\r\n</blockquote>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n\r\n<pre  style=\"padding-left: 30px;font-size:90%\">$ ./gradlew assemble</pre>\r\n\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</pre>\r\n\r\nIf you\'re on OSX you can use Homebrew:\r\n\r\n<pre  style=\"padding-left: 30px;font-size:90%\">$ brew install zookeeper\r\n$ zkServer start\r\n</pre>\r\n\r\nA Cluster service looks like this:\r\n<pre style=\"padding-left: 30px;font-size:90%\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\n\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend,\r\n        conf: ClusterConfig) extends Minion(nodeCtx) {\r\n\r\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n        notifyStarted()\r\n        log.debug(\"Started\")\r\n    }\r\n}\r\n</pre>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n<pre style=\"padding-left: 30px;font-size:90%\">/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n  * should expose the necessary API to let the Daemon babysit its minions.\r\n  *\r\n  * @param nodeContext metadata about the node where this Minion is running\r\n  */\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n    /** Whether the service is enabled on this Cluster node. */\r\n    def isEnabled: Boolean\r\n}\r\n</pre>\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">override def isEnabled: Boolean = true</pre>\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n<pre style=\"padding-left: 30px;font-size:90%\">@ClusterService(name = \"direct_connect\")</pre>\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\n<pre  style=\"padding-left: 30px;font-size:90%\">class DirectConnectService @Inject()(\r\n nodeCtx: ClusterNode.Context,\r\n backend: MidonetBackend) extends Minion(nodeCtx) {\r\n</pre>\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\n\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n<pre style=\"padding-left: 30px;font-size:90%\">2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n</pre>\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\n<pre  style=\"padding-left: 30px;font-size:90%\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n    private val mySubscriptions = new CompositeSubscription()\r\n\r\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n      * an Observable.\r\n      */\r\n    private class RouterObserver extends Observer[Router] {\r\n\r\n        // On creation we still haven\'t seen any element emitted, so we\r\n        // don\'t know our Router\'s id.\r\n        @volatile private var id: UUID = null\r\n\r\n        override def onCompleted(): Unit = {\r\n            log.info(s\"Router $id was deleted, stop watching.\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(s\"Error in Router $id update stream: \", t)\r\n        }\r\n        override def onNext(o: Router): Unit = {\r\n            if (id == null) {\r\n                id = o.getId.asJava\r\n                log.info(s\"Loaded initial state of router $id\")\r\n            } else {\r\n                log.info(s\"Router $id updated\")\r\n            }\r\n        }\r\n    }\r\n\r\n    /** This class listens on the stream of streams of Routers that\r\n      * represents the full set of Routers in the system.  When a new Router\r\n      * appears on the system the stream will emit an Observable[Router] that\r\n      * will emit updates occurred on that Router until it\'s deleted.\r\n      *\r\n      * When we first subscribe to the stream, it will emit an\r\n      * Observable[Router] for each Router that exists in the system.\r\n      * Similarly, each Observable[Router] will always emit at least 1\r\n      * element, with the initial state of the Router.\r\n      */\r\n    private val routersObserver = new Observer[Observable[Router]] {\r\n        override def onCompleted(): Unit = {\r\n            log.info(\"Completed stream of router updates (no more updates \" +\r\n                     \"will be pushed to the VTEPs)\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(\"Router update stream emits an error: \", t)\r\n        }\r\n        override def onNext(o: Observable[Router]): Unit = {\r\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\r\n        }\r\n    }\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        mySubscriptions.unsubscribe()\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n\r\n        mySubscriptions.add (\r\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n        )\r\n\r\n        notifyStarted()\r\n\r\n        log.debug(\"Started\")\r\n    }\r\n\r\n}\r\n</pre>\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(557,16,'2015-11-10 12:06:52','2015-11-10 12:06:52','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\n\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\nFirst, clone the MidoNet repo:\n\n<blockquote>\n<pre>$ git clone https://github.com/midonet/midonet</pre>\n</blockquote>\n\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\n\nWhen you can run:\n\n<pre  style=\"padding-left: 30px;font-size:80%\">$ ./gradlew assemble</pre>\n\nYou\'re good.\n\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\n\n<pre style=\"padding-left: 30px;font-size:80%\">$ sudo apt-get install zookeeper\n$ sudo service zookeeper start</pre>\n\nIf you\'re on OSX you can use Homebrew:\n\n<pre  style=\"padding-left: 30px;font-size:80%\">$ brew install zookeeper\n$ zkServer start\n</pre>\n\nA Cluster service looks like this:\n\n<pre style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\n\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend,\n        conf: ClusterConfig) extends Minion(nodeCtx) {\n\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n        notifyStarted()\n        log.debug(\"Started\")\n    }\n}\n</pre>\n\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\n\nLet\'s review the code.\n\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\n\n<pre style=\"padding-left: 30px;font-size:80%\">/** Define a sub-service that runs as part of the Midonet Cluster. This\n  * should expose the necessary API to let the Daemon babysit its minions.\n  *\n  * @param nodeContext metadata about the node where this Minion is running\n  */\nabstract class Minion(nodeContext: Context) extends AbstractService {\n    /** Whether the service is enabled on this Cluster node. */\n    def isEnabled: Boolean\n}\n</pre>\n\nWe will hardcode \'isEnabled\' for now.\n\n<pre style=\"padding-left: 30px;font-size:80%\">override def isEnabled: Boolean = true</pre>\n\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\n\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\n\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\n\n<pre style=\"padding-left: 30px;font-size:80%\">@ClusterService(name = \"direct_connect\")</pre>\n\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\n\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\n\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\n\n<pre  style=\"padding-left: 30px;font-size:80%\">class DirectConnectService @Inject()(\n nodeCtx: ClusterNode.Context,\n backend: MidonetBackend) extends Minion(nodeCtx) {\n</pre>\n\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\n\n<ul>\n    <li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\n    <li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\n</ul>\n\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\n\n<pre style=\"padding-left: 30px;font-size:80%\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\n\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\n\nTo see the logs you have 2 options, tail the log file:\n\n<pre style=\"padding-left: 30px;font-size:80%\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\n\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\n\n<pre style=\"padding-left: 30px;font-size:80%\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\n\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\n\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\n\nAnd a bit further down, it\'ll be started:\n\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\n</pre>\n\nDone!\n\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\n\n<pre  style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\n\nimport java.util.UUID\n\nimport com.google.inject.Inject\nimport org.slf4j.LoggerFactory\nimport rx.subscriptions.CompositeSubscription\nimport rx.{Observable, Observer}\n\nimport org.midonet.cluster.ClusterNode\nimport org.midonet.cluster.models.Topology.Router\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\nimport org.midonet.cluster.util.UUIDUtil._\n\n@ClusterService(name = \"direct_connect\")\nclass DirectConnectService @Inject()(\n        nodeCtx: ClusterNode.Context,\n        backend: MidonetBackend) extends Minion(nodeCtx) {\n\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\n    private val mySubscriptions = new CompositeSubscription()\n\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\n      * an Observable.\n      */\n    private class RouterObserver extends Observer[Router] {\n\n        // On creation we still haven\'t seen any element emitted, so we\n        // don\'t know our Router\'s id.\n        @volatile private var id: UUID = null\n\n        override def onCompleted(): Unit = {\n            log.info(s\"Router $id was deleted, stop watching.\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(s\"Error in Router $id update stream: \", t)\n        }\n        override def onNext(o: Router): Unit = {\n            if (id == null) {\n                id = o.getId.asJava\n                log.info(s\"Loaded initial state of router $id\")\n            } else {\n                log.info(s\"Router $id updated\")\n            }\n        }\n    }\n\n    /** This class listens on the stream of streams of Routers that\n      * represents the full set of Routers in the system.  When a new Router\n      * appears on the system the stream will emit an Observable[Router] that\n      * will emit updates occurred on that Router until it\'s deleted.\n      *\n      * When we first subscribe to the stream, it will emit an\n      * Observable[Router] for each Router that exists in the system.\n      * Similarly, each Observable[Router] will always emit at least 1\n      * element, with the initial state of the Router.\n      */\n    private val routersObserver = new Observer[Observable[Router]] {\n        override def onCompleted(): Unit = {\n            log.info(\"Completed stream of router updates (no more updates \" +\n                     \"will be pushed to the VTEPs)\")\n        }\n        override def onError(t: Throwable): Unit = {\n            log.warn(\"Router update stream emits an error: \", t)\n        }\n        override def onNext(o: Observable[Router]): Unit = {\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\n        }\n    }\n\n    override def isEnabled: Boolean = true\n\n    override def doStop(): Unit = {\n        log.debug(\"Stopping\")\n        mySubscriptions.unsubscribe()\n        notifyStopped()\n        log.debug(\"Stopped\")\n    }\n\n    override def doStart(): Unit = {\n        log.debug(\"Starting\")\n\n        mySubscriptions.add (\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\n        )\n\n        notifyStarted()\n\n        log.debug(\"Started\")\n    }\n\n}\n</pre>\n\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\n\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\norg/midonet/cluster/services package for further learning.\n\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\n\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.','Writing your own MidoNet v5 Cluster Service','','inherit','closed','closed','','548-revision-v1','','','2015-11-10 12:06:52','2015-11-10 12:06:52','In a <a href=\"https://blog.midonet.org/introducing-midonet-cluster-services/\">previous post</a> we introduced the MidoNet Cluster Services that are supported from the MidoNet v5.0.0 release (currently in rc).  These services can implement dedicated management functions and run embedded in the MidoNet Cluster that acts as a very simple container providing access to common infrastructure and resources.\r\n\r\nHere we will offer a hands-on example for developers demostrating how to write and integrate a simple management service in the MidoNet Cluster. We\'ll use a real service that\'s just getting implemented for a <a href=\"https://docs.google.com/document/d/1yusy76r50aLL6_-oUvu58pdY85CD9n8dfKaZSXa_isk/edit\">new feature</a>, but you don\'t need any familiarity with it.\r\nFirst, clone the MidoNet repo:\r\n<blockquote>\r\n<pre>$ git clone https://github.com/midonet/midonet</pre>\r\n</blockquote>\r\nFollow the instructions detaled in DEVELOPMENT.md to setup your environment.\r\n\r\nWhen you can run:\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">$ ./gradlew assemble</pre>\r\n\r\nYou\'re good.\r\n\r\nSince we\'ll be running the MidoNet Cluster locally, we want to run a ZooKeeper server in our own box.  On GNU/Linux run the equivalent to this on your own distro:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">$ sudo apt-get install zookeeper\r\n$ sudo service zookeeper start</pre>\r\n\r\nIf you\'re on OSX you can use Homebrew:\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">$ brew install zookeeper\r\n$ zkServer start\r\n</pre>\r\n\r\nA Cluster service looks like this:\r\n<pre style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\n\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.{ClusterConfig, ClusterNode}\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend,\r\n        conf: ClusterConfig) extends Minion(nodeCtx) {\r\n\r\n    val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n        notifyStarted()\r\n        log.debug(\"Started\")\r\n    }\r\n}\r\n</pre>\r\nI saved this in ./midonet-cluster/src/main/scala/org/midonet/cluster/services/direct_connect/DirectConnectService.scala\r\n\r\nLet\'s review the code.\r\n\r\nA Cluster service is an ordinary class that extends the \'Minion\' abstract class (devs refer to cluster services as Minions, since they do the actual work for the Cluster Daemon).  Here is the interface:\r\n<pre style=\"padding-left: 30px;font-size:80%\">/** Define a sub-service that runs as part of the Midonet Cluster. This\r\n  * should expose the necessary API to let the Daemon babysit its minions.\r\n  *\r\n  * @param nodeContext metadata about the node where this Minion is running\r\n  */\r\nabstract class Minion(nodeContext: Context) extends AbstractService {\r\n    /** Whether the service is enabled on this Cluster node. */\r\n    def isEnabled: Boolean\r\n}\r\n</pre>\r\n\r\nWe will hardcode \'isEnabled\' for now.\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">override def isEnabled: Boolean = true</pre>\r\nMinion is actually a subclass of <a href=\"https://github.com/google/guava\">Guava</a>\'s AbstractService [4], which imposes the two overrides of \'doStart\' and \'doStop\'.  If you\'ve used <a href=\"https://github.com/google/guice/wiki/Motivation\">Guice</a> before, this will be familiar to you.  Otherwise no worries, for our purposes you only need to know that both methods will be invoked by the Cluster Daemon during the initialisation and tear down phases of the application\'s lifecycle.\r\n\r\nIn our implementation of doStart and doStop, show above, we\'re simply logging some messages, and using the \"notifyStarted\" and \"notifyStopped\" methods to signal the framework that our service has completed all required tasks to start/stop.\r\n\r\nHow does the MidoNet Cluster know that we want to run this service? The following annotation on the class does the trick:\r\n<pre style=\"padding-left: 30px;font-size:80%\">@ClusterService(name = \"direct_connect\")</pre>\r\nThe MidoNet Cluster Daemon will scan the classpath for this annotation, and include.  The \"name\" property should be set with a unique name for this management service.  In our case we\'re using \"direct_connect\".\r\n\r\nIn v5.0 we don\'t support multiple Minions using the same service name, later releases will add strategies to select one out of several competing Minions for the same service.\r\n\r\nFinally, we\'re using Guice\'s @Inject to inject some dependencies into our Minion.\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">class DirectConnectService @Inject()(\r\n nodeCtx: ClusterNode.Context,\r\n backend: MidonetBackend) extends Minion(nodeCtx) {\r\n</pre>\r\n\r\nThe MidoNet Cluster offers an instance of some common-purpose components that are shared by all Minions.  The most important are these two:\r\n\r\n<ul>\r\n	<li>The <a href=\"https://github.com/midonet/midonet/blob/master/midonet-cluster/src/main/scala/org/midonet/cluster/ClusterNode.scala#L60\">ClusterNode.Context</a> exposes information about the Cluster node, such as its UUID.</li>\r\n	<li>The MidonetBackend is our API to the NSDB that can be used to access virtual network configuration and state.  We\'ll use this one in a bit.</li>\r\n</ul>\r\n\r\nOk so let\'s run this!  You can start a Cluster node by running this onvthe root of your MidoNet clone:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">$ ./gradlew midonet-cluster:run -Dmidonet.host_id_filepath=/tmp/midonet_host_id.properties</pre>\r\n\r\n(You can avoid the -D.. chunk by having /etc/midonet_host_id.properties writable)\r\n\r\nTo see the logs you have 2 options, tail the log file:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">tail -f midonet-cluster.log.dir_IS_UNDEFINEDmidonet-cluster.log</pre>\r\n\r\nOr if you prefer to see them in the console, enable DEBUG logs by adding the logback-test.xml file to the classpath, for example:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">cp ./midonet-util/build/resources/test/logback-test.xml ./midonet-util/build/resources/main/logback-test.xml</pre>\r\n\r\nBy default the Cluster will connect to ZK at 127.0.0.1, so you should soon see your new Minion being picked up and started among the rest.\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:14.603 INFO  [main] cluster -  Minion: direct_connect provided by org.midonet.cluster.services.direct_connect.DirectConnectService.</pre>\r\n\r\nAnd a bit further down, it\'ll be started:\r\n\r\n<pre style=\"padding-left: 30px;font-size:80%\">2015.11.10 13:41:15.837 INFO  [ForkJoinPool-1-worker-3] cluster - Starting cluster minion: direct_connect\r\n2015.11.10 13:41:15.841 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Starting\r\n2015.11.10 13:41:15.861 DEBUG [ForkJoinPool-1-worker-3] direct_connect - Started\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Started cluster minion: direct_connect\r\n2015.11.10 13:41:15.861 INFO  [ForkJoinPool-1-worker-3] cluster - Minion direct_connect started successfully\r\n</pre>\r\n\r\nDone!\r\n\r\nNow it would be nice to do something useful with this Minion.  Direct Connect needs to use the MidonetBackend to subscribe to all Routers in the system (learn about existing ones, updates, and deletions).  Doing this is fairly simple.  We\'ll be using some of the reactive APIs, you should be able to follow the code easily, but feel free to dig deeper in our <a href=\"https://blog.midonet.org/zoom-reactive-programming-zookeeper/\">previous post on the topic</a>, or the <a href=\"https://github.com/ReactiveX/RxJava/\">RxJava</a> site.\r\n\r\n<pre  style=\"padding-left: 30px;font-size:80%\">package org.midonet.cluster.services.direct_connect\r\n\r\nimport java.util.UUID\r\n\r\nimport com.google.inject.Inject\r\nimport org.slf4j.LoggerFactory\r\nimport rx.subscriptions.CompositeSubscription\r\nimport rx.{Observable, Observer}\r\n\r\nimport org.midonet.cluster.ClusterNode\r\nimport org.midonet.cluster.models.Topology.Router\r\nimport org.midonet.cluster.services.{ClusterService, MidonetBackend, Minion}\r\nimport org.midonet.cluster.util.UUIDUtil._\r\n\r\n@ClusterService(name = \"direct_connect\")\r\nclass DirectConnectService @Inject()(\r\n        nodeCtx: ClusterNode.Context,\r\n        backend: MidonetBackend) extends Minion(nodeCtx) {\r\n\r\n    private val log = LoggerFactory.getLogger(\"org.midonet.cluster.direct_connect\")\r\n    private val mySubscriptions = new CompositeSubscription()\r\n\r\n    /** This class listens to notifications on a single [[org.midonet.cluster.models.Topology.Router]] emitted on\r\n      * an Observable.\r\n      */\r\n    private class RouterObserver extends Observer[Router] {\r\n\r\n        // On creation we still haven\'t seen any element emitted, so we\r\n        // don\'t know our Router\'s id.\r\n        @volatile private var id: UUID = null\r\n\r\n        override def onCompleted(): Unit = {\r\n            log.info(s\"Router $id was deleted, stop watching.\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(s\"Error in Router $id update stream: \", t)\r\n        }\r\n        override def onNext(o: Router): Unit = {\r\n            if (id == null) {\r\n                id = o.getId.asJava\r\n                log.info(s\"Loaded initial state of router $id\")\r\n            } else {\r\n                log.info(s\"Router $id updated\")\r\n            }\r\n        }\r\n    }\r\n\r\n    /** This class listens on the stream of streams of Routers that\r\n      * represents the full set of Routers in the system.  When a new Router\r\n      * appears on the system the stream will emit an Observable[Router] that\r\n      * will emit updates occurred on that Router until it\'s deleted.\r\n      *\r\n      * When we first subscribe to the stream, it will emit an\r\n      * Observable[Router] for each Router that exists in the system.\r\n      * Similarly, each Observable[Router] will always emit at least 1\r\n      * element, with the initial state of the Router.\r\n      */\r\n    private val routersObserver = new Observer[Observable[Router]] {\r\n        override def onCompleted(): Unit = {\r\n            log.info(\"Completed stream of router updates (no more updates \" +\r\n                     \"will be pushed to the VTEPs)\")\r\n        }\r\n        override def onError(t: Throwable): Unit = {\r\n            log.warn(\"Router update stream emits an error: \", t)\r\n        }\r\n        override def onNext(o: Observable[Router]): Unit = {\r\n            mySubscriptions.add(o.subscribe(new RouterObserver()))\r\n        }\r\n    }\r\n\r\n    override def isEnabled: Boolean = true\r\n\r\n    override def doStop(): Unit = {\r\n        log.debug(\"Stopping\")\r\n        mySubscriptions.unsubscribe()\r\n        notifyStopped()\r\n        log.debug(\"Stopped\")\r\n    }\r\n\r\n    override def doStart(): Unit = {\r\n        log.debug(\"Starting\")\r\n\r\n        mySubscriptions.add (\r\n            backend.store.observable(classOf[Router]).subscribe(routersObserver)\r\n        )\r\n\r\n        notifyStarted()\r\n\r\n        log.debug(\"Started\")\r\n    }\r\n\r\n}\r\n</pre>\r\n\r\nThe code for both examples is in <a href=\"https://github.com/srvaroa/midonet/tree/cluster.service.example\">GitHub</a>, in separate commits.\r\n\r\nThis should give you a good foundation to start experimenting with your own management services and even contribute them to the MidoNet codebase. Feel free to use the existing services already in the\r\norg/midonet/cluster/services package for further learning.\r\n\r\nOf course, if you need a hand you can always ping us on the #development <a href=\"https://slack.midonet.org/\">Slack</a> channel.\r\n\r\nOur next post in this series will examine how to deploy Cluster services as plugins that can be distributed independently from the MidoNet OSS packages.',548,'https://blog.midonet.org/548-revision-v1/',0,'revision','',0),(558,8,'2015-11-12 14:13:51','2015-11-12 14:13:51','','Auto Draft','','trash','open','open','','auto-draft','','','2015-11-12 14:14:10','2015-11-12 14:14:10','',0,'https://blog.midonet.org/?p=558',0,'post','',0),(559,8,'2015-11-12 14:14:10','2015-11-12 14:14:10','','Auto Draft','','inherit','closed','closed','','558-revision-v1','','','2015-11-12 14:14:10','2015-11-12 14:14:10','',558,'https://blog.midonet.org/558-revision-v1/',0,'revision','',0),(561,8,'2015-11-13 11:56:23','2015-11-13 11:56:23','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n\nMidoNet brings production-grade distributed overlay networking to OpenStack and containers.\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This version is compatible with Openstack Kilo and Liberty.</div>\n&nbsp;\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n&nbsp;\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n&nbsp;\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','publish','open','open','','midonet-5-0-0-release','','','2015-11-13 11:56:23','2015-11-13 11:56:23','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n\r\nMidoNet brings production-grade distributed overlay networking to OpenStack and containers.\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This version is compatible with Openstack Kilo and Liberty.</div>\r\n&nbsp;\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n&nbsp;\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n&nbsp;\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',0,'https://blog.midonet.org/?p=561',0,'post','',0),(562,8,'2015-11-12 14:37:59','2015-11-12 14:37:59','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<div></div>\n<div>This release particularly focuses on usability and additional network services.\nDetails on all new features and improvements are in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n</div>\nThe packages are available in the repository (URL in the release notes).\n\n<hr />\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n</div>\n<div>\n\n<hr />\n\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, great job!</div>\n\n<div>\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>\n\n<div>\n\n<hr />\n\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>\n$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n</div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-12 14:37:59','2015-11-12 14:37:59','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<div></div>\r\n<div>This release particularly focuses on usability and additional network services.\r\nDetails on all new features and improvements are in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n</div>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n<hr />\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n</div>\r\n<div>\r\n\r\n<hr />\r\n\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, great job!</div>\r\n<div>\r\nTo ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>\r\n<div>\r\n\r\n<hr />\r\n\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>\r\n$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n</div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(563,8,'2015-11-12 14:40:02','2015-11-12 14:40:02','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<div></div>\n<div>This release particularly focuses on usability and additional network services.\nDetails on all new features and improvements are in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n</div>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, great job!</div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>\n\n<div>\n\n<hr />\n\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n</div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-12 14:40:02','2015-11-12 14:40:02','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<div></div>\r\n<div>This release particularly focuses on usability and additional network services.\r\nDetails on all new features and improvements are in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n</div>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, great job!</div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>\r\n<div>\r\n\r\n<hr />\r\n\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n</div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(564,8,'2015-11-12 15:00:20','2015-11-12 15:00:20','','Sunrise_in_the_fog_7723','','inherit','open','closed','','sunrise_in_the_fog_7723','','','2015-11-12 15:00:20','2015-11-12 15:00:20','',561,'https://blog.midonet.org/wp-content/uploads/2015/11/Sunrise_in_the_fog_7723.jpg',0,'attachment','image/jpeg',0),(565,8,'2015-11-12 15:05:44','2015-11-12 15:05:44','','Sunset','Sunset view from MidoDay Tokyo','inherit','open','closed','','img_2487','','','2015-11-13 11:31:33','2015-11-13 11:31:33','',561,'https://blog.midonet.org/wp-content/uploads/2015/11/IMG_2487.jpg',0,'attachment','image/jpeg',0),(566,8,'2015-11-13 11:10:30','2015-11-13 11:10:30','<div>\n<div>\n<div>\n<div>\n<div>\n<div></div>\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<div></div>\n<h2>Overview</h2>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, <strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:10:30','2015-11-13 11:10:30','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div></div>\r\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<div></div>\r\n<h2>Overview</h2>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, <strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(567,8,'2015-11-13 11:11:25','2015-11-13 11:11:25','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<div></div>\n<h2>Overview</h2>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, <strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:11:25','2015-11-13 11:11:25','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<div></div>\r\n<h2>Overview</h2>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, <strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(568,8,'2015-11-13 11:11:58','2015-11-13 11:11:58','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<div></div>\n<h2>Overview</h2>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release, <strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:11:58','2015-11-13 11:11:58','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<div></div>\r\n<h2>Overview</h2>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release, <strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(569,8,'2015-11-13 11:20:02','2015-11-13 11:20:02','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<div></div>\n<h2>Overview</h2>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:20:02','2015-11-13 11:20:02','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<div></div>\r\n<h2>Overview</h2>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(570,8,'2015-11-13 11:21:06','2015-11-13 11:21:06','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div></div>\n<div>\n<h2>Overview</h2>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n</div>\n</div>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<div></div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:21:06','2015-11-13 11:21:06','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div></div>\r\n<div>\r\n<h2>Overview</h2>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<div></div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(571,8,'2015-11-13 11:23:16','2015-11-13 11:23:16','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div></div>\n<div></div>\n<div>\n<h2>Overview</h2>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n<div></div>\n</div>\n</div>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<div></div>\n<div></div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:23:16','2015-11-13 11:23:16','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div></div>\r\n<div></div>\r\n<div>\r\n<h2>Overview</h2>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<div></div>\r\n<div></div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(572,8,'2015-11-13 11:35:40','2015-11-13 11:35:40','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div></div>\n<div></div>\n<div>\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n<div></div>\n</div>\n</div>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<div></div>\n<div></div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:35:40','2015-11-13 11:35:40','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div></div>\r\n<div></div>\r\n<div>\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<div></div>\r\n<div></div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(573,8,'2015-11-13 11:37:52','2015-11-13 11:37:52','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div></div>\n<div></div>\n<div>\n<br>\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n<div></div>\n</div>\n</div>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<div></div>\n<div></div>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:37:52','2015-11-13 11:37:52','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div></div>\r\n<div></div>\r\n<div>\r\n<br>\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<div></div>\r\n<div></div>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(574,8,'2015-11-13 11:38:54','2015-11-13 11:38:54','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div></div>\n<div></div>\n<div>\n<br>\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div></div>\n<div></div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n<div></div>\n<div></div>\n</div>\n</div>\n<br>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n<div></div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div></div>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<div></div>\n<div></div>\n<br>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:38:54','2015-11-13 11:38:54','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div></div>\r\n<div></div>\r\n<div>\r\n<br>\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div></div>\r\n<div></div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n<div></div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<br>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n<div></div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div></div>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<div></div>\r\n<div></div>\r\n<br>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(575,8,'2015-11-13 11:40:35','2015-11-13 11:40:35','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<br>\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n<br>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<br>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:40:35','2015-11-13 11:40:35','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<br>\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n<br>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<br>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(576,8,'2015-11-13 11:44:03','2015-11-13 11:44:03','<div>\n<div>\n<div>\n<div>\n<div>\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n<br>\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<br>\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<br>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n<br>\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n<br>\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n<br>\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:44:03','2015-11-13 11:44:03','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div> The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n<br>\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<br>\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<br>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n<br>\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n<br>\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n<br>\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(577,8,'2015-11-13 11:49:47','2015-11-13 11:49:47','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>MidoNet brings production-grade distributed overlay networking to OpenStack and containers.\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This version is compatible with Openstack Kilo and Liberty.</div>\n&nbsp;\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n&nbsp;\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n&nbsp;\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:49:47','2015-11-13 11:49:47','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>MidoNet brings production-grade distributed overlay networking to OpenStack and containers.\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This version is compatible with Openstack Kilo and Liberty.</div>\r\n&nbsp;\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n&nbsp;\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n&nbsp;\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(578,8,'2015-11-13 11:52:37','2015-11-13 11:52:37','Sunrise on Midoday in Tokyo','MidoDay','Sunrise on Midoday in Tokyo','inherit','open','closed','','midoday','','','2015-11-13 11:53:14','2015-11-13 11:53:14','',561,'https://blog.midonet.org/wp-content/uploads/2015/11/MidoDay.jpg',0,'attachment','image/jpeg',0),(579,8,'2015-11-13 11:53:25','2015-11-13 11:53:25','<div>\n<div>\n<div>\n<div>\n<div>\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\n<div>\n\nMidoNet brings production-grade distributed overlay networking to OpenStack and containers.\n<h2>Overview of the release</h2>\n<div>It is the 5th release of the project as Open Source software.</div>\n<div>This version is compatible with Openstack Kilo and Liberty.</div>\n&nbsp;\n<div>This release particularly focuses on usability and additional network services.\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\n</div>\n<div>\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\n</div>\n</div>\n&nbsp;\n<h2>Installing MidoNet 5.0</h2>\nThe packages are available in the repository (URL in the release notes).\n\n</div>\n</div>\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\n</div>\n</div>\n\n<div>\n<div>\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\n&nbsp;\n<h2>Going further</h2>\n</div>\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\n\n</div>\n\n<div>We would like to thank everyone who contributed to this release,</div>\n\n<div><strong>great job!</strong></div>\n\n&nbsp;\n\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\n\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\n\n<div></div>','MidoNet 5.0.0 release','','inherit','closed','closed','','561-revision-v1','','','2015-11-13 11:53:25','2015-11-13 11:53:25','<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>\r\n<div>The MidoNet project is pleased to announce the release of <strong>MidoNet 5.0.0</strong>.</div>\r\n<div>\r\n\r\nMidoNet brings production-grade distributed overlay networking to OpenStack and containers.\r\n<h2>Overview of the release</h2>\r\n<div>It is the 5th release of the project as Open Source software.</div>\r\n<div>This version is compatible with Openstack Kilo and Liberty.</div>\r\n&nbsp;\r\n<div>This release particularly focuses on usability and additional network services.\r\nThe list of new features and improvements is in the <a href=\"https://github.com/midonet/midonet/wiki/MidoNet-5.0.0-Release-Notes\">release notes</a>.</div>\r\n</div>\r\n<div>\r\n<div>This release introduces a change in the release cycle: we will now use <a href=\"http://semver.org/\">semantic versioning</a>.</div>\r\n<div>You can see the <a href=\"https://lists.midonet.org/pipermail/midonet/2015-June/000226.html\">details of the discussion</a> related to this change on the mailing list.</div>\r\n</div>\r\n</div>\r\n&nbsp;\r\n<h2>Installing MidoNet 5.0</h2>\r\nThe packages are available in the repository (URL in the release notes).\r\n\r\n</div>\r\n</div>\r\n<div>See the <a href=\"http://docs.midonet.org/\">MidoNet documentation</a> for details on installing and operating MidoNet.</div>\r\n</div>\r\n</div>\r\n<div>\r\n<div>\r\n<div>Trying MidoNet is as simple as executing this command inside an Ubuntu 14.04 Virtual Machine with at least 4GB of memory (8GB recommended)</div>\r\n<pre>$ curl -sL quickstart.midonet.org | sudo bash</pre>\r\n<div>This command will install the latest MidoNet with OpenStack. Once finished, you will be told how to log into Horizon.</div>\r\n&nbsp;\r\n<h2>Going further</h2>\r\n</div>\r\nIf you have any question, please reach out via <a href=\"http://slack.midonet.org/\">Slack</a> or on the <a href=\"mailto:midonet@lists.midonet.org\" target=\"_blank\">MidoNet user list</a>.\r\n\r\n</div>\r\n<div>We would like to thank everyone who contributed to this release,</div>\r\n<div><strong>great job!</strong></div>\r\n&nbsp;\r\n<div>To ensure its success and improve further the quality, more help is needed: organizations and individuals using MidoNet are encouraged to contribute to the project.</div>\r\n<div>You can find ways to become an active contributor on the <a href=\"https://github.com/midonet/midonet/wiki/Contributing-to-MidoNet\">MidoNet wiki</a>.</div>\r\n<div></div>',561,'https://blog.midonet.org/561-revision-v1/',0,'revision','',0),(580,15,'2015-11-26 03:09:26','0000-00-00 00:00:00','','Auto Draft','','auto-draft','open','open','','','','','2015-11-26 03:09:26','0000-00-00 00:00:00','',0,'https://blog.midonet.org/?p=580',0,'post','',0);
/*!40000 ALTER TABLE `wp_posts` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

